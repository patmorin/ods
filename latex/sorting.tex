\chapter{Algoritmos de Ordenação}
\chaplabel{sorting}
%%% TODO verificar consistencia dos nomes dos algoritmos de ordenação: quicksort, heap sort, mergesort..
% quicksort, mergesort, heapsort

Este capítulo apresenta algoritmos para ordenação de um conjunto de #n# itens.
Esse pode ser um tópico estranho para um livro sobre estruturas de dados,
mas existem várias boas razões para incluí-lo aqui.
A razão mais óbvia é que dois desses algoritmos de ordenação (quicksort e heapsort)
são intimamente relacionados com duas das estruturas de dados que são estudadas neste livro (árvores binárias de busca aleatórias e heaps).

A primeira parte deste capítulo discute algoritmos que ordenam usando somente
comparações e apresenta três algoritmos que rodam em 
$O(#n#\log #n#)$ de tempo.
Acontece que esses três algoritmos são assintoticamente ótimos;
nenhum algoritmo que usa somente comparações pode evitar fazer
aproximadamente $#n#\log #n#$ comparações no pior caso e mesmo no caso médio.

Antes de continuar, devemos notar que quaisquer implementações do #SSet#
ou da #Queue# de prioridades apresentadas nos capítulos anteriores
também podem ser usadas para obter um algoritmo de ordenação 
 $O(#n#\log #n#)$.

Por exemplo, podemos ordenar #n# itens fazendo #n#
 operações #add(x)# seguidas de 
 #n# operações #remove()# em uma #BinaryHeap# ou em uma #MeldableHeap#. 
 Alternativamente, podemos usar #n# operações #add(x)# 
 em qualquer estrutura de dados de árvore de busca binária e então
 realizar uma travessia em ordem 
(\excref{tree-traversal}) para extrair os elementos ordenados 
Porém, nos dois casos teremos muitos gastos para construir uma estrutura
que nunca é completamente usada. Ordenação é um problema extremamente 
importante e vale a pena desenvolver métodos diretos que são o mais rápidos,
simples e econômicos em uso da memória possíveis.

A segunda parte deste capítulo mostra que, se for possível usar outras 
operações além de comparações, então tudo é possível. Realmente, 
ao usar indexação por arrays, é possível ordenar um conjunto de #n# inteiros
no intervalo $\{0,\ldots,#n#^c-1\}$ usando somente $O(c#n#)$ de tempo.


\section{Ordenação Baseada em Comparação}

\index{ordenação baseada em comparação}%
\index{sorting algorithm!comparison-based}%
\index{algoritmo de ordenação!baseado em comparação}%
Nesta seção, apresentamos três algoritmos: mergesort, quicksort e heapsort.
Cada um desses algoritmos recebe um array de entrada #a#
e ordena os elementos de #a# em ordem não decrescente em 
$O(#n#\log #n#)$ de tempo (esperado).
Esses algoritmos são todos \emph{baseados em comparação}.
\javaonly{O segundo argumento, #c#, é um #Comparator#
\index{Comparator@#Comparator#}%
que implementa o método
#compare(a,b)#
\index{compare@#compare(a,b)#}%
.} Esses algoritmos dependem de qual tipo de dados está sendo ordenado;
a única operação que eles fazem nos dados é comparações
usando o método 
#compare(a,b)#. Lembre-se, da \secref{sset},
que #compare(a,b)# retorna um valor negativo se $#a#<#b#$, um valor positivo
se $#a#>#b#$ e zero se $#a#=#b#$.

\subsection{Mergesort}
\seclabel{merge-sort}

\index{mergesort}%
O algoritmo \emph{mergesort} é um clássico exemplo do paradigma de divisão e conquista (recursiva):
\index{divisão e conquista}%
Se o tamanho de 
#a# for até 1, então #a# está ordenado e não fazemos nada. 
Caso contrário, dividimos #a# em duas metades,
$#a0#=#a[0]#,\ldots,#a[n/2-1]#$ e $#a1#=#a[n/2]#,\ldots,#a[n-1]#$.
Nós recursivamente ordenamos #a0# e #a1# e então fazermos o merge (em português, a combinação) dos, agora 
ordenados, #a0# e #a1# para ordenar o array #a# completo:
\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
\pcodeimport{ods/Algorithms.mergeSort(a)}
Um exemplo é mostrado na \figref{merge-sort}.
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
  \caption[Merge sort]{A execução de #mergeSort(a,c)#}
  \figlabel{merge-sort}
\end{figure}

Comparada à ordenação, fazer a combinação (\emph{merge}) de dois arrays ordenados #a0#
e #a1# é razoavelmente simples. Adicionamos elementos a #a# um por vez.
Se #a0# ou #a1# estão vazios, então adicionamos os próximos elementos do
outro array com elementos.
Caso contrário, pegamos o mínimo entre o próximo elemento em #a0# e
o próximo elemento em #a1# e o adicionamos a #a#:
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}
\pcodeimport{ods/Algorithms.merge(a0,a1,a)}
Note que o algoritmo 
#merge(a0,a1,a,c)# realiza até $#n#-1$
comparações antes de acabarem os elementos em #a0# ou #a1#.

Para entender o tempo de execução do mergesort, é mais fácil pensar
em termos de sua árvore de recursão. 
Considere por agora que #n# seja uma potência de dois, para que
 $#n#=2^{\log #n#}$ e $\log #n#$ seja um inteiro. 
Veja a \figref{mergesort-recursion}. Mergesort torna-se o problema de ordenar
#n# elementos em dois subproblemas, cada qual ordenando $#n#/2$ elementos.
Esse dois subproblemas se tornam em dois subproblemas cada, totalizando 
quatro subproblemas, cada um de tamanho
 $#n#/4$. Esses quatro subproblemas se tornam oito 
subproblemas, cada um de tamanho $#n#/8$, assim por diante.
Na base desse processo,
$#n#/2$ subproblemas, cada um de tamanho dois, são convertidos em #n# problemas,
cada de tamanho um. Para cada subproblema de tamanho
 $#n#/2^{i}$, o tempo gasto fazendo merge e copiando dados é 
 $O(#n#/2^i)$. Como há $2^i$
subproblemas de tamanho $#n#/2^i$, o tempo total gasto trabalhando em problemas de tamanho 
$2^i$, sem contar as chamadas recursivas é 
\[
       2^i\times O(#n#/2^i) = O(#n#) \enspace .
\]
Portanto a tempo total usado pelo mergesort é 
\[
   \sum_{i=0}^{\log #n#} O(#n#) = O(#n#\log #n#) \enspace .
\]

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{A árvore de recursão do mergesort.}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}

A prova do teorema a seguir é baseada na análise discutida anteriormente,
mas tem que ser um pouco cuidadosa para considerar os casos em que #n#
não sejam uma potência de 2.

\begin{thm}
O algoritmo \javaonly{#mergeSort(a,c)#}\cpponly{mergeSort(a)}\pcodeonly{merge\_sort(a)} roda em $O(#n#\log #n#)$ de tempo e faz até $#n#\log #n#$ comparações.
\end{thm}

\begin{proof}
Essa prova é por indução em
 $#n#$.  O caso base, em que $#n#=1$, é trivial;
  ao ser apresentado com um array de tamanho 0 ou 1, o algoritmo
  retorna sem fazer nenhuma comparação.
  Fazer o merge de duas listas ordenadas de tamanho total
$#n#$ requer no máximo $#n#-1$ comparações.
Seja 
$C(#n#)$ o número máximo de comparações realizadas por 
#mergeSort(a,c)# em um array #a# de tamanho #n#.  Se $#n#$ for par, então
aplicamos a hipótese indutiva para os dois subproblemas e obtemos
\begin{align*}
  C(#n#) 
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n# \enspace .
\end{align*}
O caso em que 
 $#n#$ é ímpar é ligeiramente mais complicado. Para esse caso,
 usamos duas desigualdades que são de fácil verificação:
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace , \eqlabel{log-ineq-a}
\end{equation}
para todo $x\ge 1$ e 
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace , \eqlabel{log-ineq-b}
\end{equation}
para todo $x\ge 1/2$.  A desigualdade~\myeqref{log-ineq-a} vem do fato que $\log(x)+1 = \log(2x)$ enquanto a \myeqref{log-ineq-b} segue do fato que $\log$ é uma função côncava. 
  Com essas ferramentas em mãos, para #n# ímpar,
\begin{align*}
  C(#n#) 
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2) 
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2) 
           - \log (#n#/2-1/2)) \\
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace . \qedhere
\end{align*} 
\end{proof}

\subsection{Quicksort}

\index{quicksort}%
O algoritmo \emph{quicksort} é outro exemplo clássico do paradigma de divisão e conquista.
Diferentemente do mergesort, que faz uma combinação após resolver os dois subproblemas,
o quicksort faz esse trabalho logo de uma vez.

Quicksort é simples de descrever: escolher um elemento aleatório chamado de \emph{pivot},
\index{elemento pivot}%
#x#, de #a#; particionar #a# em um conjunto de elementos menores que #x#, 
um conjunto de elementos igual a #x# e um conjunto de elementos maiores que #x#;
e, finalmente, ordenar recursivamente o primeiro e o terceiro conjunto dessa partição.
Um exemplo é mostrado na \figref{quicksort}.
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\pcodeimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{Um exemplo de execução de \javaonly{#quickSort(a,0,14,c)#}\cpponly{#quickSort(a,0,14)#}\pcodeonly{#quickSort(a,0,14)#}}
    \figlabel{quicksort}
  \end{center}
\end{figure}
Isso tudo é feito \emph{in place} (ou seja, no próprio espaço de memória do array),
então em vez de fazer cópias de subarrays sendo ordenados, o método
 #quickSort(a,i,n,c)# somente ordena o subarray 
$#a[i]#,\ldots,#a[i+n-1]#$.  Inicialmente, esse método é chamado com os argumentos que indicam o uso do array completo da seguinte forma
#quickSort(a,0,a.length,c)#.

No coração do algoritmo quicksort está o particionamento \emph{in place}.
Esse algoritmo, sem usar espaço extra, troca elementos em #a#
e computa índices #p# e #q# tal que
\[
   #a[i]# \begin{cases} 
         {}< #x# & \text{se $0\le #i#\le #p#$} \\
         {}= #x# & \text{se $#p#< #i# < #q#$} \\
         {}> #x# & \text{se $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
Esse particionamento, que é feito pelo laço 
#while# no código, funciona iterativamente aumentando #p# e 
decrementando #q# enquanto mantém a primeira e última dessas condições.
A cada passo, o elemento na posição
#j# é movido para a frente, à esquerda do onde está ou movido para trás.
Nos dois primeiros casos,
 #j# é incrementado, enquanto no último caso 
#j# não é incrementado pois o novo elemento na posição #j# ainda não foi processado.

O quicksort é profundamente ligado às árvores binárias aleatórias de busca
estudadas na \secref{rbst}.  De fato, se a entrada ao quicksort
consistir de #n# elementos distintos, então a árvore de recursão do quicksort 
será uma árvore binária de busca aleatória.

Para ver isso, lembre-se que ao construir uma árvore binária de busca aleatória
a primeira coisa a fazer é escolher um elemento aleatório #x# e 
torná-lo a raiz da árvore. Após isso, todo elemento será eventualmente
comparado a #x#, com elementos menores indo para a subárvore à esquerda
e elementos maiores à direita.

No quicksort, escolhemos um elemento aleatório #x# e imediatamente
comparamos todo o array a #x#, colocando os elementos menores que ele no começo
do array e elementos maiores no final do array.
O quicksort então recursivamente ordena o começo do array e o final do array,
enquanto a árvore binária de busca aleatória recursivamente insere os 
elementos menores na subárvores à esquerda da raiz e os maiores elementos 
na subárvore à direita da raiz.

Essa correspondência entre as árvores binárias de busca aleatórias e o
quicksort significa que podemos traduzir o 
\lemref{rbs} em uma afirmação sobre o quicksort: 

\begin{lem}\lemlabel{quicksort}
  Quando o quicksort é chamado para ordenar um array contendo os inteiros
  $0,\ldots,#n#-1$, o número esperado de comparações entre o elemento #i# 
  e o pivot é até $H_{#i#+1} + H_{#n#-#i#}$.
\end{lem}

Uma soma aproximada dos números harmônicos nos dá o seguinte teorema
sobre o tempo de execução do quicksort:

\begin{thm}\thmlabel{quicksort-i}
  Quando o quicksort é chamado para ordenar um array contendo 
  #n# elementos distintos, o número esperado de comparações realizadas
  é até $2#n#\ln #n# + O(#n#)$.
\end{thm}

\begin{proof}
Seja $T$ o número de comparações realizadas pelo quicksort para ordenar
#n# elementos distintos. Usando o \lemref{quicksort} e a linearidade da esperança, temos:
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

O \thmref{quicksort} descreve o caso em que os elementos 
sendo ordenados são todos distintos. 
Quando o array de entrada, #a#, contém elementos duplicados
o tempo esperado do quicksort não é pior e pode ser ainda melhor;
toda vez que um elemento duplicado #x# é escolhido como um pivot,
todas as ocorrências de #x# estão agrupadas e não participam 
de nenhum dos dois subproblemas que se seguem.

\begin{thm}\thmlabel{quicksort}
  O método \javaonly{#quickSort(a,c)#} \cpponly{#quickSort(a)#}
  \pcodeonly{#quickSort(a,c)#} roda em tempo esperado de $O(#n#\log #n#)$ 
  e o número esperado de comparações que ele
  realiza é $2#n#\ln #n# +O(#n#)$.
\end{thm}

\subsection{Heapsort}
\seclabel{heapsort}

\index{heap-sort}%
 O algoritmo \emph{heapsort} é outro algoritmo de ordenação que funciona \emph{in place}.

O heapsort usa as heaps binárias discutidas na \secref{binaryheap}.
Lembre-se que a estrutura de dados 
#BinaryHeap# representa uma heap usando um único array. 
O algoritmo heapsort converte o array de entrada #a# 
em uma heap e então repetidamente extrai o valor mínimo.

Mais especificamente, uma heap guarda #n# elementos em um array, #a#, em
em posições do array
$#a[0]#,\ldots,#a[n-1]#$ com o menor valor guardado na raiz, 
#a[0]#.  
Após transformar #a# em uma #BinaryHeap#, o algoritmo heapsort
repetidamente troca #a[0]# e #a[n-1]#, decrementa #n# e 
chama #trickleDown(0)# para que $#a[0]#,\ldots,#a[n-2]#$ volte a ser
uma representação válida de uma heap. Quando esse processo termina
(porque $#n#=0$) os elementos de #a# são guardados em ordem decrescente, 
então #a# é invertido para obter a forma ordenada final.
\footnote{O algoritmo poderia redefinir a função
#compare(x,y)# para que o heapsort guarde os elementos
diretamente na ordem crescente.} 
A \figref{heapsort} mostra um exemplo da execução do #heapSort(a,c)#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption[Heapsort]{Uma etapa da execução do #heapSort(a,c)#.
  A parte destacada do array já está ordenada. A parte não destacada é uma 
       #BinaryHeap#.
       Durante a próxima iteração, o elemento $5$ será colocado na posição $8$ do array.}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}
\pcodeimport{ods/Algorithms.heapSort(a)}

Uma sub-rotina chave do heapsort é construtor que transforma um
array desordenado #a# em uma heap. Seria fácil fazer isso usando
$O(#n#\log#n#)$ de tempo ao chamar repetidamente o método da #BinaryHeap#
#add(x)#, mas podemos fazer melhor ao usar um algoritmo 
bottom-up, que atua de baixo para cima no processo, ou seja, 
dos subarrays menores até os maiores. 

Lembre-se que, em uma heap binária, os filhos de 
#a[i]# são guardados nas posições 
#a[2i+1]# e #a[2i+2]#.  Isso implica que os elementos nas posições 
$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$ não têm filhos.
Em outras palavras, cada 
um dos elementos em $#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$ é uma subheap de tamanho 1.
Agora, trabalhando de trás para frente, podemos chamar
#trickleDown(i)# para cada 
$#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$. Isso funciona porque
ao chamarmos a operação
 #trickleDown(i)#, cada um dos dois filhos de #a[i]#
 são raízes de uma subheap, então chamar 
#trickleDown(i)# torna #a[i]# a raiz de sua própria subheap.
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

Um fato interessante sobre essa estratégia bottom-up é que ela é mais 
eficiente que chamar #add(x)# #n# vezes. Para ver isso, note que para 
$#n#/2$ elementos, não é necessária nenhuma operação, para $#n#/4$ elementos, chamamos
#trickleDown(i)# em uma subheap com raiz em #a[i]# e cuja altura é 1, para 
$#n#/8$ elementos, chamamos #trickleDown(i)# em uma subheap cuja 
altura é dois e assim segue. 
Como o trabalho feito por 
 #trickleDown(i)# é proporcional à altura da subheap enraizada em 
#a[i]#, isso significa que o trabalho total realizado é até 
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#) \enspace .
\]

A penúltima igualdade pode ser obtida ao reconhecer que a soma
$\sum_{i=1}^{\infty} i/2^{i}$ é igual, pela definição de valor esperado,
ao número esperado de lançamentos de uma moeda, incluindo no primeiro lançamento, cair
do lado cara e aplicar o \lemref{coin-tosses}.

O teorema a seguir descreve o desempenho do #heapSort(a,c)#.
\begin{thm}
  O método 
  #heapSort(a,c)# roda em $O(#n#\log #n#)$ de tempo e realiza no máximo 
  $2#n#\log #n# + O(#n#)$ comparações.
\end{thm}

\begin{proof}
  O algoritmo roda em três passos:
  (1)~transformar #a# em uma heap,
  (2)~repetidamente extrair o elemento mínimo de #a# e
  (3)~inverter os elementos em #a#.

Previamente argumentamos que o passo~1 leva 
$O(#n#)$ de tempo e realiza
$O(#n#)$ comparações.  Passo~3 leva $O(#n#)$ de tempo e não faz comparações. 
Passo~2 faz #n# chamadas a #trickleDown(0)#.
A #i#-ésima chamada opera em uma heap de tamanho
$#n#-i$ e faz até 
$2\log(#n#-i)$ comparações. Somando os custos do passo~2 sobre os possíveis valores de $i$ resulta em 
\[
   \sum_{i=0}^{#n#-i} 2\log(#n#-i) 
   \le \sum_{i=0}^{#n#-i} 2\log #n#
   =  2#n#\log #n#
\]
A soma do número de comparações realizadas em cada um dos três passos completa a prova.
\end{proof}

\subsection{Um Limitante Inferior para Ordenação}
\index{limitante inferior}
\index{limitante inferior para ordenação}
Estudamos até agora três algoritmos de ordenação baseados em comparação em funcionam em $O(#n#\log #n#)$ de tempo.  Nesse momento, podemos nos perguntar se existem algoritmos mais rápidos. 
A resposta a essa questão é não. Se as únicas operações permitidas nos elementos
de #a# são comparações, então nenhum algoritmo pode evitar fazer aproximadamente 
$#n#\log #n#$ comparações. Isso não é difícil provar, mas requer um pouco
de imaginação. No final das contas, isso vêm do fato que 
\[
   \log(#n#!) 
     = \log #n# + \log (#n#-1) + \dots + \log(1) 
     = #n#\log #n# - O(#n#)
    \enspace .
\]
(Fazer a prova desse fato é sugerido como o \excref{log-factorial}.)

Iniciaremos ao focar nossa atenção em algoritmos determinísticos como
mergesort e heapsort e em um valor fixo de #n#. Imagine 
que tal algoritmo está sendo usado para ordenar #n# elementos distintos.
A chave para provar o limitante inferior é observar que, para um algoritmo
determinístico com valor fixo de #n#, o primeiro par de elementos que
são comparados é sempre o mesmo.
Por exemplo, no
 #heapSort(a,c)#, quando #n# é par, a primeira chamada a
#trickleDown(i)# é com #i=n/2-1# e a primeira comparação está entre os 
elementos #a[n/2-1]# e #a[n-1]#.

Uma vez que todos os elementos de entrada são distintos, essa primeira 
comparação tem somente duas possíveis saídas. A segunda comparação
feita pelo algoritmo depende da saída da primeira comparação.
A terceira comparação depende dos resultados das primeiras duas e assim segue.
Desse jeito, qualquer algoritmo determinístico de ordenação baseado em 
comparações pode ser visto como uma \emph{árvore de comparações}.
\index{árvore de comparações}%
Cada nodo interno, #u#, dessa árvore é marcado com um par de índices #u.i# e
#u.j#. Se $#a[u.i]#<#a[u.j]#$, o algoritmo segue para a subárvore à esquerda.
caso contrário ela vai para a subárvore à direita.
Cada folha #w# dessa árvore é marcada com uma permutação
$#w.p[0]#,\ldots,#w.p[n-1]#$ de
$0,\ldots,#n#-1$.  
Essa permutação representa aquilo que é necessário para ordenar #a#
se a árvore de comparação atingir essa folha.
Isso é, 
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]# \enspace .
\]
Um exemplo de uma árvore de comparações para um array de tamanho #n=3#
é mostrado na \figref{comparison-tree}.
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption[Uma árvore de comparações]{Uma árvore de comparações para ordenar um array $#a[0]#,#a[1]#,#a[2]#$ de tamanho #n=3#.}
  \figlabel{comparison-tree}
\end{figure}

A árvore de comparações para um algoritmo de ordenação nos diz tudo sobre
o algoritmo. Ela nos diz exatamente a sequência de comparações que
será realizada para qualquer array de entrada, #a#, tendo #n# elementos distintos
e nos diz como o algoritmo irá permutar #a# para organizá-lo.

Consequentemente, a árvore de comparações deve ter pelo menos
$#n#!$ folhas;
senão, então há duas permutações distintas que levam à mesma folha; portanto,
o algoritmo não ordena corretamente pelo uma dessas permutações.

Por exemplo, a árvore de comparações na \figref{comparison-tree-2} tem somente 
$4< 3!=6$ folhas. Ao inspecionar essa árvore, vemos que os dois arrays de entrada
$3,1,2$ e $3,2,1$ levam à folha mais a direita.
Na entrada 
$3,1,2$ essa folha corretamente produz
$#a[1]#=1,#a[2]#=2,#a[0]#=3$.  Entretanto, na entrada
$3,2,1$, esse nodo incorretamente produz $#a[1]#=2,#a[2]#=1,#a[0]#=3$.
Essa discussão nos conduz ao limitante inferior para algoritmos baseados em 
comparação.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{Uma árvore de comparações que não ordena corretamente todas as permutações de entrada.}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
  Para qualquer algoritmo de ordenação determinístico baseado em comparação
  $\mathcal{A}$ e qualquer inteiro 
  $#n#\ge 1$, existe um array de entrada #a# de tamanho #n#
  tal que $\mathcal{A}$ usa pelo menos $\log(#n#!) =
  #n#\log#n#-O(#n#)$ comparações ao ordenar #a#.
\end{thm}

\begin{proof}
  De acordo com a discussão anterior, a árvore de comparações definida por
$\mathcal{A}$ deve ter
  pelo menos $#n#!$ folhas. Uma prova indutiva fácil mostra que
  qualquer árvore binária com $k$ folhas tem altura de pelo menos
  $\log k$.
  Portanto, a árvore de comparações para
  $\mathcal{A}$ tem uma folha, #w#, com profundidade de pelo menos
   $\log(#n#!)$ e existe um array de entradas #a#
  que leva a essa folha. O array de entrada #a# é uma entrada para qual
  $\mathcal{A}$ faz pelo menos $\log(#n#!)$ comparações.
\end{proof}

O \thmref{deterministic-sorting-lower-bound} trata de algoritmos
determinísticos como 
mergesort e heapsort, mas não nos diz nada sobre algoritmos randomizados
como o quicksort. 
Poderia um algoritmo randomizado usar um número de comparações menor que
o limitante inferior $\log(#n#!)$?
A resposta é, novamente, não. De novo, a forma de prová-lo é 
pensar diferentemente sobre o que um algoritmo randomizado é.

Na discussão a seguir, iremos assumir que nossas árvores de decisões
foram ``limpadas'' da seguinte forma: 
qualquer nodo que não pode ser alcançado por algum array de entrada #a# é removido.
Essa limpeza implica que a árvore tem exatamente 
$#n#!$ folhas. Ela tem pelo menos $#n#!$ folhas porque, caso contrário,
não ordenaria corretamente. Ela tem até $#p#!$ folhas pois
cada uma das possíveis permutações $#n#!$ de #n# elementos distintos segue
exatamente um caminho da raiz à folha na árvore de decisões.

Podemos imaginar que um algoritmo de ordenação randomizado
 $\mathcal{R}$ age como um algoritmo determinístico que recebe duas 
 entradas: o array de entradas #a# que devem ser ordenado e uma longa
 sequência números reais aleatórios $b=b_1,b_2,b_3,\ldots,b_m$ no intervalo $[0,1]$.
 Os números aleatórios provêem a randomização ao algoritmo. Quando o algoritmo
 quer lançar uma moeda ou fazer uma escolha aleatória, ele o faz usando algum
 elemento de $b$. Por exemplo, para computar o índice do primeiro pivot no 
 quicksort, o algoritmo poderia usar a fórmula $\lfloor n b_1\rfloor$.

 Agora, note que se fixarmos $B$ a uma sequência particular
 $\hat{b}$ então
$\mathcal{R}$ torna-se um algoritmo de ordenação determinístico, 
$\mathcal{R}(\hat{b})$, que tem uma árvore de comparações associada,
$\mathcal{T}(\hat{b})$.  A seguir, note que se selecionarmos 
 #a# para ser uma permutação aleatória de 
$\{1,\ldots,#n#\}$, então isso é equivalente a selecionar uma folha aleatória
 #w# dentre as $#n#!$ folhas de $\mathcal{T}(\hat{b})$.

O \excref{randomized-lower-bound} pede que você prove que, se selecionarmos 
uma folha aleatória de qualquer árvore binária com $k$ folhas, então
a profundidade esperada daquela folha é pelo menos 
$\log k$.  Portanto, o número esperado de comparações realizado pelo algoritmo (determinístico) 
$\mathcal{R}(\hat{b})$ quando passado um array de entrada contendo uma permutação aleatória de 
$\{1,\ldots,n\}$ é pelo menos $\log(#n#!)$.  Finalmente, note que isso é verdadeiro para toda escolha de 
$\hat{b}$, portanto isso vale mesmo para $\mathcal{R}$. Isso completa a prova do limitante inferior para algoritmos randomizados.

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
  Para qualquer inteiro 
  $n\ge 1$ e qualquer algoritmo de ordenação baseado em comparações (determinístico ou randomizado) 
   $\mathcal{A}$, o número esperado de comparações feito por
   $\mathcal{A}$ ao ordenar uma permutação aleatória 
  de $\{1,\ldots,n\}$ é pelo menos $\log(#n#!) = #n#\log#n#-O(#n#)$.
\end{thm}

\section{Counting Sort e Radix Sort}

Nesta seção estudamos dois algoritmos de ordenação que não são
baseados em comparação.
Especializados para ordenar valores inteiros baixos, esses algoritmos
se esquivam dos limites 
do \thmref{deterministic-sorting-lower-bound} ao usar (partes de) elementos em #a#
como índices para um array. 
Considere um comando da forma
\[
  #c[a[i]]# = 1 \enspace .
\]
Esse comando executa em tempo constante, mas tem 
#c.length# diferentes saídas, dependendo do valor de #a[i]#.  Isso
significa que a execução de um algoritmo que usa tal comando não pode ser
modelado como uma árvore binária.
É por essa razão que algoritmos desta seção são capazes de ordenar mais
rapidamente que algoritmos baseados em comparação.

\subsection{Counting Sort}

Suponha que temos um array de entrada #a# consistindo de #n# inteiros,
cada qual no intervalo
$0,\ldots,#k#-1$.  O algoritmo \emph{counting sort}
\index{counting-sort}%
ordena #a# usando um array auxiliar 
#c# de contadores.  Ele produz uma versão ordenada de #a# como um array auxiliar #b#.

A ideia por trás do counting sort é simples: para cada
$#i#\in\{0,\ldots,#k#-1\}$, conte o número de ocorrências de #i# em #a# 
e guarde essa contagem em #c[i]#. Agora, após a ordenação, a saída 
vai ser do tipo 
#c[0]# ocorrências de 0, seguida de #c[1]# ocorrências de 1, seguida de 
#c[2]# ocorrências de 2,\ldots, seguida de #c[k-1]# ocorrências de #k-1#.
O código que faz isso é bem elegante e sua execução é ilustrada na 
\figref{countingsort}:
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{A execução do counting sort em um array de tamanho $#n#=20$ que guarda inteiros $0,\ldots,#k#-1=9$.}
  \figlabel{countingsort}
\end{figure}

O primeiro laço #for# nesse código usa cada contador #c[i]# tal que 
ele conta o número de ocorrências de #i# em #a#.
Ao usar os valores de #a# como índices, esses contadores podem
ser computados em 
$O(#n#)$ de tempo com um único #for#.
Nesse ponto, poderíamos usar #c# para preencher o array de saída diretamente. 
Porém isso não funcionaria se os elementos de #a# tiverem dados associados. 
Portanto, gastamos um pouco de esforço extra para copiar os elementos de #a#
em #b#.

O próximo laço #for#, que leva $O(#k#)$ de tempo, computa uma soma acumulativa 
dos contadores tal que 
#c[i]# se torna o número de elementos em #a# que são menores que ou iguais a #i#.
Em particular, para todo 
$#i#\in\{0,\ldots,#k#-1\}$, o array de saída, #b#, terá 
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i# \enspace .
\]
Finalmente, o algoritmo percorre #a# de trás para frente para posicionar seus
elementos em ordem no array de saída #b#. Ao percorrer, o elemento
#a[i]=j# é colocado na posição #b[c[j]-1]# e o valor #c[j]# é decrementado.

\begin{thm}
  O método #countingSort(a,k)# ordena um array #a# contendo #n#
  inteiros no conjunto $\{0,\ldots,#k#-1\}$ em $O(#n#+#k#)$ de tempo.
\end{thm}

O algoritmo counting sort tem a interessante propriedade de ser \emph{estável};
\index{algoritmo de ordenação estável}%
ele preserva a ordem relativa de elementos iguais. Se dois elementos
#a[i]# e #a[j]# tem o mesmo valor  e $#i#<#j#$ então #a[i]# irá
aparecer antes de #a[j]# em #b#.  Isso será útil na seção a seguir.

\subsection{Radix sort}

Counting sort é muito eficiente para ordenar um array de inteiros quando
o tamanho, #n#, do array não é muito menor que o maior valor 
$#k#-1$ que aparece no array.  O algoritmo \emph{radix sort}
\index{radix sort}%
, que agora descreveremos, usa várias passadas do counting sort para 
permitir o uso seu uso em arrays cujo maior valor é bem maior que o tamanho do array.

O radix sort ordena inteiros 
de #w#-bits usando $#w#/#d#$ iterações do counting sort
para ordenar esses inteiros considerando somente #d# bits por iteração.
\footnote{Assumimos que #d# divide #w#, caso contrário podemos sempre aumentar
#w# para $#d#\lceil
#w#/#d#\rceil$.}  Mais precisamente, o radix sort primeiro ordena 
os inteiros usando seus #d# bits menos significativos e então seus próximos #d# bits significativos e assim por diante até, na última iteração, os inteiros estão ordenados pelos seus #d# bits mais significativos.

\codeimport{ods/Algorithms.radixSort(a)}
(Nesse código, a expressão #(a[i]>>d*p)&((1<<d)-1)# extrai o inteiro 
cuja representação binária é dada pelos bits 
$(#p#+1)#d#-1,\ldots,#p##d#$ de #a[i]#.)
Um exemplo dos passos desse algoritmo é mostrado na \figref{radixsort}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{Usando radix sort para ordenar inteiros de $#w#=8$-bits usando 4 iterações do counting sort em inteiros de $#d#=2$-bits.}
  \figlabel{radixsort}
\end{figure}

Esse algoritmo memorável ordena corretamente porque o counting sort
é um algoritmo de ordenação estável. 
Se $#x# < #y#$ são dois elementos de #a#, e o bit mais significativo 
em que #x# difere de #y# tem índice $r$, então #x# será posicionado 
antes de #y# durante a 
iteração $\lfloor r/#d#\rfloor$ e as iterações seguintes não alterarão
a ordem relativa entre #x# e #y#.

O radix sort usa #w/d# iterações do algoritmo counting sort.  Cada iteração
requer $O(#n#+2^{#d#})$ de tempo. Portanto, o desempenho do 
radix sort é dado pelo seguinte teorema. 
\begin{thm}\thmlabel{radix-sort}
  Para qualquer inteiro $#d#>0$, o método #radixSort(a,k)# pode ordenar um array 
  #a# contendo #n# inteiros de #w#-bits em $O((#w#/#d#)(#n#+2^{#d#}))$ de tempo.
\end{thm}

Alternativamente, se os elementos do array estiverem no intervalo
$\{0,\ldots,#n#^c-1\}$ e usarmos $#d#=\lceil\log#n#\rceil$, então obtemos 
a seguinte versão do \thmref{radix-sort}.
\begin{cor}\corlabel{radix-sort-poly}
O método #radixSort(a,k)# pode ordenar um array #a# contendo #n# valores inteiros no intervalo $\{0,\ldots,#n#^c-1\}$ em $O(c#n#)$ de tempo.
\end{cor}

\section{Discussão e Exercícios}

Ordenação é \emph{o} principal problema fundamental em Ciência da Computação 
e tem uma longa história.
Knuth \cite{k97v3} atribui o algoritmo merge sort a 
von~Neumann (1945).  O quicksort foi proposto por Hoare \cite{h61}.
O algoritmo heapsort original é de Williams \cite{w64}, mas a 
versão apresentada aqui (na qual a heap é construída bottom-up em 
$O(#n#)$ de tempo) foi proposta por Floyd \cite{f64}.  Limitantes 
inferiores para ordenação baseada em comparações parecem ser folclore.
A tabela a seguir resume o desempenho desses algoritmos baseados em comparações:

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
      & \multicolumn{3}{c|}{comparações} & in place  \\ \hline
    mergesort & $#n#\log #n#$ & &  pior caso& Não  \\
    quicksort & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & esperado & Sim\\
    heapsort & $2#n#\log #n#$ & ${}+ O(#n#)$ & pior caso & Sim \\ \hline
  \end{tabular}
\end{center}

Cada um desses algoritmos baseados em comparação tem suas vantagens e desvantagens.
O mergesort faz o menor número de comparações e não precisa de randomização.
Infelizmente, ele usa um array auxiliar durante sua fase de merge.
Alocar esse array pode ser caro e é um ponto de falha se a memória estiver escassa.
O quicksort funciona \emph{in place}
\index{algoritmo in place}%
e é o segundo melhor em termos do número de comparações, mas é 
randomizado então esse tempo de execução nem sempre é garantido.
Heapsort faz mais comparações mas não usa memória extra e é determinístico.
Existe um cenário em que o mergesort é o vencedor: ao ordenar uma lista ligada.
Nesse caso, o array auxiliar extra não é necessário; duas listas 
ligadas ordenadas são facilmente juntadas em uma única lista ligada 
ordenada usando manipulação de ponteiros (veja o \excref{list-merge-sort}).

Os algoritmos counting sort e radix sort aqui descritos foram propostos por
Seward \cite[Section~2.4.6]{s54}.  Porém, variantes do radix sort
têm sido usadas desde a década de 1920 para ordenar cartões 
perfurados usando máquinas de ordenação.
Essas máquinas podem ordenar uma pilha de cartões em duas pilhas 
usando a existência (ou não) de um furo em uma posição específica no cartão.
Ao repetir esse processo para diferentes posições de furos resulta em
uma implementação do radix sort.

Finalmente, notamos que counting sort e radix sort podem ser usados
para ordenar outros tipos de números além de inteiros não negativos.
Modificações do counting sort podem ordenar inteiros em qualquer intervalo
$\{a,\ldots,b\}$, em $O(#n#+b-a)$ de tempo.  De modo similar, radix sort 
pode ordenar inteiros no mesmo intervalo em 
 $O(#n#(\log_{#n#}(b-a))$ de tempo.  Finalmente, esses dois algoritmos
também podem ser usados para ordenar números em ponto flutuante no formato IEEE 754.
Isso porque o formato IEEE 754 é projetado para permitir a 
comparação de dois números ponto flutuantes ao comparar seus 
valores como se estivessem em uma representação binária de inteiros com bit de sinal. 
\cite{ieee754}.

\begin{exc}
  Ilustre a execução do mergesort e do heapsort em um array de entrada 
  contendo $1,7,4,6,2,8,3,5$.  Dê um exemplo simples de uma possível 
  execução do quicksort no mesmo array.
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
  Implemente uma versão do algoritmo mergesort que ordena uma
   #DLList# sem usar um array auxiliar (Veja o \excref{dllist-sort}).
\end{exc}

\begin{exc}
  Algumas implementações do 
  #quickSort(a,i,n,c)# usam #a[i]# como um pivot.
  Dê um exemplo de um array de entrada de tamanho #n# no
  qual tal implementação faria $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Algumas implementações de
   #quickSort(a,i,n,c)# usam #a[i+n/2]# como um pivot.
   Dê um exemplo de um array de entrada de tamanho #n# no
   qual tal implementação faria $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Mostre que, para qualquer implementação de
   #quickSort(a,i,n,c)# que escolha um pivot deterministicamente, 
   sem primeiro olhar os valores em 
   $#a[i]#,\ldots,#a[i+n-1]#$, existe um array de entrada de tamanho #n#
   que faz que essa implementação realize $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Projete um #Comparator#, #c#, que você poderia passar como um argumento ao
   #quickSort(a,i,n,c)# e que faria o quicksort usar 
  $\binom{#n#}{2}$ comparações.  (Dica: O seu comparador não precisa 
  olhar nos valores sendo comparados.) 
\end{exc}

\begin{exc}
Analise o número esperado de comparações feitos pelo quicksort 
um pouco mais cuidadosamente que a prova 
do \thmref{quicksort}.  Em particular, mostre que o número esperado
de comparações é $2#n#H_#n# -#n# + H_#n#$.
\end{exc}

\begin{exc}
  Descreva um array de entrada que faz o heapsort realizar pelo menos
  $2#n#\log #n#-O(#n#)$ comparações. Justifique sua resposta.
\end{exc}

\javaonly{
\begin{exc}
  A implementação do heapsort aqui descrita ordena os elementos 
  em ordem reversa e então inverte o array.
  Esse último passo poderia ser evitar ao definir
  um novo #Comparator# que nega os resultados 
  da entrada #Comparator#, #c#.  Explique porque isso não 
  seria seria uma boa otimização. (Dica: considere quantas 
  negações seriam necessárias para fazer 
  em relação a quanto tempo levar a inverter o array.)
\end{exc}
}

\begin{exc}
  Ache outro par de permutações de 
   $1,2,3$ que não são corretamente ordenados pela árvore de comparações 
  na \figref{comparison-tree-2}.
\end{exc}

\begin{exc}\exclabel{log-factorial}
  Prove que $\log #n#! = #n#\log #n#-O(#n#)$.
\end{exc}

\begin{exc}
  Prove que uma árvore binária com $k$ folhas tem altura de pelo menos $\log k$.
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
  Prove que, se escolhermos uma folha aleatória de uma árvore 
  binária com $k$ folhas, então a altura esperada dessa folha é, pelo menos, $\log k$.
%  (Hint: Use induction along with the inequality $(k_1/k)\log k_1 +
%  (k_2/k)\log k_2) \ge  \log k-1$, when $k_1+k_2=k$.)
\end{exc}

% This was a stupid exercise, since IEEE 754 floating point numbers 
% are correctly ordered when they are treated as signed magnitude integers
% [i.e., a sign bit followed by an integer ]
%\begin{exc}
%  Simple floating point numbers are numbers of the form $x\cdot10^{y}$,
%  where $0\le x\le 1$ and $y$ is an integer and each of $x$ and $y$ can
%  be represented by at most $k$ bits.  Describe a version of radix-sort
%  that can be used to sort simple floating point numbers.
%
%  Extend your version of radix sort so that it can handle signed values
%  of both $x$ and $y$ and implement a version of radix-sort that sorts
%  arrays of type #float#.
%\end{exc}

\begin{exc}
  A implementação do 
   #radixSort(a,k)# fornecida aqui funciona quando o array de entrada #a#
   contém somente inteiros 
  \cpponly{sem sinal}\javaonly{não negativos}
  .  \javaonly{Adapte essa implementação para que também funcione corretamente quando #a# contém inteiros negativos e não negativos.}\cpponly{Escreva uma versão que funcione corretamente para inteiros com sinal.}
\end{exc}

