\chapter{Algoritmos de Ordenação}
\chaplabel{sorting}

Este capítulo discute algoritmos para ordenação de um conjunto de #n# itens.
Esse pode ser um tópico estranho para um livro em estruturas de dados,
mas existem várias boas razões para incluí-lo aqui.
A razão mais óbvia é que dois desses algoritmos de ordenação (quicksort e heap-sort)
são intimamente relacionados com duas das estruturas de dados que são estudadas neste livro (árvores binárias de busca aleatórias e heaps).

A primeira parte deste capítulo discute algoritmos que ordenam usando somente
comparações e apresenta três algoritmos que rodam em 
$O(#n#\log #n#)$ de tempo.
Acontece que, esses três algoritmos são assintoticamente ótimos;
nenhum algoritmo que usa somente comparações pode evitar fazer
aproximadamente $#n#\log #n#$ comparações no pior caso e mesmo no caso médio.

Antes de continuar, devemos notar que quaisquer implementações do #SSet#
ou da #Queue# de prioridades apresentadas nos capítulos anteriores
também podem ser usadas para obter um algoritmo de ordenação 
 $O(#n#\log #n#)$.

Por exemplo, podemos ordenar #n# itens ao fazer #n#
 operações #add(x)# seguidas de 
 #n# operações #remove()# em uma #BinaryHeap# ou #MeldableHeap#. 
 Alternativamente, podemos usar #n# operações #add(x)# 
 em qualquer estrutura de dados de árvore de busca binária e então
 realizar uma travessia em ordem 
(\excref{tree-traversal}) para extrair os elementos ordenados 
Porém, nos dois casos teremos muitos gastos para construir uma estrutura
que nunca é completamente usada. Ordenação é um problema extremamente 
importante e vale a pena desenvolver métodos diretos que são o mais rápidos,
simples e econômicos em uso da memória possíveis.

A segunda parte deste capítulo mostra que, se for possível usar outras 
operações além de comparações, então tudo é possível. Realmente, 
ao usar indexação por arrays, é possível ordenar um conjunto de #n# inteiros
no intervalo $\{0,\ldots,#n#^c-1\}$ usando somente $O(c#n#)$ de time.


\section{Ordenação Baseada em Comparação}

\index{ordenação baseada em comparação}%
\index{sorting algorithm!comparison-based}%
\index{algoritmo de ordenação!baseado em comparação}%
Nesta seção, apresentamos três algoritmos: merge-sort, quicksort e heap-sort.
Cada um desses algoritmos recebe um array de entrada #a#
e ordena os elementos de #a# em ordem não-decrescente em 
$O(#n#\log #n#)$ de tempo (esperado).
Esses algoritmos são todos \emph{baseados em comparação}.
\javaonly{O segundo argumento, #c#, é um #Comparator#
\index{Comparator@#Comparator#}%
que implementa o método
#compare(a,b)#
\index{compare@#compare(a,b)#}%
.} Esses algoritmos dependem de qual tipo de dados está sendo ordenado;
a única operação que eles fazem nos dados é comparações
usando o método 
#compare(a,b)#. Lembre-se, de \secref{sset},
que #compare(a,b)# returna um valor negativo se $#a#<#b#$, um valor positivo
se $#a#>#b#$ e zero se $#a#=#b#$.

\subsection{Merge-Sort}
\seclabel{merge-sort}

\index{merge-sort}%
O algoritmo \emph{merge-sort} é um clássico exemplo do paradigma de divisão e conquista (recursiva):
\index{divide-and-conquer}%
\index{divisão e conquista}%
Se o tamanho de 
#a# for até 1, então #a# está ordenado então não fazemos nada. 
Caso contrário, dividimos #a# em duas metades,
$#a0#=#a[0]#,\ldots,#a[n/2-1]#$ e $#a1#=#a[n/2]#,\ldots,#a[n-1]#$.
Nós recusivamente ordemos #a0# e #a1# e então fazermos o merge dos (agora 
ordenados) #a0# e #a1# para ordenar nos array #a# completo:
\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
\pcodeimport{ods/Algorithms.mergeSort(a)}
Um exemplo é mostrado em \figref{merge-sort}.
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
  \caption[Merge sort]{A execução de #mergeSort(a,c)#}
  \figlabel{merge-sort}
\end{figure}

Comparada à ordaneção, fazer a junção (merge) de dois arrays ordenados #a0#
e #a1# é razoavelmente simples. Adicionamos elemento a #a# um por vez.
Se #a0# ou #a1# estão vazios, então adicionamos os próximos elementos do
outro array com elementos.
Por outro lado, pegamos o mínimo entre o próximo elemento em #a0# e
o próximo elemento em #a1# e adicioná-los a #a#:
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}
\pcodeimport{ods/Algorithms.merge(a0,a1,a)}
Note que o algoritmo 
#merge(a0,a1,a,c)# realiza em até  $#n#-1$
comparações antes de acabarem os elementos em #a0# ou #a1#.

Para entender o tempo de execução de merge-sort, é mais fácil pensar
em termos de sua árvore de recursão. 
Considere por agora que #n# seja uma potência de dois, tal que
 $#n#=2^{\log #n#}$ e $\log #n#$ é um inteiro. 
Veja \figref{mergesort-recursion}. Merge-sort torna-se o problema de ordenar
#n# elementos em dois problemas, cada qual ordenando $#n#/2$ elementos.
Esse dois subproblemas se tornam em dois problemas cada, a um total de
quatro subproblemas, cada um de tamanho
 $#n#/4$. Esses quatro subproblemas se torna oito 
subproblemas, cada um de tamanho $#n#/8$, assim por diante.
Na base nesse processo,
$#n#/2$ subproblemas, cada um de tamanho dois, são convertidos em #n# problemas,
cada de tamanho um. Para cada subproblema de tamanho
 $#n#/2^{i}$, o tempo gasto fazendo merge e copiando dados é 
 $O(#n#/2^i)$. Como há $2^i$
subproblemas de tamanho $#n#/2^i$, o tempo total gasto trabalhando em problemas de tamanho 
$2^i$, sem contar as chamadas recursivas é 
\[
       2^i\times O(#n#/2^i) = O(#n#) \enspace .
\]
Portanto a tempo total usado pelo merge-sort é 
\[
   \sum_{i=0}^{\log #n#} O(#n#) = O(#n#\log #n#) \enspace .
\]

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{A árvore de recusão do merge-sort.}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}

A prova do teorma a seguir é baseada na análise discutida anteriormente,
mas tem que ser um pouco cuidadosa para considerar os casos onde #n#
não é uma potência de 2.

\begin{thm}
O algoritmo \javaonly{#mergeSort(a,c)#}\cpponly{mergeSort(a)}\pcodeonly{merge\_sort(a)} roda em $O(#n#\log #n#)$ de tempo e faz até 
  $#n#\log #n#$ comparações.
\end{thm}

\begin{proof}
Essa prova é por indução em
 $#n#$.  O caso base, em que $#n#=1$, é trivial;
  ao ser apresentado com um array de tamano de 0 ou 1 o algoritmo
  retorna sem fazer nenhuma comparação.

  Fazer o merge de duas listas ordenadas de tamanho total
$#n#$ requer no máximo $#n#-1$ comparações.
Seja 
$C(#n#)$ o número máximo de comparações realizadas por 
#mergeSort(a,c)# em um array #a# de tamanho #n#.  Se $#n#$ é par, então
aplicamos a hipótese indutiva para os dois subproblemas e obter
\begin{align*}
  C(#n#) 
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n# \enspace .
\end{align*}
O caso em que 
 $#n#$ é ímpar é ligeiramente mais complicado. Para esse caso,
 usamos duas desigualdade que são de fácil verificação:
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace , \eqlabel{log-ineq-a}
\end{equation}
para todo $x\ge 1$ e 
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace , \eqlabel{log-ineq-b}
\end{equation}
para todo $x\ge 1/2$.  A desigualdade~\myeqref{log-ineq-a} vem do fato que $\log(x)+1 = \log(2x)$ enquanto \myeqref{log-ineq-b} segue do fato que $\log$ é uma função côncava. Com essas ferramentas em mãos, para #n# ímpar,
\begin{align*}
  C(#n#) 
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2) 
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2) 
           - \log (#n#/2-1/2)) \\
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace . \qedhere
\end{align*} 
\end{proof}

\subsection{Quicksort}

\index{quicksort}%
O algoritmo \emph{quicksort} é outro exemplo clássico do paradigma de divisão e conquista.
Diferentemente do merge-sort, que faz merging após resolver os dois subproblemas,
o quicksort faz esse trabalho logo de uma vez.

Quicksort é simples de descrever: escolher um elemento aleatório chamado de \emph{pivot},
\index{elemento pivot}%
#x#, de #a#; particionar #a# em um conjunto de elementos menores que #x#, um conjunto de elementos igual a #x# e um conjunto de elementos maiores que #x#;
e, finalmente, ordenar recursivamente o primeiro e o terceiro conjunto dessa partição.
Um exemplo é mostrado no
\figref{quicksort}.
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\pcodeimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{Um exemplo de execução de \javaonly{#quickSort(a,0,14,c)#}\cpponly{#quickSort(a,0,14)#}\pcodeonly{#quickSort(a,0,14)#}}
    \figlabel{quicksort}
  \end{center}
\end{figure}
Isso tudo é feito \emph{in place} (ou seja, no próprio espaço de memória do array),
então em vez de fazer cópias de subarrays sendo ordenados, o método
 #quickSort(a,i,n,c)# somente ordena o subarray 
$#a[i]#,\ldots,#a[i+n-1]#$.  Inicialmente, esse método é invocado com os argumentos
#quickSort(a,0,a.length,c)#.

No coração do algoritmo quicksort está o particionamento \emph{in place}.
Esse algoritmo, sem usar espaço extra, troca elementos em #a#
e computa índices #p# e #q# tal que
\[
   #a[i]# \begin{cases} 
         {}< #x# & \text{se $0\le #i#\le #p#$} \\
         {}= #x# & \text{se $#p#< #i# < #q#$} \\
         {}> #x# & \text{se $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
Esse particionamento, que é feito pelo laço 
#while# no código, funciona iterativamente aumento #p# e decrementando #q# enquanto mantém a primeira e últma dessas condições.
A cada passo, o elemento na posição
#j# é ou movido para a frente, à esquerda do onde está, ou movido para trás.
Nos dois primeiros casos,
 #j# é incrementado, enquanto no último caso 
#j# não é incrementado pois o novo elemento na posição #j# ainda não foi processado.

O quicksort é muito ligado às árvores binárias aleatórias de busca
estudadas em 
\secref{rbst}.  De fato, se a entrada ao quicksort
consiste de #n# elementos distintos, então a árvore de recursão do quicksort 
é uma árvore binária de busca aleatória

Para ver isso, lembre-se que ao construir uma árvore binária de busca aleatória
a primeira coisa a fazer é escolher um elemento aleatório #x# e 
torná-lo a raiz da árvore. Após isso, todo elemento será eventualmente
comparado a #x#, com elementos menores indo para a subárvore à esquerda
e elementos maiores à direita.

No quicksort, escolhemos um elemento aleatório #x# e imediatamente
comparamos todo o array a #x#, colocando os elementos menores no começo
do array e elementos maiores no final do array.
O quicksort então recursivamente ordena o começo do array e o final do array,
enquanto a árvore binária de busca aleatória recursivamente insere os 
elementos menores na subáres à esquerda da raiz e os maiores elementos 
na subárvore à direita da raiz.

Ess correspondência entre as árvores binárias de busca aleatórias e o
quicksort significa que podemos traduzir
\lemref{rbs} para uma afirmação sobre o quicksort: 

\begin{lem}\lemlabel{quicksort}
  Quando o quicksort é chamado para ordenar um array contendo os inteiros
  $0,\ldots,#n#-1$, o número esperado de vezes que o elemento #i# é comparado 
  ao elemento pivot é até $H_{#i#+1} + H_{#n#-#i#}$.
\end{lem}

A soma rápida dos números harmônicos nos dá o seguinte teorema
sobre o tempo de execução do quicksort:

\begin{thm}\thmlabel{quicksort-i}
  Quando o quicksort é chamado para ordenar um array contendo 
  #n# elementos distintos, o número esperado de comparações realizadas
  é até $2#n#\ln #n# + O(#n#)$.
\end{thm}

\begin{proof}
Seja $T$ o número de comparações realizadas pelo quicksort ao ordenar 
#n# elementos distintos. Usando \lemref{quicksort} e a linearidade da esperança, temos:
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

\thmref{quicksort} descreve o caso em que os elementos sendo ordenados são todos distintos. 
Guando o array de entrada, #a#, contém elementos duplicados
o tempo esperado do quicksort não é pior e pode ser ainda melhor;
toda vez que um elemento duplicado #x# é escolhido como um pivot,
todas as ocorrências de #x# são agrupadas e não participam 
de nenhum dos dois subproblemas que se seguem.

\begin{thm}\thmlabel{quicksort}
  O método \javaonly{#quickSort(a,c)#} \cpponly{#quickSort(a)#}
  \pcodeonly{#quickSort(a,c)#} roda em tempo esperado de $O(#n#\log #n#)$ 
  e o número esperado de comparações que 
  realiza é em até 
  $2#n#\ln #n# +O(#n#)$.
\end{thm}

\subsection{Heap sort}
\seclabel{heapsort}

\index{heap-sort}%
 O algoritmo \emph{heap sort} é outro algoritmo de ordenação que funciona \emph{in place}.

O Heap sort usa as heaps binárias discutidas em \secref{binaryheap}.
Lembre-se que a estrutura de dados 
#BinaryHeap# representa uma heap usando um único array. 
O algoritmo heap sort converte o array de entrada #a# 
em uma heap e então repetidamente extrai o valor mínimo.

Mais especificamente, uma heap guarda #n# elementos em um array, #a#, em
em posições do arrau 
$#a[0]#,\ldots,#a[n-1]#$ com o menor valor guardado na raiz, 
#a[0]#.  
Após transformar #a# em uma 
 #BinaryHeap#, o algoritmo heap sort
algorithm repetidamente troca #a[0]# e #a[n-1]#, decrementa #n#, e 
chama #trickleDown(0)# tal que $#a[0]#,\ldots,#a[n-2]#$ volte a ser
uma representação válida de uma heap. Quando esse processo termina
(porque $#n#=0$) os elementos de #a# são guardados em ordem decrescente, 
então #a# é invertido para obter a forma ordenada final.
\footnote{O algoritmo poderia como opção redefinir a função
#compare(x,y)# para que o heap sort guarde os elementos
diretamente na ordem crescente.} 
\figref{heapsort} mostra um exemplo da execução do #heapSort(a,c)#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption[Heap sort]{Uma etapa da execução do #heapSort(a,c)#.
  A parte sombreada do array já está ordenada. A parte não sombreada é uma 
       #BinaryHeap#.
       Durante a próxima iteração, o elemento $5$ será colocado na posição $8$ do array.}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}
\pcodeimport{ods/Algorithms.heapSort(a)}

Uma subrotina chave no heap sort é construtor que transforma um
array desordenado #a# em uma heap. Seria fácil fazer isso usando
$O(#n#\log#n#)$ de tempo ao chamar repetidamente o método da #BinaryHeap#
#add(x)#, mas podemos faer melhor ao usar um algoritmo bottom-up, que atua de baixo para cima no processo, ou seja, dos subarrays menores até os maiores. 

Lembre-se que, em uma heap binária, os filhos de 
#a[i]# são guardados nas posições 
#a[2i+1]# e #a[2i+2]#.  Isso implica que os elementos 
$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$ não tem filhos.
Em outras palavras, cada 
um dos elementos em $#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$ é uma subheap de tamanho 1.
Agora, trabalhando de trás para frente, podemos chamar
#trickleDown(i)# para cada 
$#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$. Isso funciona porque
ao chamarmos 
 #trickleDown(i)#, cada um dos dois filhos de #a[i]#
 são raizes de uma subheap, então chamar 
#trickleDown(i)# torna #a[i]# a raiz de sua própria subheap.
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

Um fato interessante sobre essa estratégia bottom-up é que ela é mais 
eficiente que chamar
 #add(x)# #n# vezes. Para ver isso, note que para 
$#n#/2$ elementos, não é necessária nenhum operação, para $#n#/4$ elementos, chamamos
#trickleDown(i)# em uma subheap com raiz em #a[i]# e cuja altura é 1, para 
$#n#/8$ elementos, chamamos #trickleDown(i)# em uma subheap cuja altura é dois, e assim segue. 
Como o trabalho feito por 
 #trickleDown(i)# é proporcional à altura da subheap enraizada em 
#a[i]#, isso significa que o trabalho total realizado é até 
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#) \enspace .
\]

A penúltima igualdade pode ser obtida ao reconhecer que a soma
$\sum_{i=1}^{\infty} i/2^{i}$ é igual, pela definição de valos esperado,
ao número esperado de lançamentos de uma moeda, incluindo a primeira vez, cair
do lado cara e aplicar \lemref{coin-tosses}.

O teorema a seguir descreve o desempenho de #heapSort(a,c)#.
\begin{thm}
  O método 
  #heapSort(a,c)# roda em $O(#n#\log #n#)$ de tempo e realiza no máximo 
  $2#n#\log #n# + O(#n#)$ comparações.
\end{thm}

\begin{proof}
  O algoritmo roda em três passos:
  (1)~transformar #a# em uma heap,
  (2)~repetidamente extrair o elemento mínimo de #a# e
  (3)~inverter os elementos em #a#.

Previamente argumentamos que o passo~1 leva 
$O(#n#)$ de tempo e realiza
$O(#n#)$ comparações.  Passo~3 leva $O(#n#)$ de tempo e não faz comparações. 
Passo~2 faz #n# chamadas a #trickleDown(0)#.
A #i#-ésima chamada opera em uma heap de tamanho
$#n#-i$ e faz até 
$2\log(#n#-i)$ comparações. Somando os custos do Passo~2 sobre os possíveis valores de $i$ resulta em 
\[
   \sum_{i=0}^{#n#-i} 2\log(#n#-i) 
   \le \sum_{i=0}^{#n#-i} 2\log #n#
   =  2#n#\log #n#
\]
Somando o número de comparações realizadas em cada um dos três passos completa a prova.
\end{proof}

\subsection{Um Limitante Inferior para Ordenação}
\index{limitante inferior}
\index{lower-bound}%
\index{sorting lower-bound}%
\index{limitante inferior para ordenação}
Estudamos até agora três algoritmos de ordenação baseados em comparação em funcionam em $O(#n#\log #n#)$ de tempo.  Nesse momento, podemos nos perguntar se existem algoritmos mais rápidos. 
A resposta a essa questão é não. Se as únicas operações permitidas nos elementos
de #a# são comparações, então nenhum algoritmo pode evitar fazer aproximadamente 
$#n#\log #n#$ comparações. Isso não é difícil provar, mas requer um pouc
de imaginação. No final das contas, isso vêm do fato que 
\[
   \log(#n#!) 
     = \log #n# + \log (#n#-1) + \dots + \log(1) 
     = #n#\log #n# - O(#n#)
    \enspace .
\]
(Fazer a prova desse fato é sugerida como \excref{log-factorial}.)

Iniciaremos ao focar nossa atenção em algoritmos determinísticos como
merge sort e heap sort e em um valor fixo de #n#. Imagine 
que tal algoritmo está sendo usado para ordenar #n# elementos distintos.
A chave para provar o limitante inferior é observar que, para um algoritmo
determinístico com valor fixo de #n#, o primeiro par de elementos que
são comparados é sempre o mesmo.
Por exemplo, no
 #heapSort(a,c)#, quando #n# é par, a primeira chamada a
#trickleDown(i)# é com #i=n/2-1# e a primeira comparação está entre os 
elementos #a[n/2-1]# e #a[n-1]#.

Uma vez que todos os elementos de entrada são distintos, essa primeira 
comparação tem somente duas possíveis saídas. A segunda comparação
feita pelo algoritmo depende na saída da primeira comparação.
A terceira comparação depende nos resultados das primeiras duas e assim segue.
Desse jeito, qualquer algoritmo determinístico de ordenação baseado em 
comparações pode ser visto como uma \emph{árvore de comparações}.
\index{árvore de comparações}%
Cada nodo interno, #u#, dessa árvore é marcada com um par de índices #u.i# e
#u.j#. Se 
$#a[u.i]#<#a[u.j]#$ o algoritmo segue para a subárvore à esquerda.
caso contrária ela vai para a subárvore à direita.
Cada folha #w# dessa árvore é marcada com uma permutação
$#w.p[0]#,\ldots,#w.p[n-1]#$ de
$0,\ldots,#n#-1$.  
Essa permutação representa aquilo que é necessário para ordenar #a#
se a árvore de comparação atinge esse folha.
Isso é, 
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]# \enspace .
\]
Um exemplo de uma árvore de comparações para um array de tamanho #n=3#
é mostrado em 
\figref{comparison-tree}.
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption[Uma árvore de comparações]{Uma árvore de comparações para ordenar um array $#a[0]#,#a[1]#,#a[2]#$ de tamanho #n=3#.}
  \figlabel{comparison-tree}
\end{figure}

A árvore de comparações para um algoritmo de ordenação nos diz tudo sobre
o algoritmo. Ela nos diz exatamente a sequência de comparações que
será realizada para qualquer array de entrada, #a#, tendo #n# elementos distintos
e nos diz como o algoritmo irá reordenar #a# para organizá-lo.

Consequentemente, a árvore de comparação deve ter pelo menos
$#n#!$ folhas;
senão, então há duas permutações distintas que levam à mesma folha; portanto,
o algoritmo não ordena corretamente pelo uma dessas permutações.

Por exemplo, a árvore de comparações em
\figref{comparison-tree-2} tem somente 
$4< 3!=6$ folhas. Ao inspecionar essa árvore, vemos que os dois arrays de entrada
$3,1,2$ e $3,2,1$ ambos levam à folha mais a direita.
Na entrada 
$3,1,2$ essa folha corretamente produz
$#a[1]#=1,#a[2]#=2,#a[0]#=3$.  Entretanto, na entrada
$3,2,1$, esse nodo incorretamente produz $#a[1]#=2,#a[2]#=1,#a[0]#=3$.
Essa discussão nos conduz ao limitante inferior para algoritmos baseados em 
comparação.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{Uma árvore de comparações que não ordena corretamente todas as permutações de entrada.}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
  Para qualquer algoritmo de ordenação determinístico baseado em comparação
  $\mathcal{A}$ e qualquer inteiro 
  $#n#\ge 1$, existe um array de entrada #a# de tamanho #n#
  tal que $\mathcal{A}$ usa pelo menos $\log(#n#!) =
  #n#\log#n#-O(#n#)$ comparações ao ordenar #a#.
\end{thm}

\begin{proof}
  De acordo com a discussão anterior, a árvore de comparações definida por
$\mathcal{A}$ deve ter
  pelo menos $#n#!$ folhas. Uma prova indutiva fácil mostra que
  qualquer árvore binária com $k$ folhas tem altura de pelo menos
  $\log k$.
  Portante, a árvore de comparações para
  $\mathcal{A}$ tem uma folha, #w#, com profundidade de pelo menos
   $\log(#n#!)$ e existe um array de entradas #a#
  que leva a essa folha. O array de entrada #a# é uma entrada para qual
  $\mathcal{A}$ faz pelo menos $\log(#n#!)$ comparações.
\end{proof}

O \thmref{deterministic-sorting-lower-bound} trata de algoritmos
determinísticos como 
merge-sort e heap-sort, mas não nos diz nada sobre algoritmos randomizados
como o quicksort. 
Poderia um algoritmo randomizado usar um número de comparações menor que
o limitante inferior $\log(#n#!)$?
A resposta é, novamente, não. De novo, a forma de prová-lo é 
pensar diferentemente sobre o que um algoritmo randomizado é.

Na discussão a seguir, iremos assumir que nossas árvores de decisões
foram ``limpadas'' da seguinte forma: 
qualquer nodo que não pode ser alcançado por algum array de entrada #a# é removido.
Essa limpeza implica que a árvore tem exatamente 
$#n#!$ folhas. Ela tem pelo menos $#n#!$ folhas porque, caso contrário,
não ordenaria corretamente. Ela tem até $#p#!$ folhas pois
cada uma das possíveis permutações $#n#!$ de #n# elementos distintos segue
exatamente um caminho da raiz à folha na árvore de decisões.

Podemos imaginar que um algoritmo de ordenação randomizado
 $\mathcal{R}$ age como um algoritmo determinístico que recebe duas 
 entradas: o array de entradas #a# que devem ser ordenado e uma longa
 sequência números reais aleatórios $b=b_1,b_2,b_3,\ldots,b_m$ no intervalo $[0,1]$.
 Os números aleatórios provêem a randomização ao algoritmo. Quando o algoritmo
 quer lançar uma moeda ou fazer uma escolha aleatória, ele o faz usando algum
 elemento de $b$. Por exemplo, para computar o índice do primeiro pivot no 
 quicksort, o algoritmo poderia usar a fórmula $\lfloor n b_1\rfloor$.

 Agora, note que se fixarmos $B$ a uma sequência particular
 $\hat{b}$ então
$\mathcal{R}$ torna-se um algoritmo de ordenação determinístico, 
$\mathcal{R}(\hat{b})$, que tem uma árvore de comparações associada,
$\mathcal{T}(\hat{b})$.  A seguir, note que se selecionarmos 
 #a# para ser uma permutação aleatória de 
$\{1,\ldots,#n#\}$, então isso é equivalente a selecionar uma folha aleatória
 #w# dentre as $#n#!$ folhas de $\mathcal{T}(\hat{b})$.

\excref{randomized-lower-bound} pede que você prove que, se selecionarmos 
uma folha aleatória de qualquer árvore binária com $k$ folhas, então
a profundidade esperada daquela folha é pelo menos 
$\log k$.  Portanto, o número esperado de comparações realizado pelo algoritmo (determinístico) 
$\mathcal{R}(\hat{b})$ quando passado um array de entrada contendo uma permutação aleatória de 
$\{1,\ldots,n\}$ é pelo menos $\log(#n#!)$.  Finalmente, note que isso é verdadeiro para toda escolha de 
$\hat{b}$, portanto isso vale mesmo para $\mathcal{R}$. Isso completa a prova do limitante inferior para algoritmos randomizados.

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
  Para qualquer inteiro 
  $n\ge 1$ e qualquer algoritmo de ordenação baseado em comparações (determinístico ou randomizado) 
   $\mathcal{A}$, o número esperado de comparações feito por
   $\mathcal{A}$ ao ordenar uma permutação aleatória 
  de $\{1,\ldots,n\}$ é pelo menos $\log(#n#!) = #n#\log#n#-O(#n#)$.
\end{thm}

\section{Counting Sort e Radix Sort}

Nesta seção estudamos dois algoritmos de ordeanção que não são
baseados em comparação.
Especializados para ordenar valores inteiros baixos, esses algoritmos
se esquivam dos limites 
do \thmref{deterministic-sorting-lower-bound} ao usar (partes de) elementos em #a#
como índices para um array. 
Considere um comando da forma
\[
  #c[a[i]]# = 1 \enspace .
\]
Esse comando executa em tempo constante, mas tem 
#c.length# diferentes saídas, dependendo do valor de #a[i]#.  Isso
significa que a execução de um algoritmo que usa tal comando não pode ser
modelado como uma árvore binária.
É por essa razão que algoritmos desta seção são capazes de ordenar mais
rapidamente que algoritmos baseados em comparação.

\subsection{Counting Sort}

Suponha que temos um array de entrada #a# consistindo de #n# inteiros,
cada qual no intervalo
$0,\ldots,#k#-1$.  O algoritmo \emph{counting sort}
\index{counting-sort}%
ordena #a# usando um array auxiliar 
#c# de contadores.  Ele produz uma versão ordenada de #a# como um array auxiliar #b#.

A ideia por trás do counting sort é simples: para cada
$#i#\in\{0,\ldots,#k#-1\}$, conte o número de ocorrências de #i# em #a# 
e guarde essa contagem em #c[i]#. Agora, após a ordenação, a saída 
vai ser do tipo 
#c[0]# ocorrências de 0, seguida de #c[1]# ocorrências de 1, seguida de 
#c[2]# ocorrências de 2,\ldots, seguida de #c[k-1]# ocorrências de #k-1#.
O código que faz isso é bem elegante e sua execução é ilustrada em 
\figref{countingsort}:
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{A execução do counting sort em um array de tamanho $#n#=20$ que guarda inteiros $0,\ldots,#k#-1=9$.}
  \figlabel{countingsort}
\end{figure}

O primeiro laço #for# nesse código usa cada contador #c[i]# tal que 
ele conta o número de ocorrências de #i# em #a#.
Ao usar os valores de #a# como índices, esses contadores podem
ser computados em 
$O(#n#)$ de tempo com um único #for#.
Nesse ponto, poderíamos usar #c# para preencher o array de saída diretamente. 
Porém isso não funcionaria se os elementos de #a# tem dados associados. 
Portanto, gastamos um pouco de esforço extra para copiar os elementos de #a#
em #b#.

O próximo laço #for#, que leva $O(#k#)$ de tempo, computa uma soma acumulativa dos contadores tal que 
#c[i]# se torna o número de elementos em #a# que são menores que ou iguais a #i#.
Em particular, para todo 
$#i#\in\{0,\ldots,#k#-1\}$, o array de saída, #b#, terá 
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i# \enspace .
\]
Finalmente, o algoritmo percorre #a# de trás para frente para posicionar seus
elementos em ordem no array de saída #b#. Ao percorrer, o elemento
#a[i]=j# é colocado na posição #b[c[j]-1]# e o valor #c[j]# é decrementado.

\begin{thm}
  O método 
  #countingSort(a,k)# pode ordenar um array #a# contendo #n#
  inteiros no conjunto $\{0,\ldots,#k#-1\}$ em $O(#n#+#k#)$ de tempo.
\end{thm}

O algoritmo counting sort tem a interessante propriedade de ser \emph{estável};
\index{algoritmo de ordenação estável}%
ele preserva a ordem relativa de elementos iguais. Se dois elementos
#a[i]# e #a[j]# tem o mesmo valor  e $#i#<#j#$ então #a[i]# irá
aparecer antes de #a[j]# em #b#.  Isso será útil na seção a seguir.

\subsection{Radix sort}

Counting sort é muito eficiente para ordenar um array de inteiros quando
o tamanho, #n#, do array não é muito menor que o maior valor 
$#k#-1$ que aparece no array.  O algoritmo \emph{radix sort}
\index{radix sort}%
, que agora descreveremos, usa várias passadas do counting sort para 
permitir o uso seu uso em arrays cujo maior valor é bem maior que o tamanho do array.

O radix sort ordena inteiros 
de #w#-bits usando $#w#/#d#$ iterações do counting sort
para ordenar esses inteiros considerando somente #d# bits por iteração.
\footnote{Assumimos que #d# divide #w#, caso contrário podemos sempre aumentar
#w# para 
$#d#\lceil
#w#/#d#\rceil$.}  Mais precisamente, o radix sort primeiro ordena 
os inteiros usando seus #d# bits menos significativos e então seus próximos #d# bits significativos e assim por diante até, na última iteração, os inteiros estão ordenados pelos seus #d# bits mais significativos.

\codeimport{ods/Algorithms.radixSort(a)}
(Nesse código, a expressão #(a[i]>>d*p)&((1<<d)-1)# extrai o inteiro 
cuja representação binária é dada pelos bits 
$(#p#+1)#d#-1,\ldots,#p##d#$ de #a[i]#.)
Um exemplo dos passos desse algoritmo é mostrado em 
 \figref{radixsort}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{Usando radix sort para ordenar inteiros de $#w#=8$-bits usando 4 iterações do counting sort em inteiros de $#d#=2$-bits.}
  \figlabel{radixsort}
\end{figure}

Esse algoritmo memorável ordena corretamente porque counting sort
é um algoritmo de ordenação estável. 
Se $#x# < #y#$ são dois elementos de #a#, e o bit mais significativo em que #x# difere de #y# tem índice $r$, então #x# será posicionado antes de #y# durante a 
iteração $\lfloor r/#d#\rfloor$ e as iterações seguintes não alterarão
a ordem relativa entre #x# e #y#.

Radix sort usar #w/d# iterações de counting sort.  Cada iteração
requer 
$O(#n#+2^{#d#})$ de tempo. Portanto, o desempenho do radix sort é dado pelo seguinte teorema. 
\begin{thm}\thmlabel{radix-sort}
  Para qualquer inteiro $#d#>0$, o método #radixSort(a,k)# pode ordenar um array 
  #a# contendo #n# inteiros de #w#-bits em $O((#w#/#d#)(#n#+2^{#d#}))$ de tempo.
\end{thm}

Alternativamente, se os elementos do array estarem no intervalo
$\{0,\ldots,#n#^c-1\}$, e usarmos $#d#=\lceil\log#n#\rceil$ obtemos 
a seguinte versão do 
 \thmref{radix-sort}.
\begin{cor}\corlabel{radix-sort-poly}
O método #radixSort(a,k)# pode ordenar um array #a# contendo #n# valores inteiros no intervalo $\{0,\ldots,#n#^c-1\}$ em $O(c#n#)$ de tempo.
\end{cor}

\section{Discussão e Exercícios}

Ordenação é \emph{o} problema fundamental em Ciência da Computação e tem uma longa história.
Knuth \cite{k97v3} atribui o algoritmo merge sort a 
von~Neumann (1945).  Quicksort foi proposto por Hoare \cite{h61}.
O algoritmo heap sort original é de Williams \cite{w64}, mas a 
versão apresentada aqui (na qual a heap é construída bottom-up em 
$O(#n#)$ de tempo) foi proposta por Floyd \cite{f64}.  Limitantes inferiores para ordenação baseada em comparações parecem ser folklore.
A tabela a seguir resume o desempenho desses algoritmos baseados em comparações:

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
      & \multicolumn{3}{c|}{comparações} & in place  \\ \hline
    Merge sort & $#n#\log #n#$ & &  pior caso& Não  \\
    Quicksort & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & esperado & Sim\\
    Heap sort & $2#n#\log #n#$ & ${}+ O(#n#)$ & pior caso & Sim \\ \hline
  \end{tabular}
\end{center}

Cada um desses algoritmos baseados em comparação tem suas vantagens e desvantagens.
Merge sort faz o menor número de comparações e não precisa de randomização.
Infelizmente, ele usa um array auxiliar durante sua fase de merge.
Alocar esse array pode ser caro e é um ponto de falha se a memória estiver escassa.
Quicksort funciona \emph{in place}
\index{algoritmo in place}%
e é o segundo melhor em termos do número de comparações, mas é randomizado então esse tempo de execução nem sempre é garantido.
Heap sort faz mais comparações mas não usa memória extra e é determinístico.
Existe um cenário em que o merge sort é o vencedor: ao ordenar uma lista ligada.
Nesse caso, o array auxiliar extra não é necessário; duas listas ligadas ordenadas são fácilmente juntadas em uma única lista ligada ordenada usando manipulação de ponteiros (ver 
\excref{list-merge-sort}).

Os algoritmos counting sort e radix sort aqui foram apresentador por
Seward \cite[Section~2.4.6]{s54}.  Porém, variantes do radix sort
têm sido usados desde a década de 1920 para ordenar cartões perfurados usando máquinas de ordenação.
Essas máquinas podem ordenar uma pilha de cartões em duas pilhas usando a existência (ou não) de um furo em uma posição específica no cartão.
Ao repetir esse processo para diferentes posições de furos resulta em
uma implementação do radix sort.

Finalmente, notamos que counting sort e radix sort podem ser usados
para ordenar outros tipos de números além dos inteiros não negativos.
Modificações do counting sort podem ordenar inteiros em qualquer intervalo
$\{a,\ldots,b\}$, em $O(#n#+b-a)$ de tempo.  De modo similar, radix sort pode ordenar inteiros no mesmo intervalo em 
 $O(#n#(\log_{#n#}(b-a))$ de tempo.  Finalmente, esses dois algoritmos
também podem ser usados para ordenar números em ponto flutunte no formato IEEE 754.
Isso porque o formato IEEE 754 é projetado para permitir a comparação de dois números ponto flutuantes ao comparar seus valores como se estivessem em uma representação binária de inteiros com bit de sinal. 
\cite{ieee754}.

\begin{exc}
  Ilustre a execução do merge sort e do heap sort em um array de entrada 
  contendo $1,7,4,6,2,8,3,5$.  Dê umo exemplo simples de uma possível execução de quicksort no mesmo array.
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
  Implementar uma versão do algoritmo merge sort que ordena uma
   #DLList# sem usar um array auxiliar (Ver \excref{dllist-sort}).
\end{exc}

\begin{exc}
  Algumas implementações do 
  #quickSort(a,i,n,c)# sempre usa #a[i]# como um pivot.
  Dê um exemplo de um array de entrada de tamanho #n# no
  qual tal implementação faria $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Algumas implementações de
   #quickSort(a,i,n,c)# sempre usa #a[i+n/2]# como um pivot.
   Dê um exemplo de um array de entrada de tamanho #n# no
   qual tal implementação faria $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Mostre que, para qualquer implementação de
   #quickSort(a,i,n,c)# que escolha um pivot deterministicamente, sem primeiro
   olhar os valores em 
   $#a[i]#,\ldots,#a[i+n-1]#$, existe um array de entrada de tamanho #n#
   que faz que essa implementação realize
   $\binom{#n#}{2}$ comparações.
\end{exc}

\begin{exc}
  Projete um 
  #Comparator#, #c#, que você poderia passar como um argumento ao
   #quickSort(a,i,n,c)# e que faria o quicksort usar 
  $\binom{#n#}{2}$ comparações.  (Dica: O seu comparador não precisa olhar nos valores sendo comparados.) 
\end{exc}

\begin{exc}
  Analise o número esperado de comparações feitos pelo quicksort 
  um pouco mais cuidadosamente que a prova 
   \thmref{quicksort}.  Em particular, mostre que o número esperado
   de comparações é $2#n#H_#n# -#n# + H_#n#$.
\end{exc}

\begin{exc}
  Descreva um array de entrada que causa o heap sort realizar pelo menos
  $2#n#\log #n#-O(#n#)$ comparações. Justifique sua resposta.
\end{exc}

\javaonly{
\begin{exc}
  A implementação do heap sort descrita aqui ordena os elementos 
  em ordem reversa e então inverte o array.
  Esse último passo poderia ser evitar ao definir
  um novo #Comparator# que nega os resultados 
  da entrada #Comparator#, #c#.  Explique porque isso não seria seria uma boa otimização. (Dica: considere quantas negações seriam necessárias para fazer 
  em relação a quanto tempo levar a inverter o array.)
\end{exc}
}

\begin{exc}
  Achar outro par de permutações de 
   $1,2,3$ que não são corretamente ordenados pela árvore de comparações 
  em \figref{comparison-tree-2}.
\end{exc}

\begin{exc}\exclabel{log-factorial}
  Prove que $\log #n#! = #n#\log #n#-O(#n#)$.
\end{exc}

\begin{exc}
  Prove que uma árvore binária com $k$ folhas tem altura de pelo menos $\log k$.
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
  Prove que, se escolhermos uma folha aleatória de uma árvore binária com $k$ folhas, então a altura esperada dessa folha é pelo menos $\log k$.
%  (Hint: Use induction along with the inequality $(k_1/k)\log k_1 +
%  (k_2/k)\log k_2) \ge  \log k-1$, when $k_1+k_2=k$.)
\end{exc}

% This was a stupid exercise, since IEEE 754 floating point numbers 
% are correctly ordered when they are treated as signed magnitude integers
% [i.e., a sign bit followed by an integer ]
%\begin{exc}
%  Simple floating point numbers are numbers of the form $x\cdot10^{y}$,
%  where $0\le x\le 1$ and $y$ is an integer and each of $x$ and $y$ can
%  be represented by at most $k$ bits.  Describe a version of radix-sort
%  that can be used to sort simple floating point numbers.
%
%  Extend your version of radix sort so that it can handle signed values
%  of both $x$ and $y$ and implement a version of radix-sort that sorts
%  arrays of type #float#.
%\end{exc}

\begin{exc}
  A implementação do 
   #radixSort(a,k)# fornecida aqui funciona quando o array de entrada #a#
   contém somente inteiros 
  \cpponly{sem sinal}\javaonly{não negativos}
  .  \javaonly{Adapte essa implementação para que também funcione corretamente quando #a# contém inteiros negativos e não negativos.}\cpponly{Escreva uma versão que funcione corretamente para inteiros com sinal.}
\end{exc}

