\chapter{Listas Baseadas em Arrays}
\chaplabel{arrays}

Neste capítulo, iremos estudar implementações das interfaces #List# e #Queue#,
onde os dados são armazenados em um array chamado de \emph{backing array} ou \emph{array de apoio}.
\index{backing array}%
\index{array de apoio}%
A tabela a seguir resume o tempo de execução de operações para estruturas de dados apresentadas nestes capítulo:
\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}
Estruturas de dados que funcionam armazenando em um único array têm muitas vantagens e limitações em comum:
\index{arrays}%
\begin{itemize}
  \item Arrays permitem acesso em tempo constante a qualquer valor no array.
  Isso é o que permite #get(i)# e #set(i,x)# rodarem em tempo constante.

  \item Arrays não são muito dinâmicos. Adicionar ou remover um elemento perto do meio de uma lista significa que um grande número de elemento no array precisam
ser deslocados para abrir espaço para o elemento recentemente adicionado ou
preencher a lacuna criada pelo elemento removido. Essa é a razão pela qual as operações 
  #add(i,x)# e #remove(i)# têm tempos de execução que dependem de 
   #n# e #i#.

  \item Arrays não podem expandir ou encolher por si só. Quando o número de elementos na
    estrutura de dados excede o tamanho do array de apoio, um novo array precisa
    ser alocado e os dados do array antigo precisa ser copiado 
    no novo array. Essa é uma operação cara.
\end{itemize}
Um terceiro ponto é importante. Os tempos de execução citados na tabela
acima não incluem o custo associado com expandir ou encolher o array de apoio.
Veremos que, se não gerenciado com cuidado, o custo de expandir ou encolher o array de apoio não aumenta muito o custo \emph{médio} de uma operação.
Mais precisamente, se iniciarmos com uma estrutura de dados vazia e 
realizarmos qualquer sequência de $m$ operações #add(i,x)# ou #remove(i)#
, então o custo total de expandir e encolher o array de apoio, sobre a sequência inteira de $m$ operações é $O(m)$. Embora algumas operações individuais sejam mais caras, o custo amortizado, quando dividido por todas as $m$ operações, é somente $O(1)$ por operação.

\cpponly{
Neste capítulo, e ao longo deste livro, seria conveniente ter arrays que guardam seus tamanhos. Os arrays típicos do C++ não fazem isso, então definimos uma classe, #array#, que registra seu tamanho. A implementação dessa classe é direta. Ela é feita como um array C++ padrão, #a#, e um inteiro, #length#:
}
\cppimport{ods/array.a.length}
\cpponly{
O tamanho de um #array# é especificado no momento de criação:
}
\cppimport{ods/array.array(len)}
\cpponly{Os elementos de um array podem ser indexados:}
\cppimport{ods/array.operator[]}
\cpponly{Finalmente, quando um array é atribuído para outro, ocorre apenas uma manipulação de ponteiros que leva um tempo constante:}
\cppimport{ods/array.operator=}

\section{#ArrayStack#: Operações de Stack Rápida Usando um Array}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%
Um #ArrayStack# implementa a interface lista usando um array #a#, chamado de 
\emph{array de apoio}. O elemento da lista com índice #i# é armazenado
em #a[i]#.  Na maior parte do tempo, #a# é maior que o estritamente necessário,  
então um inteiro 
#n# é usado para registrar o número de elementos realmente armazenados em #a#. 
Dessa maneira, os elementos da lista são guardados em 
#a[0]#,\ldots,#a[n-1]# e, sempre, $#a.length# \ge #n#$.

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{O Básico}

Acessar e modificar os elementos de uma 
#ArrayStack# usando #get(i)# e #set(i,x)# é trivial. 
Após realizar as verificações de limites necessárias, simplesmente retornamos ou atribuímos, respectivamente, #a[i]#.

\codeimport{ods/ArrayStack.get(i).set(i,x)}

As operações de adicionar e remover elementos de um 
 #ArrayStack#
estão ilustradas na 
 \figref{arraystack}.  Para implementar a operação #add(i,x)#,
primeiro verificamos se #a# está cheio. Caso positivo, chamamos o método
#resize()# para aumentar o tamanho de #a#. Como #resize()#
é implementado será discutido depois. Por ora, é suficiente 
saber que, após uma chamada para #resize()#, temos certeza que $#a.length#
> #n#$.  
Com isso resolvido, deslocamos os elementos
$#a[i]#,\ldots,#a[n-1]#$ para uma posição à direita para
abrir espaço para #x#, atribuir
#a[i]# igual a #x# e incrementar #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption[Adicionando a um ArrayStack]{Uma sequência de operações #add(i,x)# e #remove(i)# em um
  #ArrayStack#.  Flechas denotam elementos sendo copiados. Operações que
  resultam em uma chamada para 
  #resize()# são marcados com um asterisco.}
  \figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
Se ignorarmos o custo de uma potencial chamada a
#resize()#, então o custo da operação 
#add(i,x)# é proporcional ao número de elementos que temos que deslocar para
abrir espaço para 
 #x#.  Portanto o custo dessa operação 
(ignorando o custo de redimensionar #a#) é $O(#n#-#i#)$.

Implementar a operação
#remove(i)# é similar. Desloca-se os elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda por uma posição (sobrescrevendo #a[i]#) 
e decrementa-se o valor de  
 #n#. Após fazer isso, verificamos se #n# é muito menor 
 que #a.length# ao verificar se $#a.length# \ge 3#n#$. 
Caso positivo, então chamamos #resize()# para reduzir o tamanho de #a#.

\codeimport{ods/ArrayStack.remove(i)}
% TODO: Add shifting figure
Ignorando o custo do método #resize()#, o custo de uma operação #remove(i)#
é proporcional ao número de elementos que deslocamos, que é $O(#n#-#i#)$.

\subsection{Expansão e Redução}

O método 
 #resize()# é razoavelmente simples; ele aloca um novo array
#b# de tamanho $2#n#$ e copia os #n# elementos de #a# nas primeiras 
#n# posições em #b# e então atribui #a# em #b#. Após isso, faz uma chamada a #resize()#, $#a.length# = 2#n#$.

\codeimport{ods/ArrayStack.resize()}

Analisar o custo real da operação 
#resize()# é fácil.
Ela aloca um array 
 #b# de tamanho $2#n#$ e copia os #n# elementos de #a# em 
#b#. Isso leva tempo $O(#n#)$.

A análise de tempo de execução da seção anterior ignorou o custo de chamadas a
#resize()#.  Nesta seção analisaremos esse custo usando uma técnica chamada de 
\emph{análise amortizada}.  Essa técnica não tenta determinar o custo de 
redimensionar o array durante cada operação 
#add(i,x)# e #remove(i)#.  Em vez disso, ela considera o custo de todas as chamadas a #resize()# durante a sequência de $m$ chamadas a #add(i,x)# ou #remove(i)#.
Em particular, mostraremos que:

\begin{lem}\lemlabel{arraystack-amortized}
  Se um 
   #ArrayStack# vazio é criado e uma sequência de $m\ge 1$ chamadas a  
  #add(i,x)# e #remove(i)# são executadas, então o tempo total gasto durante
  todas as chamadas a #resize()# é $O(m)$.
\end{lem}

\begin{proof}
  Mostraremos que em qualquer momento que
 #resize()# é chamada, o número de chamadas a 
  #add()# ou #remove()# desde a última chamada a #resize()# é pelo menos 
  $#n#/2-1$.  Portanto, se $#n#_i$ denota o valor de #n# durante a 
  $i$-ésima chamada a #resize()# e $r$ denota o número de chamadas a 
  #resize()#, então o número total de chamadas a #add(i,x)# ou
  #remove(i)# é pelo menos 
  \[
     \sum_{i=1}^{r} (#n#_i/2-1) \le m  \enspace ,
  \]
  o que é equivalente a
  \[
    \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace .
  \]
  Por outro lado, o tempo total gasto durante todas as chamadas a #resize()# é 
  \[
     \sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace ,
  \]
  uma vez que 
  $r$ não é maior que $m$.  O que resta é mostrar que o número de chamadas a 
   #add(i,x)# ou #remove(i)# entre a $(i-1)$-ésima
  e a $i$-ésima chamada a #resize()# é de pelo menos $#n#_i/2$.

  Há dois casos a considerar. No primeiro caso, 
 #resize()# está sendo chamado por 
#add(i,x)# pois o array de apoio #a# está cheio, i.e.,
  $#a.length# = #n#=#n#_i$.  Considere a chamada anterior a #resize()#:
  após essa chamada prévia, o tamanho de 
 #a# era #a.length#, mas o número de elementos guardados em #a# 
  era no máximo $#a.length#/2=#n#_i/2$.
  Mas agora o número de elementos guardados em 
 #a# é $#n#_i=#a.length#$, então deve ter havido pelo menos
$#n#_i/2$ chamadas a #add(i,x)# desde a chamada anterior a 
   #resize()#.
  % TODO: Add figure
  
  O segundo caso ocorre quando 
 #resize()# está sendo chamado por 
  #remove(i)# porque $#a.length# \ge 3#n#=3#n#_i$.  Novamente, após a
  chamada anterior a
 #resize()# o número de elementos guardados em #a# era pelo menos 
   $#a.length/2#-1$.\footnote{O ${}-1$ nessa fórmula inclui o caso especial que ocorre quando 
   $#n#=0$ e $#a.length# = 1$.} Agora há 
  $#n#_i\le#a.length#/3$ elementos guardados em #a#.  Portanto, o número de 
  operações #remove(i)# desde a última chamada a #resize()# é pelo menos 
  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1\enspace .
  \end{align*}
Nos dois casos, o número de chamadas a 
 #add(i,x)# ou #remove(i)# que ocorrem 
 entre 
   $(i-1)$-ésima chamada a #resize()# e a $i$-ésima chamada a 
  #resize()# é pelo menos $#n#_i/2-1$, conforme exigido para completar a prova.
\end{proof}

\subsection{Resumo}

O teorema a seguir resume o desempenho de uma #ArrayStack#:

\begin{thm}\thmlabel{arraystack}
  Uma
   #ArrayStack# implementa a interface #List#. Ignorando o custo de chamadas a 
  #resize()#, uma #ArrayStack# possui as operações
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e
    \item #add(i,x)# e #remove(i)# em tempo $O(1+#n#-#i#)$ por operação.
  \end{itemize}
  Além disso, ao começarmos com um 
 #ArrayStack# vazio e realizarmos uma sequência de $m$ operações 
   #add(i,x)# e #remove(i)# resulta em um total de
   $O(m)$ tempo gasto durante todas as chamadas a #resize()#.
\end{thm}

O #ArrayStack# é um jeito eficiente de implementar a #Stack#.
Em especial, podemos implementar #push(x)# como #add(n,x)# e #pop()#
como #remove(n-1)# e nesse caso essas operações rodarão em tempo $O(1)$
amortizado.

\section{#FastArrayStack#: Uma ArrayStack otimizada}
\seclabel{fastarraystack}

\index{FastArrayStack@#FastArrayStack#}%
Muito do trabalho feito por uma 
 #ArrayStack# envolve o deslocamento (por 
#add(i,x)# e #remove(i)#) e cópias (pelo #resize()#) de dados.
\notpcode{Nas implementações mostradas acima, isso era feito usando laços #for#.}%
\pcodeonly{Em uma implementação ingênua, isso seria feito usando laços #for#.}
Acontece que muitos ambientes de programação têm funções específicas que são muito 
eficientes em copiar e mover blocos de dados.
Na linguagem C,
existem as funções #memcpy(d,s,n)# e #memmove(d,s,n)#. 
Na linguagem C++, existe o algoritmo #std::copy(a0,a1,b)#.
Em Java, existe o 
método #System.arraycopy(s,i,d,j,n)#.
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\cppimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
\javaimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

Essas funções são, em geral, altamente otimizadas e podem usar até mesmo  
instruções de máquina especiais que podem fazer esse cópia muito mais rápida do 
que poderíamos usando um laço #for#.
Embora o uso dessas funções não diminuam o tempo de execução assintoticamente falando, pode ser uma otimização que vale a pena.

\pcodeonly{Nas nossas implementações em C++ e Java, o uso de funções de cópia rápida de arrays
}
\notpcode{Nas implementações em \lang\ deste livro, o uso do método disponível nativamente \javaonly{#System.arraycopy(s,i,d,j,n)#}\cpponly{#std::copy(a0,a1,b)#}}
resultaram em um fator de aceleração, \emph{speedup}, entre 2 e 3, dependendo dos tipos de operações realizadas.
O resultados podem variar de acordo com o caso.

\section{#ArrayQueue#: Uma Queue Baseada Em Array}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%

Nesta seção apresentamos a estrutura de dados 
 #ArrayQueue#, que implementa uma queue do tipo 
 FIFO (first-in-first-out, primeiro-que-chega-primeiro-que-sai);
 elementos são removidos (usando a operação 
#remove()#) da queue na mesma ordem em que são adicionados 
(usando a operação #add(x)#).

Note que uma 
#ArrayStack# é uma escolha ruim para uma implementação de uma 
queue do tipo FIFO. Não é uma boa escolha porque precisamos escolher um lado da lista ao qual adicionaremos elementos e então remover elementos do outro lado. 
Uma das duas operações precisa trabalhar na cabeça da lista, o que envolve chamar
#add(i,x)# ou #remove(i)# com um valor de $#i#=0$.
Isso resulta em um tempo de execução proporcional a #n#.

Para obter uma implementação de queue eficiente baseada em array,
primeiro observamos que o problema seria fácil se tivéssemos um array #a# infinito.
Poderíamos manter um índice
 #j# que guarda qual é o próximo elemento a remover e um inteiro
#n# que conta o número de elementos na queue.
Os elementos da queue sempre seriam guardados em 
\[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace . \]
Inicialmente, ambos #j# e #n# receberiam o valor 0. 
Para adicionar um elemento, teríamos que colocá-lo em #a[j+n]# e incrementar #n#.
Para remover um elemento, o removeríamos de 
 #a[j]#, incrementando #j#, e 
decrementando #n#.

É claro, o problema com essa solução é que ela requer um array infinito.
Um 
#ArrayQueue# simula isso ao usar um array finito #a#
e \emph{aritmética modular}.
\index{aritmética modular}%
Esse é o tipo de aritmética usada quando estamos falando sobre a hora do dia.
Por exemplo, 10:00 mais cinco horas resulta em 3:00. Formalmente, dizemos que
\[
    10 + 5 = 15 \equiv 3 \pmod{12} \enspace .
\]
Lemos a última parte dessa equação da forma ``15 é congruente a 3 módulo 12.''
Podemos também tratar 
 $\bmod$ como um operador binário, tal que 
\[
   15 \bmod 12 = 3 \enspace .
\]

De modo mais geral, para um inteiro $a$ e um inteiro positivo 
$m$, $a \bmod m$ é o único inteiro 
 $r\in\{0,\ldots,m-1\}$ tal que $a = r + km$ para algum inteiro $k$. 
Informalmente, o valor $r$ é o resto obtido quando dividimos $a$ por $m$.
\pcodeonly{Em muitas linguagens de programação, incluindo C, C++ e Java, o operador mod é representado usando o símbolo \%.} 
\notpcode{Em muitas linguagens de programação incluindo 
\javaonly{Java}\cpponly{C++}, o operador $\bmod$ é representado 
usando o símbolo  #%#.\footnote{Isso às vezes é chamado de 
operador mod com \emph{morte cerebral}, pois não implementa corretamente
o operador matemático mod quando o primeiro argumento é negativo.}}

Aritmética modular é útil para simular um array infinito 
uma vez que 
$#i#\bmod #a.length#$ sempre resulta em um valor no intervalo
$0,\ldots,#a.length-1#$.  Usando aritmética modular podemos guardar
os elementos da queue em posições do array
\[ #a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\enspace. \]
Desse jeito trata-se o array #a# como um \emph{array circular}
\index{array circular}%
\index{circular!array}%
no qual os índices do array maiores que $#a.length#-1$ ``dão a volta'' 
ao começo do array. 
% TODO: figure

A única coisa que falta é cuidar para que o número de elementos no
 #ArrayQueue# não ultrapasse o tamanho de #a#.

\codeimport{ods/ArrayQueue.a.j.n}

A sequência de operações  
#add(x)# e #remove()# em um #ArrayQueue# é
ilustrado na \figref{arrayqueue}.  Para implementar #add(x)#, primeiro 
verificamos se 
 #a# está cheio e, se necessário, chamamos #resize()# para aumentar o tamanho de  
#a#. Em seguida, guardamos #x# em
#a[(j+n)%a.length]# e incrementamos #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption[Adicionar e remover de um ArrayQueue]{Uma sequência de operações #add(x)# e #remove(i)# em um 
  #ArrayQueue#.  Flechas denotam elementos sendo copiados. Operações que resultam em uma chamada a #resize()# são marcadas com um asterisco.}
  \figlabel{arrayqueue}
\end{figure}

\codeimport{ods/ArrayQueue.add(x)}

Para implementar 
#remove()#, primeiro guardamos #a[j]# para que possamos reutilizá-lo depois. 
A seguir, decrementamos #n# e incrementamos #j# (módulo #a.length#)
ao atribuir
$#j#=(#j#+1)\bmod #a.length#$.  Finalmente, retornamos o valor guardado de
#a[j]#. Se necessário, podemos chamar  #resize()# para diminuir o tamanho de  #a#.

\codeimport{ods/ArrayQueue.remove()}

Finalmente, a operação  
#resize()# é muito similar à operação #resize()#
do #ArrayStack#. Ela aloca um novo array, #b#, de tamanho $2#n#$
e copia 
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
em 
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
e atribui $#j#=0$.

\codeimport{ods/ArrayQueue.resize()}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados #ArrayQueue#:

\begin{thm}
Um #ArrayQueue# implementa a interface (FIFO) #Queue#. Ignorando o custo das chamadas a 
#resize()#, uma #ArrayQueue# aceita as operações 
#add(x)# e #remove()# em tempo $O(1)$ por operação. 
Além disso, ao começar com um #ArrayQueue# vazio, qualquer sequência de $m$
operações #add(i,x)# e #remove(i)# resulta em um total de tempo $O(m)$ gasto
durante todas as chamadas a #resize()#.
\end{thm}

%TODO: Discuss the use of bitwise-and as a replacement for the mod operator

\section{#ArrayDeque#: Operações Rápidas para Deque Usando um Array}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
O #ArrayQueue# da seção anterior é uma estrutura de dados para 
representação de sequências que nos permite eficientemente adicionar a
um lado da sequência e remover do outro.

A estrutura de dados #ArrayDeque# permite a edição e remoção eficiente em ambos lados.
Essa estrutura implementa a interface 
 #List# ao usar a mesma técnica de array circular
usada para representar um #ArrayQueue#.

\codeimport{ods/ArrayDeque.a.j.n}

As operações #get(i)# e #set(i,x)# em um #ArrayDeque# são simples
. Elas obtém ou atribui a um elemento do array $#a[#{#(j+i)#\bmod
#a.length#}#]#$.

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

A implementação de 
 #add(i,x)# é um pouco mais interessante. Como sempre, primeiro 
 verificamos se #a# está cheio e, se necessário, chamados 
#resize()# para redimensionar #a#.  Lembre-se que queremos que essa operação
seja rápida quando 
#i# for pequeno (perto de 0) ou quando #i# é grande (perto de 
#n#).  Portanto, verificamos se $#i#<#n#/2$.  Caso positivo, deslocamos os
elementos $#a[0]#,\ldots,#a[i-1]#$ à esquerda por uma posição.  Caso contrário, 
($#i#\ge#n#/2$), deslocamos os elementos $#a[i]#,\ldots,#a[n-1]#$ à direita por uma posição. 
Veja a \figref{arraydeque} para uma representação das operações 
#add(i,x)# e #remove(x)# em um #ArrayDeque#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption[Adição e remoção de ArrayDeque]{Uma sequência de operações #add(i,x)# e #remove(i)# em um 
  #ArrayDeque#.  Flechas denotam elementos sendo copiados.}
  \figlabel{arraydeque}
\end{figure}


\codeimport{ods/ArrayDeque.add(i,x)}

Ao deslocar dessa maneira, nós garantimos que #add(i,x)# nunca tem que deslocar mais de 
 $\min\{ #i#, #n#-#i# \}$ elementos.  Então, o tempo de execução da operação 
#add(i,x)# (ignorando o custo de uma operação #resize()#) é $O(1+\min\{#i#,#n#-#i#\})$.

A implementação da operação #remove(i)# é similar. Ela desloca elementos
$#a[0]#,\ldots,#a[i-1]#$ à direita em uma posição ou desloca elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda em uma posição dependendo se 
$#i#<#n#/2$.  Novamente, isso significa que #remove(i)# nunca gasta mais de 
$O(1+\min\{#i#,#n#-#i#\})$ tempo para deslocar elementos.

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados
 #ArrayDeque#:
\begin{thm}\thmlabel{arraydeque}
  Uma #ArrayDeque# implementa a interface #List#.  Ignorando o custo de chamadas 
  a #resize()#, um #ArrayDeque# aceita as operações 
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(1+\min\{#i#,#n#-#i#\})$ 
          por operação.
  \end{itemize}
  Além disso, começando com um 
 #ArrayDeque# vazio, uma sequência de $m$ operações 
  #add(i,x)# e #remove(i)# resulta em um
  total de tempo $O(m)$ gasto durante todas as chamadas a #resize()#.
\end{thm}

\section{#DualArrayDeque#: Construção de um Deque a Partir de Duas Stacks}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%
A seguir, apresentamos uma estrutura de dados, o 
 #DualArrayDeque#, que tem o mesmo desempenho que um #ArrayDeque# ao usar 
duas #ArrayStack#s.  Embora o desempenho assintótico do
#DualArrayDeque# não seja melhor que do #ArrayDeque#, ainda vale estudá-lo 
, pois oferece um bom exemplo de como obter uma estrutura de dados mais sofisticada pela combinação de duas estruturas de dados mais simples.

Um #DualArrayDeque# representa uma lista usando duas #ArrayStack#s.  Relembre-se que um 
#ArrayStack# é rápido quando as operações dele modificam elementos perto do final.
Uma #DualArrayDeque# posiciona duas #ArrayStack#s, chamadas de #frontal#
e #traseira#, de modo complementar para que as operações sejam rápidas em ambas as direções.

\codeimport{ods/DualArrayDeque.front.back}

Um
 #DualArrayDeque# não guarda explicitamente o número, #n#,
 de elementos contidos. Ele não precisa, pois ele contém
$#n#=#front.size()# + #back.size()#$ elementos. De qualquer forma, ao
analisar o 
#DualArrayDeque# iremos usar ainda o #n# para denotar o número de 
elementos nele.

\codeimport{ods/DualArrayDeque.size()}

A #ArrayStack# #front# guarda os elementos da lista cujos índices são 
$0,\ldots,#front.size()#-1$, mas guarda-os em ordem reversa.
O #ArrayStack# #back# contém elementos da lista com índices 
em $#front.size()#,\ldots,#size()#-1$ na ordem normal. Desse jeito, 
#get(i)# e #set(i,x)# traduzem-se em chamadas apropriadas para #get(i)#
ou #set(i,x)# e ambos #front# ou #back#, que levam tempo $O(1)$ time por operação.

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

Note que se um índice
 $#i#<#front.size()#$, então ele corresponde ao elemento 
de #front# na posição $#front.size()#-#i#-1$, pois
os elementos de #front# são guardados em ordem inversa.

A adição e a remoção de elementos de uma #DualArrayDeque# são ilustradas na 
\figref{dualarraydeque}.  A operação #add(i,x)# manipula #front#
ou #back#, conforme apropriado:

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dualarraydeque}
  \end{center}
  \caption[Adição e remoção em um DualArrayDeque]{Uma sequência de operações #add(i,x)# e #remove(i)# em um 
  #DualArrayDeque#.  Flechas denotam elementos sendo copiados.  Operações que resultam em um
  rebalanceamento por #balance()# são marcados com um asterisco.}
  \figlabel{dualarraydeque}
\end{figure}

\codeimport{ods/DualArrayDeque.add(i,x)}

O método 
#add(i,x)# realiza o balanceamento das duas #ArrayStack#s
#front# e #back#, ao chamar o método #balance()#.
A seguir descrevemos a implementação de #balance()#, mas no momento é suficiente 
saber que #balance()# garante que, a não ser que $#size()#<2$,
#front.size()# e #back.size()# não diferem por mais de um fator 
de 3.  Em especial, $3\cdot#front.size()# \ge #back.size()#$ e
$3\cdot#back.size()# \ge #front.size()#$.

A seguir, nós analisamos o custo de 
 #add(i,x)#, ignorando o custo de chamadas 
#balance()#. Se $#i#<#front.size()#$, então #add(i,x)# é implementada 
pela chamada a
 $#front.add(front.size()-i-1,x)#$.  Como #front# é uma 
#ArrayStack#, o custo disso é 
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(#i#+1) \enspace .
  \eqlabel{das-front}
\end{equation}
Por outro lado, se 
 $#i#\ge#front.size()#$, então #add(i,x)# é 
implementada como $#back.add(i-front.size(),x)#$.  O custo disso é 
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(#n#-#i#+1) \enspace .
  \eqlabel{das-back}
\end{equation}

Note que o primeiro caso 
 \myeqref{das-front} ocorre quando $#i#<#n#/4$.
 O segundo caso
 \myeqref{das-back} ocorre quando $#i#\ge 3#n#/4$.  Quando
$#n#/4\le#i#<3#n#/4$, não temos certeza se a operação afeta 
#front# ou #back#, mas nos dois casos, a operação leva tempo
$O(#n#)=O(#i#)=O(#n#-#i#)$, pois $#i#\ge #n#/4$ e $#n#-#i#>
#n#/4$.  Resumindo a situação, temos 
\[
     \mbox{Tempo de execução } #add(i,x)# \le 
          \begin{cases}
            O(1+ #i#) & \mbox{se $#i#< #n#/4$} \\
            O(#n#) & \mbox{se $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{se $#i# \ge 3#n#/4$}
          \end{cases}
\]
Então, o tempo de operação de 
 #add(i,x)#, se ignorarmos o custo à chamada 
#balance()#, é $O(1+\min\{#i#, #n#-#i#\})$.

A operação #remove(i)# e sua análise lembra a análise de #add(i,x)#.


\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{Balanceamento}

Finalmente, passamos à operação 
 #balance()# usada por #add(i,x)#
e #remove(i)#.  Essa operação garante que nem #front# nem #back#
se tornem grandes demais (ou pequenos demais). 
Ela garante que, ao menos que haja menos de dois elementos, o 
 #front# e o #back# possuem $#n#/4$ elementos cada. 
Se esse não for o caso, então ele move elementos entre elas 
de tal forma que #front# e #back# contenham exatamente $\lfloor#n#/2\rfloor$ elementos e $\lceil#n#/2\rceil$ elementos, respectivamente.

\codeimport{ods/DualArrayDeque.balance()}

Aqui há pouco para analisar. Se a operação 
#balance()# faz rebalanceamento 
, então ela move $O(#n#)$ elementos e isso leva $O(#n#)$ de tempo.
Isso é ruim, pois
 #balance()# é chamada a cada operação
#add(i,x)# e #remove(i)#.  Porém, o lema a seguir mostra que
na média, #balance()# gasta somente um tempo constante por operação.

\begin{lem}\lemlabel{dualarraydeque-amortized}
  Se uma 
  #DualArrayDeque# vazia é criada e qualquer sequência de chamadas $m\ge 1$ a
 #add(i,x)# e #remove(i)# são realizadas, então o tempo total gasto 
 durante todas as chamadas a 
 #balance()# é $O(m)$.
\end{lem}

\begin{proof}
  Iremos mostrar que, se 
 #balance()# é forçada a deslocar elementos, então o número de operações 
  #add(i,x)# e #remove(i)# desde a última vez que elementos foram deslocador por 
   #balance()# é pelo menos $#n#/2-1$.
   Assim como na prova do 
 \lemref{arraystack-amortized}, é suficiente provar que o tempo total gasto por 
   #balance()# é $O(m)$.

   Iremos realizar a nossa análise usando uma técnica conhecida como o \emph{método do potencial}.
  \index{potencial}%
  \index{método do potencial}%
  Definimos o \emph{potencial}, $\Phi$, do 
  #DualArrayDeque# como a diferença dos tamanhos entre #front# e #back#:
  \[  \Phi = |#front.size()# - #back.size()#| \enspace . \]
  Uma propriedade interessante desse potencial é que uma chamada a 
 #add(i,x)#
  ou #remove(i)# que não faz nenhum balanceamento pode aumentar o potencial em até 1.

  Observe que, imediatamente após uma chamada a #balance()# que desloca elementos 
  a ,o potencial, $\Phi_0$, é no máximo 1, pois
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace .\]

  Considere a situação no momento exatamente anterior a uma chamada #balance()# que 
  desloca elementos e suponha, sem perda de generalidade, que 
 #balance()#
está deslocando elementos porque $3#front.size()# < #back.size()#$.
Note que, nesse caso
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  Além disso, o potencial nesse momento é 
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  Portanto, o número de chamadas a 
 #add(i,x)# ou #remove(i)# desde a última vez que 
   #balance()# deslocou elementos é pelo menos $\Phi_1-\Phi_0
  > #n#/2-1$. Isso completa a prova.
\end{proof}

\subsection{Resumo}

O teorema a seguir resume as propriedades de uma #DualArrayDeque#:

\begin{thm}\thmlabel{dualarraydeque}
  Uma
   #DualArrayDeque# implementa a interface #List#. Ignorando o custo de chamadas a 
   #resize()# e #balance()#, uma #DualArrayDeque#
  possui as operações 
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(1+\min\{#i#,#n#-#i#\})$ 
          por operação.
  \end{itemize}
  Além disso, ao começar com uma 
#DualArrayDeque# vazia, uma sequência de $m$ operações
  #add(i,x)# e #remove(i)# resulta em um tempo total $O(m)$
  durante todas as chamadas a #resize()# e #balance()#.
\end{thm}


\section{#RootishArrayStack#: Uma Stack Array Eficiente No Uso de Espaço}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%
Uma das desvantagens de todas as estruturas de dados anteriores neste capítulo
é que, porque elas guardam os dados em um array ou dois e evitam redimensionar
esses arrays com frequência, os arrays frequentemente não estão muito cheios.
Por exemplo, imediatamente após uma operação 
 #resize()# em uma #ArrayStack#,
o array de apoio #a# tem somente metade do espaço em uso.
E pior, às vezes somente um terço de #a# contém dados.

Nesta seção, discutimos a estrutura de dados 
#RootishArrayStack#, que resolve o problema de espaço desperdiçado.
A #RootishArrayStack# guarda 
#n# elementos usando arrays de tamanho $O(\sqrt{#n#})$.
Nesses arrays, no máximo 
$O(\sqrt{#n#})$ posições do array estão vazias a qualquer momento.
Todo o restante do array está sendo usado para guardar dados. Portanto,
essas estruturas de dados gastam até 
 $O(\sqrt{#n#})$ de espaço ao guardar #n# elementos.

Uma #RootishArrayStack# guarda seus elementos em uma lista de #r#
arrays chamados de \emph{blocos} que são numerados $0,1,\ldots,#r#-1$.
Veja a \figref{rootisharraystack}.  O bloco $b$ contém $b+1$ elementos.
Então, todos os 
 #r# blocos contêm um total de
\[
  1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
elementos. A fórmula acima pode ser obtida conforme mostrado na \figref{gauss}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
  \end{center}
  \caption[Adição e remoção em uma RootishArrayStack]{Uma sequência de operações #add(i,x)# e #remove(i)# em uma 
  #RootishArrayStack#.  Flechas denotam elementos sendo copiados. }
  \figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/gauss}
  \end{center}
  \caption{O número de quadrados brancos é $1+2+3+\cdots+#r#$.  O número de quadrados sombreados é o mesmo.
  Juntos, os quadrados brancos e sombreados compõem um retângulo consistindo de 
   $#r#(#r#+1)$ quadrados.}
  \figlabel{gauss}
\end{figure}

Como podemos esperar, os elementos da lista estão dispostos em ordem dentro dos blocos.
O elemento da lista com índice 0 é guardado no bloco 0, 
elementos com índices 1 e 2 são guardados no bloco 1, elementos
com índices 3, 4 e 5 são guardados no bloco 2 e assim por diante.
O principal problema que temos para resolver é o de determinar, dado um índice
 $#i#$,
 qual bloco contém 
 #i# e também o índice correspondente a #i# naquele bloco.

Determinar o índice de #i# no bloco acaba sendo fácil.
Se o índice #i# está no bloco #b#, então o número de elementos nos blocos 
$0,\ldots,#b#-1$ é $#b#(#b#+1)/2$.  Portanto, #i# é guardado na posição 
\[
     #j# = #i# - #b#(#b#+1)/2
\]
dentro do bloco #b#.  Um pouco mais desafiador é o problema de 
determinar o valor de 
#b#.  O número de elementos que têm índices menores que ou iguais a 
#i# é $#i#+1$.  Por outro lado, o número de elementos nos blocos 
$0,\ldots,b$ é $(#b#+1)(#b#+2)/2$.  Portanto, #b# é o menor inteiro tal que 

\[
    (#b#+1)(#b#+2)/2 \ge #i#+1 \enspace .
\]
Podemos reescrever essa equação da forma
\[
    #b#^2 + 3#b# - 2#i# \ge  0 \enspace .
\]
A equação quadrática correspondente 
 $#b#^2 + 3#b# - 2#i# =  0$ tem duas soluções
: $#b#=(-3 + \sqrt{9+8#i#}) / 2$ e $#b#=(-3 - \sqrt{9+8#i#}) / 2$.
A segunda solução não faz sentido na nossa aplicação pois é um valor negativo.
Portanto, obtemos a solução
$#b# = (-3 +
\sqrt{9+8i}) / 2$.  Em geral, essa solução não é um inteiro, mas retornando à nossa desigualdade, queremos o menor inteiro 
 $#b#$ tal que
$#b# \ge (-3 + \sqrt{9+8i}) / 2$.  Isso é simplesmente
\[
   #b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace .
\]

\codeimport{ods/RootishArrayStack.i2b(i)}

Com isso fora do caminho, os métodos 
#get(i)# e #set(i,x)# são diretos. Primeiramente computamos
o bloco #b# apropriado e o índice #j# dentro do bloco e realizar a operação apropriada: 

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

Se usarmos quaisquer estruturas de dados deste capítulo para representar a lista de 
#blocks#, então #get(i)# e #set(i,x)# irão rodar em tempo constante.

O método #add(i,x)# irá, a esta altura, parecer familiar. Primeiro verificamos 
se nossa estrutura de dados está cheia ao verificar se o número de blocos 
, #r#, é tal que $#r#(#r#+1)/2 = #n#$. Caso positivo, chamamos #grow()#
para adicionar outro bloco. Com isso feito, deslocamos elementos com índices
$#i#,\ldots,#n#-1$ à direita em uma posição para abrir espaço para o novo elemento com índice #i#:

\codeimport{ods/RootishArrayStack.add(i,x)}

O método #grow()# faz o que esperamos dele. Ele adiciona um novo bloco à estrutura de dados: 

\codeimport{ods/RootishArrayStack.grow()}

Ignorando o custo da operação  #grow()#, o custo da 
operação #add(i,x)# é dominado pelo custo de deslocamento 
e é portanto $O(1+#n#-#i#)$, como um #ArrayStack#.

A operação #remove(i)# é similar a #add(i,x)#.  Ela desloca os elementos com índices 
$#i#+1,\ldots,#n#$ à esquerda em uma posição e então, se há mais de um bloco vazio, ela chama o método 
#shrink()# para remover todos, exceto um dos blocos não usados:

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

Novamente, ignorando o custo da operação 
 #shrink()#, o custo de uma operação 
#remove(i)# é dominado pelo custo de deslocamento e portanto
$O(#n#-#i#)$.

\subsection{Análise de expandir e encolher}

A análise acima de  #add(i,x)# e #remove(i)# 
não leva em conta o custo de #grow()# e #shrink()#.  
Note que, diferentemente da operação
#ArrayStack.resize()#, #grow()# e #shrink()# não copiam nenhum dado. 
Elas simplesmente alocam ou liberam um array de tamanho #r#.  
Em alguns ambientes, isso leva apenas tempo constante, enquanto
em outros, pode ser necessário tempo proporcional a #r#.

Note que, imediatamente após uma chamada a 
#grow()# ou #shrink()#, a situação é clara. O bloco final 
está completamente vazio e todos os outros blocos estão 
completamente cheios. 
Outra chamada a #grow()# ou #shrink()# não acontecerá 
até que pelo menos 
 $#r#-1$ elementos tenham sido adicionados ou removidos.
 Portanto, mesmo se 
#grow()# e #shrink()# leve tempo $O(#r#)$, esse custo pode ser amortizado em pelo menos $#r#-1$ operações #add(i,x)# ou #remove(i)#
fazendo com que o custo amortizado de 
 #grow()# e #shrink()# sejam
$O(1)$ por operação.

\subsection{Uso de Espaço}
\seclabel{rootishspaceusage}

A seguir, analisaremos a quantidade de espaço extra usada por uma 
 #RootishArrayStack#.

Em especial, queremos contar qualquer espaço usado por uma #RootishArrayStack# que não seja de um elemento de array usado no momento para guardar um elemento da lista. Chamamos todo esse espaço de \emph{espaço desperdiçado}.
\index{espaço desperdiçado}%

A operação
#remove(i)# assegura que uma #RootishArrayStack# nunca tenha mais do que dois
blocos que não estejam completamente cheios.
O número de blocos #r#, usados por uma 
#RootishArrayStack# que guarda #n# elementos portanto satisfaz
\[
    (#r#-2)(#r#-1)/2 \le #n# \enspace .
\]
De novo, usando a equação quadrática resulta em
\[
   #r# \le \frac{1}{2}\left(3+\sqrt{8#n#+1}\right) = O(\sqrt{#n#}) \enspace .
\]
Os últimos dois blocos têm tamanhos #r# e #r-1#, então o espaço desperdiçado por esses dois blocos é até
 $2#r#-1 = O(\sqrt{#n#})$.  Se guardarmos os blocos 
em (por exemplo) uma #ArrayStack#, então a quantidade de espaço gasto pela 
#List# que guarda esses #r# blocks também é $O(#r#)=O(\sqrt{#n#})$.  
O espaço adicional necessário para guardar #n# e outras informações auxiliares é $O(1)$.
Portanto, a quantidade total de espaço desperdiçado em uma #RootishArrayStack#
é $O(\sqrt{#n#})$.

A seguir, demonstramos que esse uso de espaço é ótimo para qualquer estrutura 
de dados que inicia-se vazia e permite a adição de um item por vez.
Mais precisamente, mostraremos que, em algum momento durante a
adição de #n# itens, a estrutura de dados está desperdiçando pelo menos
um espaço de $\sqrt{#n#}$ (embora possa ser por apenas um curto momento).

Suponha que iniciamos com uma estrutura de dados vazia e adicionamos #n# itens, 
um por vez. No fim desse processo, todos os #n# itens são guardados na 
estrutura e distribuídos entre uma coleção de #r# blocos de memória.
Se $#r#\ge \sqrt{#n#}$, então a estrutura de dados precisa usar #r#
ponteiros (ou referências) para gerenciar esses #r# blocos e esses
ponteiros são espaço desperdiçado.
Por outro lado, se 
 $#r# < \sqrt{#n#}$
 então, pelo princípio das casas de pombos, algum bloco deve ter tamanho de
 pelo menos
$#n#/#r# > \sqrt{#n#}$.  Considere o momento no qual esse bloco 
foi inicialmente alocado. Imediatamente após alocá-lo, esse
bloco estava vazio e portanto desperdiçando
um espaço de $\sqrt{#n#}$. Portanto, em algum momento durante a inserção de  
#n# elementos, a estrutura de dados estava gastando
 $\sqrt{#n#}$ de espaço.

\subsection{Resumo}

O seguinte teorema resume a discussão sobre a estrutura de dados #RootishArrayStack#:

\begin{thm}\thmlabel{rootisharraystack}
  Uma #RootishArrayStack# implementa a interface #List#. Ignorando o custo das chamadas 
a #grow()# e #shrink()#, uma #RootishArrayStack# possui as operações
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operações; e  
    \item #add(i,x)# e #remove(i)# em tempo $O(1+#n#-#i#)$ por operação.
  \end{itemize}
  Além disso, ao começar com uma 
 #RootishArrayStack# vazia, qualquer sequência de $m$ operações
  #add(i,x)# e #remove(i)# resulta em um total de $O(m)$
  tempo gasto durante todas as chamadas de #grow()# e #shrink()#.
  O espaço (medido em palavras)\footnote{Reveja a \secref{model} para uma discussão de como a memória é medida.} usada por uma #RootishArrayStack# que guarda
#n# elementos é $#n# +O(\sqrt{#n#})$.
\end{thm}

\notpcode{

\subsection{Computando Raízes Quadradas}

\index{raízes quadradas}%
Um leitor que tenha tido alguma exposição a modelos de computação
pode notar que a #RootishArrayStack#, conforme descrita anteriormente,
não se encaixa no modelo de computação normal word-RAM 
(\secref{model}) porque requer a computação de raízes quadradas. 
A operação raiz quadrada não é geralmente considerada uma operação 
básica e portanto não é normalmente parte do modelo word-RAM.

Nesta seção, mostraremos que a operação raiz quadrada pode ser
implementada eficientemente. Em particular, mostramos que para qualquer inteiro
$#x#\in\{0,\ldots,#n#\}$,  $\lfloor\sqrt{#x#}\rfloor$ pode ser computado 
em tempo constante, após um pré-processamento com tempo de execução de 
$O(\sqrt{#n#})$ que cria dois arrays de tamanho 
$O(\sqrt{#n#})$.  O lema a seguir mostra que podemos reduzir o problema
de computar a raiz quadrada de #x# à raiz quadrada de um valor 
relacionado #x'#.

\begin{lem}\lemlabel{root}
Seja $#x#\ge 1$ e seja $#x'#=#x#-a$, onde $0\le a\le\sqrt{#x#}$. Então 
   $\sqrt{x'} \ge \sqrt{#x#}-1$.
\end{lem}

\begin{proof}
É suficiente mostrar que 
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
Eleva-se ao quadrado ambos lados dessa desigualdade para obter 
\[
 #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
e junta-se os termos para obter
\[
 \sqrt{#x#} \ge 1
\]
o que é claramente verdadeiro para qualquer $#x#\ge 1$.
\end{proof}

Inicia-se restringindo o problema um pouco e assume-se que 
 $2^{#r#} \le
#x# < 2^{#r#+1}$, tal que $\lfloor\log #x#\rfloor=#r#$, isto é, #x# é um
inteiro com $#r#+1$ bits na sua representação binária. Podemos fazer que
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$.  Agora, #x'# satisfaz
as condições do \lemref{root}, então $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Além disso, 
 #x'# tem todos os seu bits de baixa ordem $\lfloor #r#/2\rfloor$ 
iguais a 0, então só há 
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
valores possíveis para #x'#.  Isso significa que podemos um array #sqrttab#,
que guarda o valor de  $\lfloor\sqrt{#x'#}\rfloor$ para cada possível valor de 
#x'#.  Um pouco mais precisamente, temos 
\[
   #sqrttab#[i] 
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
Desse modo, $#sqrttab#[i]$ tem uma diferença de até 2 em relação a $\sqrt{#x#}$ para todo 
$#x#\in\{i2^{\lfloor #r#/2\rfloor},\ldots,(i+1)2^{\lfloor #r#/2\rfloor}-1\}$.
De outra forma, a entrada do array
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ é igual a 
$\lfloor\sqrt{#x#}\rfloor$, a
$\lfloor\sqrt{#x#}\rfloor-1$ ou a
$\lfloor\sqrt{#x#}\rfloor-2$.  A partir de #s# podemos determinar o valor 
 $\lfloor\sqrt{#x#}\rfloor$ ao incrementar 
#s# até u
$(#s#+1)^2 > #x#$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x,r)}
\cppimport{ods/FastSqrt.sqrt(x,r)}
\notpcode{
Isso somente funciona para 
 $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ e 
#sqrttab# é uma tabela especial que funciona somente para um valor especial de 
$#r#=\lfloor\log #x#\rfloor$.  Para superar isso, poderíamos computar 
$\lfloor\log #n#\rfloor$ arrays #sqrttab# diferentes, um para cada valor possível de 
$\lfloor\log #x#\rfloor$. Os tamanhos dessas tabelas formam uma sequência exponencial cujo maior valor é até $4\sqrt{#n#}$, então o tamanho total de todas as tabelas é $O(\sqrt{#n#})$.

Porém, acontece que mais um array #sqrttab# é desnecessário;
 somente precisamos de um array #sqrttab# para o valor $#r#=\lfloor\log
#n#\rfloor$.  Qualquer valor #x# com $\log#x#=#r'#<#r#$ pode ser \emph{atualizado}
ao multiplicar 
#x# por $2^{#r#-#r'#}$ e usar a equação 
\[
    \sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
A quantidade 
$2^{#r#-#r#'}x$ está no intervalo 
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ então podemos procurar sua raiz quadrada em 
#sqrttab#.  O código a seguir implementa essa ideia para computar 
$\lfloor\sqrt{#x#}\rfloor$ para todo inteiro não negativo #x# no intervalo 
$\{0,\ldots,2^{30}-1\}$ usando um array, #sqrttab#, de tamanho $2^{16}$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x)}
\cppimport{ods/FastSqrt.sqrt(x)}
\notpcode{
  Algo que tínhamos como certo até agora depende de como computamos
$#r#'=\lfloor\log#x#\rfloor$.  De novo, esse problema pode ser resolvido 
com um array, #logtab#, de tamanho $2^{#r#/2}$.  Nesse caso, o código 
é particularmente simples, pois 
 $\lfloor\log #x#\rfloor$ é somente o índice do bit 1 mais significativo na representação binária de #x#. 
Isso significa que, para 
 $#x#>2^{#r#/2}$, podemos deslocar à direita os bits de 
#x# por $#r#/2$ posições antes de usá-lo como um índice em #logtab#.
O código a seguir faz isso usando um array
 #logtab# de tamanho $2^{16}$ para computar 
$\lfloor\log #x#\rfloor$ para todo #x# no intervalo $\{1,\ldots,2^{32}-1\}$.
} % notpcode
\javaimport{ods/FastSqrt.log(x)}
\cppimport{ods/FastSqrt.log(x)}
\notpcode{
  Finalmente, para completude, incluímos o código a seguir que inicializa 
 #logtab# e #sqrttab#:
} % notpcode
\javaimport{ods/FastSqrt.inittabs()}
\cppimport{ods/FastSqrt.inittabs()}
\notpcode{
Para resumir, as computações feitas pelo método
 #i2b(i)# podem ser implementadas em tempo constante no modelo 
word-RAM usando $O(\sqrt{n})$ de memória extra para guardar 
os arrays #sqrttab# e #logtab#. Esses arrays podem ser reconstruídos
quando 
 #n# aumenta ou diminui por um fator de dois e o custo dessa reconstrução 
pode ser amortizado sobre o número de 
operações #add(i,x)# e 
#remove(i)# que causaram a mudança do mesmo jeito que o custo de
#resize()# é analisado na implementação da #ArrayStack#.
} % notpcode

\section{Discussão e Exercícios}

A maior parte das estruturas de dados descritas neste capítulo são folclore.
Elas podem ser encontradas em implementações de pelo menos 30 anos atrás.
Por exemplo, implementações de stacks, queues e deques que generalizam
facilmente para as estruturas 
#ArrayStack#, #ArrayQueue# e #ArrayDeque# descritas
aqui, são discutidas por Knuth \cite[Section~2.2.2]{k97v1}.

Brodnik \etal\ \cite{bcdms99} parecem terem sido os primeiros a descrever 
a #RootishArrayStack# e provar um limitante inferior de $\sqrt{n}$ como descrito na 
\secref{rootishspaceusage}.  Eles também apresentam uma estrutura diferente que
usa uma escolha de tamanhos de blocos mais sofisticada a fim de evitar a computação de raízes quadradas usando o método 
#i2b(i)#. Com o esquema deles, o bloco contendo 
#i# é o bloco $\lfloor\log (#i#+1)\rfloor$, que é simplesmente o índice do primeiro bit 1 na representação binária de 
$#i#+1$.  Algumas arquiteturas de computadores provêm uma instrução para computar o índice do primeiro bit 1 em um inteiro. 
\javaonly{Em 
Java, a classe #Integer# provê um método chamado #numberOfLeadingZeros(i)#
a partir do qual pode-se facilmente computar
 $\lfloor\log (#i#+1)\rfloor$.}

 Uma estrutura relacionada à 
 #RootishArrayStack# é o \emph{vetor em camadas}
\index{vetor em camadas}%
de Goodrich e Kloss \cite{gk99}.
Essa estrutura suporta as operações 
#get(i,x)# e #set(i,x)# em tempo constante e 
#add(i,x)# e #remove(i)# em $O(\sqrt{#n#})$ de tempo.
Esses tempos de execução são similares ao que pode ser conseguido com uma implementação mais cuidadosa de uma
#RootishArrayStack# discutida no \excref{rootisharraystack-fast}.

\javaonly{
\begin{exc}
  Na implementação da 
  #ArrayStack#, após a primeira chamada a #remove(i)#,
  o array de apoio, #a#, contém $#n#+1$ valores 
  não #null# apesar do fato de que 
  a #ArrayStack# somente contém #n# elementos. Onde está o valor 
  não #null# extra?  Discuta as consequências que esse valor não #null#
  possa ter no gerenciador de memória da Java Runtime Environment.
  \index{Java Runtime Environment}%
  \index{gerenciador de memória}%
\end{exc}
}

\begin{exc}
  O método da 
  #List# #addAll(i,c)# insere todos os elementos da #Collection#
  #c# na posição #i# da lista.  (O método #add(i,x)# é um caso especial onde 
  $#c#=\{#x#\}$.)  Explique porque, para as estruturas de dados deste capítulo, não é eficiente implementar #addAll(i,c)# fazendo chamadas repetidas a 
  #add(i,x)#.  Projete e codifique uma implementação mais eficiente. 
\end{exc}

\begin{exc}
  Projete e implemente uma 
\emph{#RandomQueue#}.
  \index{RandomQueue@#RandomQueue#}%
Essa é uma implementação da interface
#Queue# na qual a operação #remove()# remove um elemento que é escolhido de uma distribuição aleatória uniforme entre todos os elementos atualmente na queue. 
  (Considere uma #RandomQueue# como sendo uma sacola na qual 
  podemos adicionar elementos ou por a mão e, sem ver, remover algum elemento aleatório.)
  As operações
  #add(x)# e #remove()# em uma #RandomQueue# deve rodar em um tempo constante amortizado por operação.
\end{exc}

\begin{exc}
  Projete e implemente uma 
 #Treque# (uma queue com três pontas). 
  \index{Treque@#Treque#}%
Essa é uma implementação de uma #List#
  na qual #get(i)# e #set(i,x)# rodam em tempo constante
  e #add(i,x)# e #remove(i)# rodam em tempo 
  \[
     O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace .
  \]
  Em outras palavras, modificações são rápidas se elas estão perto ou do fim, ou do meio da lista.
\end{exc}

\begin{exc}
  Implemente um método
#rotate(a,r)# que ``rotaciona'' o array #a# de tal forma que
  #a[i]# moves para $#a#[(#i#+#r#)\bmod #a.length#]$, para todo 
  $#i#\in\{0,\ldots,#a.length#\}$.
\end{exc}

\begin{exc}
  Implemente um método 
#rotate(r)# que ``rotaciona'' uma #List# tal que o item da lista 
  #i# se torna o item $(#i#+#r#)\bmod #n#$.  Ao rodar em uma 
   #ArrayDeque# ou uma #DualArrayDeque#, #rotate(r)# deve rodar em 
  tempo $O(1+\min\{#r#,#n#-#r#\})$.
\end{exc}

\begin{exc}
  \pcodeonly{Esse exercício não é incluído na edição da linguagem\lang\ .}
  \notpcode{
    Modifique a implementação
  da #ArrayDeque# tal que o deslocamento feito por
   #add(i,x)#, #remove(i)# e #resize()# é realizado usando o método 
  rápido #System.arraycopy(s,i,d,j,n)#.}
\end{exc}

\begin{exc}
  Modifique a implementação da 
  #ArrayDeque# tal que não use o operador 
  #%# (que é caro em alguns sistemas). Em vez disso, deve fazer uso do
  fato que se #a.length# é uma potência de 2,
  então 
  \[  #k%a.length#=#k&(a.length-1)# \enspace .
  \]
  (Aqui, #&# é o operador and bit-a-bit.)
\end{exc}

\begin{exc}
  Projete e implemente uma variante da 
 #ArrayDeque# que não faz uso de aritmética modular. Em vez disso, todos os dados ficam em um bloco consecutivo, em ordem, dentro de um array. 
  Quando os dados sobrescrevem o começo ou o fim desse array, uma operação #rebuild()# modificada é realizada.
  O custo amortizado de todas as operações deve ser o mesmo que em uma 
  #ArrayDeque#.

  \noindent Dica: fazer isso funcionar tem a ver com a forma que você implementa a operação #rebuild()#. Deve-se fazer #rebuild()# para colocar dados na estrutura de dados em um estado onde os dados não podem sair pelas duas pontas até que pelo menos
  $#n#/2$ operações tenham sido realizadas. 

  Teste o desempenho da sua implementação em comparação à 
 #ArrayDeque#. Otimize sua implementação (usando #System.arraycopy(a,i,b,i,n)#)
  e veja se consegue fazê-la mais rápida que a implementação da #ArrayDeque#.
\end{exc}

\begin{exc}
  Projete e implemente uma versão de uma #RootishArrayStack# 
  que desperdiça somente $O(\sqrt{#n#})$ de espaço, mas que 
  pode realizar as operações #add(i,x)#
  e #remove(i,x)# de $O(1+\min\{#i#,#n#-#i#\})$ de tempo.
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
  Projete e implemente uma versão de 
 #RootishArrayStack# que desperdiça somente 
   $O(\sqrt{#n#})$ de espaço, mas que pode rodar as operações #add(i,x)#
  e #remove(i,x)# em $O(1+\min\{\sqrt{#n#},#n#-#i#\})$
  de tempo. (Para uma ideia de como fazer isso, veja a \secref{selist}.)
\end{exc}

\begin{exc}
  Projete e implemente uma versão de 
uma #RootishArrayStack# que desperdiça somente 
  $O(\sqrt{#n#})$ de espaço, mas que pode rodar as operações #add(i,x)# e 
  #remove(i,x)# em $O(1+\min\{#i#,\sqrt {#n#},#n#-#i#\})$ de tempo.
  (Veja a \secref{selist} para uma ideia de como conseguir isso.)
\end{exc}

\begin{exc}
  Projete e implemente uma #CubishArrayStack#.
  \index{CubishArrayStack@#CubishArrayStack#}%
Essa estrutura de três níveis implementa a interface 
  #List# desperdiçando $O(#n#^{2/3})$ de espaço.
  Nessa estrutura, #get(i)# e #set(i,x)# funcionam em tempo constante; enquanto 
  #add(i,x)# e #remove(i)# levam $O(#n#^{1/3})$ de tempo amortizado.
\end{exc}

