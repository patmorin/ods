\chapter{Linked Lists}
\chaplabel{linkedlists}

In this chapter, we continue to study implementations of the #List#
interface, this time using pointer-based data structures rather than
arrays.  The structures in this chapter are made up of nodes that
contain the list items.  The nodes are linked together into a sequence
using references (pointers).  We first study singly-linked lists, which
can implement #Stack# and (FIFO) #Queue# operations in constant time
per operation.

Compared to array-based list implementations, linked lists have their
advantages and disadvantages.  The primary disadvantage is that we
lose the ability to access any element using #get(i)# or #set(i,x)# in
constant time.  Instead, we have to walk through the list, one element
at a time, until we reach the #i#th element.  The primary advantage is
that they are more dynamic:  Given a reference to any list node #u#, we
can delete #u# or insert a node adjacent to #u# in constant time. This
is true no matter where #u# is in the list.


\section{#SLList#: A Singly-Linked List}
\seclabel{sllist}

An #SLList# (singly-linked list) is a sequence of #Node#s.  Each node
#u# stores a data value #u.x# and a reference #u.next# to the next node in
the sequence.  For the last node #w# in the sequence, $#w.next# = #null#$

% TODO: Remove constructors from SLList.Node
\javaimport{ods/SLList.Node}

For efficiency, an #SLList# uses variables #head# and #tail# to keep
track of the first and last node in the sequence, as well as an integer
#n# to keep track of the length of the sequence:
\javaimportwithclass{ods/SLList.head.tail.n}
A sequence of #Stack# and #Queue# operations on an #SLList# is
illustrated in \figref{sllist}.

\begin{figure}
  \begin{center}
    \includegraphics{figs/sllist}
  \end{center}
  \caption{A sequence of #Queue# (#add(x)# and #remove()#) and #Stack# (#push(x)# and #pop()#) operations on an #SLList#.}
  \figlabel{sllist}
\end{figure}


An #SLList# can efficiently implement the #Stack# operations #push()#
and #pop()# by adding and removing elements at the head of the sequence.
The #push()# operation simply creates a new node #u# with data value #x#,
sets #u.next# to the old head of the list and makes #u# the new head
of the list. Finally, it increments #n# since the size of the #SLList#
has increased by one:

\javaimport{ods/SLList.push(x)}

The #pop()# operation, after checking that the #SLList# is not empty,
removes the head by setting $#head#=#head.next#$ and decrementing #n#.
A special case occurs when the last element is being removed, in which case #tail# is set to #null#:

\javaimport{ods/SLList.pop()}

Clearly both the #push(x)# and #pop()# operations run in $O(1)$ time.

\subsection{Queue Operations}

An #SLList# can also efficiently implement the FIFO queue operations #add(x)# and #remove()#.  Removals are done from the head of the list, and are identical to the #pop()# operation:

\javaimport{ods/SLList.remove()}

Additions, on the other hand, are done at the tail of the list.  In most
cases, this is done by setting $#tail.next#=#u#$, where #u# is the newly
created node that contains #x#.  However, a special case occurs when
$#n#=0$, in which case $#tail#=#head#=#null#$.  In this case, both #tail#
and #head# are set to #u#.

\javaimport{ods/SLList.add(x)}

Clearly, both #add(x)# and #remove()# take constant time.

\subsection{Summary}

The following theorem summarizes the performance of an #SLList#:

\begin{thm}\thmlabel{sllist}
  An #SLList# implements the #Stack# and (FIFO) #Queue# interfaces.  
  The #push(x)#, #pop()#, #add(x)# and #remove()# operations run
  in $O(1)$ time per operation.
\end{thm}

An #SLList# comes very close to implementing the full set of #Deque#
operations.  The only missing operation is removal from the tail
of an #SLList#.  Removing from the tail of an #SLList# is difficult
because it requires updating the value of #tail# so that it points to
the node #w# that precedes #tail# in the #SLList#; this is the node #w#
such that $#w.next#=#tail#$.  Unfortunately, the only way to get to #w#
is by traversing the #SLList# starting at #head# and taking $#n#-2$ steps.

\section{#DLList#: A Doubly-Linked List}
\seclabel{dllist}

A #DLList# (doubly-linked list) is very similar to an #SLList# except
that each node #u# in a #DLList# has references to both the node #u.next#
that follows it and the node #u.prev# that precedes it.

\javaimport{ods/DLList.Node}

When implementing an #SLList#, we saw that there were always some special
cases to worry about. For example, removing the last element from an
#SLList# or adding an element to an empty #SLList# requires special
care so that #head# and #tail# are correctly updated.  In a #DLList#,
the number of these special cases increases considerably.  Perhaps the
cleanest way to take care of all these special cases in a #DLList# is to
introduce a #dummy# node. This is a node that does not contain any data,
but acts as a placeholder so that there are no special nodes; every node
has both a #next# and a #prev#, with #dummy# acting as the node that
follows the last node in the list and that precedes the first node in
the list.  In this way, the nodes of the list are (doubly-)linked into
a cycle, as illustrated in \figref{dllist}.

\begin{figure}
  \begin{center}
    \includegraphics{figs/dllist2}
  \end{center}
  \caption{A #DLList# containing a,b,c,d,e.}
  \figlabel{dllist}
\end{figure}


%TODO: Remove constructors from class Node

\javaimportwithclass{ods/DLList.n.dummy.DLList()}

Finding the node with a particular index in a #DLList# is easy;  we can
either start at the head of the list (#dummy.next#) and work forward,
or start at the tail of the list (#dummy.prev#) and work backward.
This allows us to reach the #i#th node in $O(1+\min\{#i#,#n#-#i#\})$ time:

\javaimport{ods/DLList.getNode(i)}

The #get(i)# and #set(i,x)# operations are now also easy.  We first find the #i#th node and then get or set its #x# value:

\javaimport{ods/DLList.get(i).set(i,x)}

The running time of these operations is dominated by the time it takes
to find the #i#th node, and is therefore $O(1+\min\{#i#,#n#-#i#\})$.

\subsection{Adding and Removing}

If we have a reference to a node #w# in a #DLList# and we want to insert a
node #u# before #w#, then this is just a matter of setting $#u.next#=#w#$,
$#u.prev#=#w.prev#$, and then adjusting #u.prev.next# and #u.next.prev#.
Thanks to the dummy node, there is no need to worry about #w.prev#
or #w.next# not existing.

\javaimport{ods/DLList.addBefore(w,x)}

Now, the list operation #add(i,x)# is trivial to implement.  We find the
#i#th node in the #DLList# and insert a new node #u# that contains #x#
just before it.

\javaimport{ods/DLList.add(i,x)}

The only non-constant part of the running time of #add(i,x)# is the time
it takes to find the #i#th node (using #getNode(i)#).  Thus, #add(i,x)#
runs in $O(1+\min\{#i#, #n#-#i#\})$ time.

Removing a node #w# from a #DLList# is easy.  We need only adjust pointers
at #w.next# and #w.prev# so that they skip over #w#.  Again, the use of the dummy node eliminates the need to consider any special cases:

\javaimport{ods/DLList.remove(w)}

Now the #remove(i)# operation is trivial. We find the node with index #i# and remove it:

\javaimport{ods/DLList.remove(i)}

Again, the only expensive part of this operation is finding the #i#th node
using #getNode(i)#, so #remove(i)# runs in $O(1+\min\{#i#, #n#-#i#\})$
time.

\subsection{Summary}

The following theorem summarizes the performance of a #DLList#:

\begin{thm}\thmlabel{dllist}
  A #DLList# implements the #List# interface.  
  The #get(i)#, #set(i,x)#, #add(i,x)# and #remove(i)# operations run
  in $O(1+\min\{#i#,#n#-#i#\})$ time per operation.
\end{thm}

It is worth noting that, if we ignore the cost of the #getNode(i)#
operation, then all operations on a #DLList# take constant time.
Thus, the only expensive part of operations on a #DLList# is finding
the relevant node.  Once we have the relevant node, adding, removing,
or accessing the data at that node take only constant time.

This is in sharp contrast to the array-based #List# implementations of
\chapref{arrays}. In those implementations, the relevant array
item can be found in constant time. However, addition or removal requires
shifting elements in the array and, in general, takes non-constant time.

For this reason, linked list structures are well-suited to applications
where references to list nodes can be obtained through external means.
An example of this is the #LinkedHashSet# data structure found in the
Java Collections Framework, in which a set of items is stored in a
doubly-linked list and the nodes of the doubly-linked list are stored
in a hash table (discussed in \chapref{hashing}).  When elements
are removed from a #LinkedHashSet#, the hash table is used to find the
relevant list node in constant time and then the list node is deleted
(also in constant time).


\section{#SEList#: A Space-Efficient Linked List}
\seclabel{selist}

One of the drawbacks of linked lists (besides the time it takes to
access elements that are deep within the list) is their space usage.
Each node in a #DLList# requires an additional two references to the next
and previous nodes in the list.  Two thirds of the fields in a #Node#
are dedicated to maintaining the list and only one third of the fields
are for storing data!

An #SEList# (space-efficient list) reduces this wasted space using
a simple idea: Rather than store individual elements in a #DLList#,
we store a block (array) containing several items. More precisely, an
#SEList# is parameterized by a \emph{block size} #b#. Each individual
node in an #SEList# stores a block that can hold up to #b+1# elements.

It will turn out, for reasons that become clear later, that it will
be helpful if we can do #Deque# operations on each block.  The data
structure we choose for this is a #BDeque# (bounded deque), derived
from the #ArrayDeque# structure described in \secref{arraydeque}.
The #BDeque# differs from the #ArrayDeque# in one small way: When a
new #BDeque# is created, the size of the backing array #a#
is fixed at #b+1# and it never grows or shrinks.
The important property of a #BDeque# is that it allows for addition or
removal of elements at either the front or back in constant time. This
will be useful as elements are shifted from one block to another.


\javaimport{ods/SEList.BDeque}

An #SEList# is then a doubly-linked list of blocks:

\javaimport{ods/SEList.Node}
\javaimportwithclass{ods/SEList.n.dummy}

\subsection{Space Requirements}

An #SEList# places very tight restrictions on the number of elements
in a block: Unless a block is the last block, then that block contains
at least $#b#-1$ and at most $#b#+1$ elements.  This means that, if an
#SEList# contains #n# elements, then it has at most
\[
    #n#/(#b#-1) + 1 = O(#n#/#b#)
\]
blocks.  The #BDeque# for each block contains an array of length
$#b#+1$ but, for all blocks except the last, at most a constant amount
of space is wasted in this array.  The remaining memory used by a
block is also constant.  This means that the wasted space in an
#SEList# is only $O(#b#+#n#/#b#)$.  By choosing an appropriate value
of #b# (ideally in $\Theta(\sqrt{#n#})$) we can make the
space-overhead of an SEList approach the $\Omega(\sqrt{#n#})$ lower
bound\footnote{See \secref{rootishspaceusage} for a proof of this lower
bound.}.

\subsection{Finding Elements}

The first challenge we face with an #SEList# is finding the list item
with a given index #i#.  Note that the location of an element consists
of two parts: The node #u# that contains the block that contains the
element as well as the index #j# of the element within its block:

\javaimport{ods/SEList.Location}

To find the block that contains a particular element, we proceed in the
same way as in a #DLList#.  We either start at the front of the list and
traverse in the forward direction or at the back of the list and traverse
backwards until we reach the node we want.  The only difference is that,
each time we move from one node to the next, we skip over a whole block
of elements.

\javaimport{ods/SEList.getLocation(i)}

Remember that, with the exception of at most one block, each block
contains at least $#b#-1$ elements, so each step in our search gets
us $#b#-1$ elements closer to the element we are looking for.  If we
are searching forward, this means we reach the node we want after
$O(1+#i#/#b#)$ steps.  If we search backwards, we reach the node we want
after $O(1+(#n#-#i#)/#b#)$ steps.  The algorithm takes the smaller of
these two quantities depending on the value of #i#, so the time to locate
the item with index #i# is $O(1+\min\{#i#,#n#-#i#\}/#b#)$.

Once we know how to locate the item with index #i#, the #get(i)# and
#set(i,x)# operations translate into getting or setting a particular
index in the correct block:

\javaimport{ods/SEList.get(i).set(i,x)}

These operations are dominated by the time it takes to locate the item,
so they also run in $O(1+\min\{#i#,#n#-#i#\}/#b#)$ time.

\subsection{Adding an Element}

Where things start to get complicated is in adding elements.  Before
considering the general case, we first consider the easier operation,
#add(x)#, in which #x# is added to the end of the list.  In this case,
if the last block is full (or does not exist because there are no
blocks yet), then we first allocate a new block and append it to the
list of blocks.  Now that we are sure that the last block exists and
is not full, we append #x# to the last block.

\javaimport{ods/SEList.add(x)}

Things get more complicated when we add to the interior of the list
using #add(i,x)#.  We first locate #i# to get the node #u# whose block
contains the #i#th list item.  The problem is that we want to insert
#x# into #u#'s block, but we have to be prepared for the case where
#u#'s block already contains $#b#+1$ elements, so that it is full and
there is no room for #x#.

Let $#u#_0,#u#_1,#u#_2,\ldots$ denote #u#, #u.next#, #u.next.next#,
and so on.  We explore $#u#_0,#u#_1,#u#_2,\ldots$ looking for a node
that can provide space for #x#.  Three cases can occur during our
space exploration (see \figref{selist-add}):

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics{figs/selist-add-a}\\[4ex]
      \includegraphics{figs/selist-add-b}\\[4ex]
      \includegraphics{figs/selist-add-c}\\
    \end{tabular}
  \end{center}
  \caption{The three cases that occur during the addition of an item #x# in the interior of an #SEList#.  (This #SEList# has block size $#b#=3$.)}
  \figlabel{selist-add}
\end{figure}


\begin{enumerate}
\item We quickly (in $r+1\le #b#$ steps) find a node $#u#_r$ whose block
is not full.  In this case, we perform $r$ shifts of an element from
one block into the next, so that the free space in $#u#_r$ becomes a
free space in $#u#_0$.  We can then insert #x# into $#u#_0$'s block.

\item We quickly (in $r+1\le #b#$ steps) run off the end of the list
of blocks.  In this case, we add a new empty block to the end of the
list of blocks and proceed as in the first case.

\item After #b# steps we do not find any block that is not full.
In this case, $#u#_0,\ldots,#u#_{#b#-1}$ is a sequence of #b# blocks
that each contain $#b#+1$ elements.  We insert a new block $#u#_{#b#}$
at the end of this sequence and \emph{spread} the original $#b#(#b#+1)$
elements so that each block of $#u#_0,\ldots,#u#_{#b#}$ contains exactly
#b# elements.  Now $#u#_0$'s block contains only #b# elements so it has
room for us to insert #x#.
\end{enumerate}

\javaimport{ods/SEList.add(i,x)}

The running time of the #add(i,x)# operation depends on which of
the three cases above occurs.  Cases~1 and 2 involve examining and
shifting elements through at most #b# blocks and take $O(#b#)$ time.
Case~3 involves calling the #spread(u)# method, which  moves $#b#(#b#+1)$
elements and takes $O(#b#^2)$ time.  If we ignore the cost of Case~3
(which we will account for later with amortization) this means that
the total running time to locate #i# and perform the insertion of #x#
is $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$.

\subsection{Removing an Element}

Removing an element, using the #remove(i)# method from an #SEList#
is similar to adding an element.  We first locate the node #u# that
contains the element with index #i#. Now, we have to be prepared for
the case where we cannot remove an element from #u# without causing #u#'s
block to have size less than $#b#-1$, which is not allowed.

Again, let $#u#_0,#u#_1,#u#_2,\ldots$ denote #u#, #u.next#, #u.next.next#,
We examine $#u#_0,#u#_1,#u#_2,\ldots$ in order looking for a node from
which we can borrow an element to make the size of $#u#_0$'s block larger
than $#b#-1$.  There are three cases to consider
(see \figref{selist-remove}): 

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics{figs/selist-remove-a}\\[4ex]
      \includegraphics{figs/selist-remove-b}\\[4ex]
      \includegraphics{figs/selist-remove-c}\\
    \end{tabular}
  \end{center}
  \caption{The three cases that occur during the removal of an item #x# in the interior of an #SEList#.  (This #SEList# has block size $#b#=3$.)}
  \figlabel{selist-remove}
\end{figure}


\begin{enumerate}
\item We quickly (in $r+1\le #b#$ steps) find a node whose block contains
more than $#b#-1$ elements. In this case, we perform $r$ shifts of an
element from one block into the previous, so that the extra element in
$#u#_r$ becomes an extra element in $#u#_0$.  We can then remove the
appropriate element from $#u#_0$'s block.

\item We quickly (in $r+1\le #b#$ steps) run off the end of the list of blocks.
In this case, $#u#_r$ is the last block, and there is no requirement that
$#u#_r$'s block contain at least $#b#-1$ elements.  Therefore, we proceed
as above, borrowing an element from $#u#_r$ to make an extra element in
$#u#_0$.  If this causes $#u#_r$'s block to become empty, then we remove it.

\item After #b# steps we do not find any block containing more than
$#b#-1$ elements.  In this case, $#u#_0,\ldots,#u#_{#b#-1}$ is a sequence
of #b# blocks that each contain $#b#-1$ elements.  We \emph{gather}
these $#b#(#b#-1)$ elements into $#u#_0,\ldots,#u#_{#b#-2}$ so that each
of these $#b#-1$ blocks contains exactly #b# elements and we remove
$#u#_{#b#-1}$, which is now empty.  Now $#u#_0$'s block contains #b#
elements so we can remove the appropriate element from it.
\end{enumerate}

\javaimport{ods/SEList.remove(i)}

Like the #add(i,x)# operation, the running time of the #remove(i)#
operation is $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$ if we ignore the cost of
the #gather(u)# method that occurs in Case~3.

\subsection{Amortized Analysis of Spreading and Gathering}

Next, we consider the cost of the #gather(u)# and #spread(u)# methods that may be executed by the #add(i,x)# and #remove(i)# methods.  For completeness, here they are:

\javaimport{ods/SEList.spread(u)}
\javaimport{ods/SEList.gather(u)}

The running time of each of these methods is dominated by the two
nested loops.  Both the inner loop and outer loop execute at most
$#b#+1$ times, so the total running time of each of these methods
is $O((#b#+1)^2)=O(#b#^2)$. However, the following lemma shows that
these methods execute on at most one out of every #b# calls to #add(i,x)#
or #remove(i)#.

\begin{lem}\lemlabel{selist-amortized}
  If an empty #SEList# is created and any sequence of $m\ge 1$ calls
  to #add(i,x)# and #remove(i)# are performed, then the total time
  spent during all calls to #spread()# and #gather()# is $O(#b#m)$.
\end{lem}

\begin{proof}
  We will use the potential method of amortized analysis.  We say that
  a node #u# is \emph{fragile} if #u#'s block does not contain #b#
  elements (so that #u# is either the last node, or contains $#b#-1$
  or $#b#+1$ elements).  Any node whose block contains #b# elements is
  \emph{strong}. Define the \emph{potential} of an #SEList# as the number
  of fragile nodes it contains.  We will consider only the #add(i,x)#
  operation and its relation to the number of calls to #spread(u)#.
  The analysis of #remove(i)# and #gather(u)# is identical.

  Notice that, if Case~1 occurs during the #add(i,x)# method, then
  only one node, $#u#_r$ has the size of its block changed. Therefore,
  at most one node, namely $#u#_r$, goes from being strong to being
  fragile.  If Case~2 occurs, then a new node is created, and this node
  is fragile, but no other node changes sizes, so the number of fragile
  nodes increases by one.  Thus, in either Case~1 or Case~2 the potential
  of the SEList increases by at most 1.

  Finally, if Case~3 occurs, it is because $#u#_0,\ldots,#u#_{#b#-1}$
  are all fragile nodes.  Then $#spread(#u_0#)#$ is called and these #b#
  fragile nodes are replaced with $#b#+1$ strong nodes.  Finally, #x#
  is added to $#u#_0$'s block, making $#u#_0$ fragile.  In total the
  potential decreases by $#b#-1$.

  In summary, the potential starts at 0 (there are no nodes in the list).
  Each time Case~1 or Case~2 occurs, the potential increases by at
  most 1.  Each time Case~3 occurs, the potential decreases by $#b#-1$.
  The potential (which counts the number of fragile nodes) is never
  less than 0.  We conclude that, for every occurence of Case~3, there
  are at least $#b#-1$ occurences of Case~1 or Case~2.  Thus, for every
  call to #spread(u)# there are at least #b# calls to #add(i,x)#.  This
  completes the proof.
\end{proof}

\subsection{Summary}

The following theorem summarizes the performance of the #SEList# data
structure:

\begin{thm}\thmlabel{selist}
  An #SEList# implements the #List# interface.  Ignoring the cost of
  calls to #spread(u)# and #gather(u)#, an #SEList# with block size #b#
  supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1+\min\{#i#,#n#-#i#\}/#b#)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$ time per operation.
  \end{itemize}
  Furthermore, beginning with an empty #SEList#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(#b#m)$
  time spent during all calls to #spread(u)# and #gather(u)#.

  The space (measured in words)\footnote{Recall \secref{model} for a
  discussion of how memory is measured.} used by an #SEList#
  that stores #n# elements is $#n# +O(#b# + #n#/#b#)$.
\end{thm}

The #SEList# is a tradeoff between an #ArrayList# and a #DLList# where
the relative mix of these two structures depends on the block size
#b#.  At the extreme $#b#=2$, each #SEList# node stores at most 3
values, which is really not much different than a #DLList#. At the other
extreme, $#b#>#n#$, all the elements are stored in a single array,
just like in an #ArrayList#.  In between these two extremes lies a
tradeoff between the time it takes to add or remove a list item and
the time it takes to locate a particular list item.

\section{Exercises}

\begin{enumerate}
\item Why is it not possible, in a #SLList# to use a dummy node to avoid
  all the special cases that occur in the operations #push(x)#, #pop()#,
  #add(x)#, and #remove()#?

\item Describe and implement the #List# operations #get(i)#, #set(i,x)#,
  #add(i,x)# and #remove(i)# on an #SLList#.  Each of these operations
  should run in $O(1+#i#)$ time.

\end{enumerate}

