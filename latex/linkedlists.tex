\chapter{Listas Ligadas}
\chaplabel{linkedlists}

\index{linked list}%
\index{lista ligada}%
Neste capítulo, continuamos a estudar implementações da 
interface #List# e agora usando estruturas de dados baseadas em ponteiros
em vez de arrays. As estruturas neste capítulo são compostas de nodos (ou nós)
que contêm os itens da lista. Usando referências (ponteiros), os nodos são
ligados entre si em uma sequência. Primeiro estudamos listas simplesmente ligadas (listas com somente uma ligação), que podem implementar operação da #Stack# e #Queue# (FIFO) em tempo constante por operação e então seguimos para listas
duplamente ligadas, que podem implementar operações #Deque# em tempo constante.

Listas ligadas têm vantagens e desvantagens quando comparadas a implementações
baseadas em arrays da interface #List#. A principal desvantagem é que perdemos
a habilidade de acessar elementos usando
#get(i)# ou #set(i,x)# em tempo constante.
Em vez disso, temos que percorrer a lista, um elemento por vez, até chegarmos no #i#-ésimo elemento. A principal vantagem é que são mais dinâmicos: dada uma referência a qualquer nodo da lista #u#, podemos deletar #u# ou inserir um nodo adjacente a #u# em tempo constante. Isso vale não importa onde #u# esteja na lista.

\section{#SLList#: Uma Lista Simplesmente Ligada}
\seclabel{sllist}

\index{SLList@#SLList#}%
\index{linked list!singly-}%
\index{singly-linked list}%
\index{lista simplesmente ligada}%
Uma #SLList# (em inglês, singly-linked list) é uma 
sequência de #Node#s (em português, nodo). Cada nodo 
#u# guarda um valor de dado #u.x# e uma referência #u.next# 
ao próximo nodo na sequência.  
Para o último nodo #w# na sequência vale $#w.next# = #null#$

% TODO: Remove constructors from SLList.Node
\javaimport{ods/SLList.Node}
\cppimport{ods/SLList.Node}

Para eficiência, uma 
#SLList# usa variáveis #head# (cabeça) e #tail# (cauda) para guardar 
os acessos à primeiro e último nodos na sequência, assim como um inteiro
#n# para guardar o comprimento da sequência:
\codeimport{ods/SLList.head.tail.n}
Uma sequência de operações #Stack# e #Queue# em uma #SLList# está ilustrada 
na \figref{sllist}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/sllist}
  \end{center}
  \caption[Uma sequência de operações Queue e Stack em uma SLList]{Uma sequência de operações da #Queue# (#add(x)# e #remove()#) e da #Stack# (#push(x)# e #pop()#) em uma #SLList#.}
  \figlabel{sllist}
\end{figure}

Uma #SLList# pode implementar eficientemente as operações da #Stack# #push()#
e #pop()# ao adicionar e remover elementos na cabeça da sequência.
A operação 
#push()# simplesmente cria um novo nodo #u# com valor de dado #x#,
atribuído a #u.next# à antiga cabeça da lista e transforma #u# na nova cabeça da lista.
Finalmente, ela incrementa #n# pois o tamanho de #SLList# aumentou em uma unidade:

\codeimport{ods/SLList.push(x)}

A operação #pop()#, após verificar que a 
 #SLList# não está vazia, remove a cabeça fazendo
 $#head=head.next#$ e também decrementando #n#.

\codeimport{ods/SLList.pop()}

Claramente, ambas operações #push()# e #pop()# rodam em tempo #O(1)#.

\subsection{Operações de Queue}

Uma #SLList# também pode implementar as operações de queue FIFO #add(x)# e 
#remove()# em tempo constante. Remoções são feitas da cabeça da lista e são idênticas à operação #pop()#:

\codeimport{ods/SLList.remove()}

Adições, por outro lado, são feitas na cauda da lista. Na maior parte dos casos,
isso é feito ao atribuir $#tail.next#=#u#$, onde #u# é mais novo nodo que contém #x#. Entretanto, um caso especial ocorre quando $#n#=0$, no qual $#tail#=#head#=#null#$.  Nesse caso, tanto #tail#
quanto #head# são atribuídos a #u#.

\codeimport{ods/SLList.add(x)}

Claramente, ambos 
 #add(x)# e #remove()# levam tempo constante. 

\subsection{Resumo}

O teorema a seguir resume o desempenho de uma #SLList#:

\begin{thm}\thmlabel{sllist}
Uma #SLList# implementa as interfaces #Stack# e #Queue# (FIFO).  
  As operações #push(x)#, #pop()#, #add(x)# e #remove()# rodam em 
  tempo $O(1)$ por operação.
\end{thm}

Uma #SLList# quase implementa o conjunto completo de operações de uma #Deque#.
A única operação que falta é remoção da cauda de uma 
 #SLList#.
 Remover da cauda de um #SLList# é difícil porque requer a atualização 
 do valor da #tail# de forma que ela aponte ao nodo #w# 
 que precede #tail# na #SLList#; esse é o nodo #w# tal que 
$#w.next#=#tail#$.  Infelizmente, o único jeito de chegar a #w# é percorrendo
#SLList# a começar da #head# fazendo $#n#-2$ passos.

\section{#DLList#: Uma Lista Duplamente Ligada}
\seclabel{dllist}

\index{DLList@#DLList#}%
\index{doubly-linked list}%
\index{linked list!doubly-}%
\index{lista ligada!dupla}%
\index{lista duplamente ligada}%

Uma #DLList# (do inglês, doubly-linked list) é muito similar a uma #SLList# exceto que cada nodo #u# em uma #DLList# tem referências ao nodo que ele precede, #u.next#, e ao nodo que sucede, #u.prev#.

\javaimport{ods/DLList.Node}
\cppimport{ods/DLList.Node}

Ao implementar uma #SLList#, vimos que haviam vários casos especiais para tomar cuidado. Por exemplo, ao remover o último elemento de uma #SLList# ou adicionar um elemento a uma #SLList# vazia precisamos tomar cuidado especial para garantir
que #head# e #tail# sejam corretamente atualizados.
Em uma #DLList#, o número desses casos especiais aumentam consideravelmente.
Talvez o modo mais simples de cuidar de todos esses casos especiais em uma
#DLList# é introduzir um nodo #dummy#, também conhecido como \emph{sentinela}.
\index{dummy node}%
\index{nodo dummy}%
\index{sentinela}%
Esse nodo não contém nenhum dado, mas atua como um marcador para que não tenhamos nenhum nodo especial; todo nodo tem um #next# e um #prev#, 
com um #dummy# agindo como o nodo que sucede o último nodo 
na lista e que precede o primeiro nodo da lista.
Desse jeito, os nodos da lista são (duplamente) ligados em um ciclo, 
como ilustrado na \figref{dllist}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dllist2}
  \end{center}
  \caption[Uma DLList]{Uma #DLList# contendo a,b,c,d,e.}
  \figlabel{dllist}
\end{figure}

%TODO: Remove constructors from class Node

\codeimport{ods/DLList.n.dummy.DLList()}

Encontrar o nodo com um dado índice em uma #DLList# é fácil;
podemos começar na cabeça da lista (#dummy.next#) e ir para frente,
ou começar na cauda da lista (#dummy.next#) e ir para trás.
Isso permite alcançarmos o #i#-ésimo nodo em $O(1+\min\{#i#,#n#-#i#\})$ de tempo:

\codeimport{ods/DLList.getNode(i)}

As operações 
#get(i)# e #set(i,x)# também são fáceis agora. Primeiro achamos o #i#-ésimo nodo e então pegamos ou atribuímos seu valor #x#: 

\codeimport{ods/DLList.get(i).set(i,x)}

O tempo de execução dessas operações é dominado pelo tempo para
achar o #i#-ésimo nodo, e portanto é 
 $O(1+\min\{#i#,#n#-#i#\})$.

\subsection{Adicionando e Removendo}

Se temos uma referência a um nodo #w# em uma #DLList# e queremos inserir um nodo #u# antes de #w#, então isso é só uma questão de atribuir 
 $#u.next#=#w#$,
$#u.prev#=#w.prev#$ e então ajustar #u.prev.next# e  #u.next.prev#.  (Veja a \figref{dllist-addbefore}.)
Graças ao nodo dummy, não há necessidade de se preocupar sobre #w.prev#
existir ou não.

\codeimport{ods/DLList.addBefore(w,x)}

\begin{figure}
   \begin{center}
      \includegraphics[scale=0.90909]{figs/dllist-addbefore}
   \end{center}
   \caption[Adicionando em uma DLList]{Adicionando o nodo #u# antes do nodo #w# em uma #DLList#.}
   \figlabel{dllist-addbefore}
\end{figure}

Agora, a operação da lista #add(i,x)# é trivial para implementar. 
Achamos o #i#-ésimo nodo na #DLList# e inserimos um novo nodo #u# que contém
#x# logo antes dele.

\codeimport{ods/DLList.add(i,x)}

A única parte não constante do tempo de execução de #add(i,x)# é o tempo
que leva para achar o #i#-ésimo nodo (usando #getNode(i)#). Então, #add(i,x)#
roda em tempo $O(1+\min\{#i#, #n#-#i#\})$.

Remover um nodo #w# de uma #DLList# é fácil. Precisamos somente ajustar
ponteiros em #w.next# e #w.prev# tal que eles \emph{saltem} #w#.
De novo, o uso do nodo dummy elimina a necessidade de considerar casos especiais:

\codeimport{ods/DLList.remove(w)}

Agora a operação #remove(i)# é trivial. Achamos o nodo com índice #i# e o removemos:
\codeimport{ods/DLList.remove(i)}

Novamente, a única parte cara dessa operação é achar o #i#-ésimo nodo usando
 #getNode(i)# e então #remove(i)# roda em tempo $O(1+\min\{#i#, #n#-#i#\})$.

\subsection{Resumo}
O teorema a seguir resume o desempenho de uma #DLList#:

\begin{thm}\thmlabel{dllist}
  Uma #DLList# implementa a interface #List#. Nessa implementação,
  as operações #get(i)#, #set(i,x)#, #add(i,x)# e #remove(i)# rodam em 
tempo $O(1+\min\{#i#,#n#-#i#\})$ por operação.
\end{thm}

Vale notar que, se ignorarmos o custo de #getNode(i)#, então todas
as operações em uma #DLList# leva tempo constante.
Então, a única parte cara das operações em uma #DLList# é achar o nodo relevante.
Uma vez que temos o nodo de interesse, adicionar, remover ou acessar os
dados naquele nodo leva somente um tempo constante.

Há um nítido contraste em relação às implementações da #List# baseadas em arrays do \chapref{arrays};
naquelas implementações, o item buscado no array pode ser achado em tempo constante.
Entretanto, adicionar ou remover requer o deslocamento de elementos no array e, 
em geral, não leva tempo constante.

Por essa razão, as estruturas de listas ligadas são adequadas para
aplicações em que referências a nodos da lista podem ser obtidos por meios
externos.

\javaonly{Um exemplo disso é a estrutura de dados #LinkedHashSet# existente na 
Java Collections Framework, na qual um conjunto de itens é guardado em uma lista duplamente encadeada e os nodos dessa lista são guardados em uma tabela hash
(discutido no \chapref{hashing}).
Quando um elemento é removido de uma 
#LinkedHashSet#, a tabela hash é usada para achar o nodo em questão com tempo constante e então o nodo da lista é removido (também em tempo constante).}
\cpponly{Por exemplo, ponteiros aos nodos de uma lista ligada poderiam ser guardados em uma 
 #USet#.  Então para remover um item #x# de uma lista ligada,
 o nodo que contém #x# pode ser achado rapidamente usando
 #Uset# e o nodo pode ser removido da lista em tempo constante.} 

\section{#SEList#: Uma Lista Ligada Eficiente no Espaço}
\seclabel{selist}
\index{SEList@#SEList#}%
\index{linked list!space-efficient}%
\index{eficiente no espaço!lista ligada}
Uma das limitações das listas encadeadas (além do tempo que ela leva para
acessar elementos que estão no meio da lista) é o seu uso de espaço.
Cada nodo em uma #DLList# requer duas referências adicionais para o próximo nodo e para o nodo anterior na lista. Dois dos campos em um #Node# são dedicados a manter a lista, e somente um dos campos é para guardar dados!

Uma #SEList# (do inglês, space-efficient list) reduz esse espaço desperdiçado usando uma ideia simples: 
em vez de guardar elementos individuais em uma #DLList#,
guardamos um bloco (array) contendo vários itens.
Mais precisamente, uma 
#SEList# é parametrizada pelo tamanho do bloco \emph{block size} #b#.
Cada nodo individual em uma #SEList# guarda um bloco que pode guardar até #b+1# elementos.

Para razões que vão se tornar claras depois, será útil se podermos fazer operações de #Deque# em cada bloco.

A estrutura de dados que escolhemos para isso é uma 
#BDeque# (do inglês, bounded deque -- deque delimitado),
\index{BDeque@#BDeque#}%
\index{bounded deque}%
\index{deque!bounded}%
\index{deque!delimitado}%
derivado da estrutura #ArrayDeque#
descrita na \secref{arraydeque}. A #BDeque# difere do #ArrayDeque#
em um detalhe: quando uma nova #BDeque# é criada, o tamanho do array de apoio #a#
é fixo em #b+1# e nunca expande ou escolhe.
A propriedade mais importante de um #BDeque# é que permite a adição ou remoção
de elementos tanto na frente quanto atrás em tempo constante.
Isso será útil quando elementos forem deslocados de um bloco a outro.

\javaimport{ods/SEList.BDeque} 
\cppimport{ods/SEList.BDeque} 

\notpcode{
Uma 
#SEList# é então uma lista duplamente encadeada de blocos: 
} % notpcode
\javaimport{ods/SEList.Node}
\cppimport{ods/SEList.Node}
\javaimport{ods/SEList.n.dummy}
\cppimport{ods/SEList.n.dummy}

\pcodeonly{Uma #SEList# é somente uma lista duplamente encadeada de blocos. Além dos ponteiros #next# e #prev#, cada nodo #u# em uma #SEList# contém um #BDeque#, #u.d#. }

\subsection{Requisitos de Espaço}

Uma #SEList# impõe restrições bem firmes ao número de elementos em um bloco:
a não ser que o bloco seja o último, então esse bloco contém pelo menos
$#b#-1$ e até $#b#+1$ elementos. Isso significa que, se uma 
#SEList# contém #n# elementos, então ela tem até 
\[
    #n#/(#b#-1) + 1 = O(#n#/#b#)
\]
blocos.  A #BDeque# para cada bloco contém um array de tamanho $#b#+1$
mas para todo bloco exceto o último, no máxima um quantidade constante de
espaço é desperdiçada nesse array. O resto da memória usada por um
bloco também é constante.
Isso significa que o espaço desperdiçado em uma #SEList# é somente
$O(#b#+#n#/#b#)$.  Ao escolher um valor de #b# dentro de um fator constante
de
$\sqrt{#n#}$, fazemos o custo de espaço extra de uma SEList se aproximar
ao limitante inferior $\sqrt{#n#}$ descrito na \secref{rootishspaceusage}.

\subsection{Procurando Elementos}

O primeiro desafio que enfrentamos com uma #SEList# é procurar o item da lista
com um dado índice #i#. Note que a localização de um elemento consiste de duas
partes:

\begin{enumerate}
  \item o nodo #u# que contém o bloco que contém o elemento com índice #i#; e 
  \item o índice #j# do elemento dentro desse bloco. 
\end{enumerate}

\javaimport{ods/SEList.Location}
\cppimport{ods/SEList.Location}

Para achar o bloco que contém um dado elemento, fazemos do mesmo jeito que
é feito em uma #DLList#
Iniciamos do início da lista e percorremos até o final, ou a partir do final da lista percorremos em direção ao início até encontrarmos o nodo que queremos.
A única diferença é que cada vez que movemos de um nodo ao seguinte, pulamos 
um bloco inteiro de elementos.

\pcodeimport{ods/SEList.get_location(i)}
\javaimport{ods/SEList.getLocation(i)}
\cppimport{ods/SEList.getLocation(i,ell)}

Lembre-se que, com exceção de no máximo um bloco, cada bloco
contém pelo menos
$#b#-1$ elementos, então cada passo na nossa busca nos leva a 
$#b#-1$ elementos mais próximos ao elemento que estamos procurando.
Se estamos procurando do começo ao fim, isso significa que alcançamos o nodo
que queremos após
$O(1+#i#/#b#)$ passos.  
Se procuramos de trás para frente, então alcançamos o nodo que queremos
após
$O(1+(#n#-#i#)/#b#)$ passos. O algoritmo usa a menor dessas duas quantidades
dependendo do valor de #i#, então o tempo de localizar o item com índice #i# é
 $O(1+\min\{#i#,#n#-#i#\}/#b#)$.

Uma vez que sabemos como localizar o item com índice #i#, as operações #get(i)# 
e #set(i,x)# são traduzidas em obter e atribuir em um índice particular no bloco correto:

\codeimport{ods/SEList.get(i).set(i,x)}

Os tempos de execução dessas operações são dominados pelo tempo que 
leva para achar o item, então eles também rodam em tempo $O(1+\min\{#i#,#n#-#i#\}/#b#)$.

\subsection{Adicionando um Elemento}

Adicionar elementos a uma 
 #SEList# é um pouco mais complicado.
 Antes de considerar o caso geral, consideraremos a operação mais fácil, #add(x)#,
 na qual #x# é adicionada ao final da lista. Se o último bloco está cheio
 (ou não existe porque não há blocos ainda), então primeiro alocamos um
 novo bloco e o acrescentamos na lista de blocos. 
 Agora que temos certeza de que o último bloco existe e não está cheio,
 acrescentamos #x# ao último bloco.

\cppimport{ods/SEList.add(x)}
\javaimport{ods/SEList.add(x)}
\pcodeimport{ods/SEList.append(x)}

A situação complica quando adicionamos ao interior da lista usando #add(i,x)#
Primeiro alocamos #i# para obter o nodo #u# cujo bloco contém o #i#-ésimo item da lista.
O problema é que queremos inserir #x# no bloco de #u#, mas temos que estar
preparados para o caso em que o bloco de #u# já contenha 
$#b#+1$ elementos, ou seja, cheio e sem espaço para #x#.

Considere que
$#u#_0,#u#_1,#u#_2,\ldots$ denotam #u#, #u.next#, #u.next.next# e assim por diante.
Exploramos $#u#_0,#u#_1,#u#_2,\ldots$ procurando por um nodo 
que provê espaço para #x#. Três casos podem ocorrer durante
nossa exploração (ver a \figref{selist-add}):

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{@{}l@{}}
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-a}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-b}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-c}\\
    \end{tabular}
  \end{center}
  \caption[Adição no SEList]{Os três casos que ocorrem durante a adição de um item
 #x# no interior de uma #SEList#.  (Essa #SEList# tem tamanho de bloco $#b#=3$.)}
  \figlabel{selist-add}
\end{figure}


\begin{enumerate}
\item Rapidamente (em $r+1\le #b#$ passos) achamos um nodo $#u#_r$ cujo 
  bloco não está cheio. Nesse caso, realizamos 
 $r$ deslocamentos de um elemento de um bloco ao próximo, de forma 
    que o espaço livre em $#u#_r$ torne-se um espaço livre em $#u#_0$.
    Podemos então inserir #x# no bloco do $#u#_0$.

\item Podemos rapidamente (em $r+1\le #b#$ passos) ir ao fim da lista de blocos.
  Nesse caso, adicionamos um bloco vazio ao fim da lista de bloco e preceder como no primeiro caso. 

\item Após #b# passos não achamos nenhum bloco que não esteja cheio.
  Nesse caso,
$#u#_0,\ldots,#u#_{#b#-1}$ é uma sequência de #b# blocos
onde cada contém 
 $#b#+1$ elementos. Nós inserimos um novo bloco $#u#_{#b#}$
    no fim dessa sequência e \emph{espalhamos} os 
 $#b#(#b#+1)$ elementos originais para que cada bloco de 
 $#u#_0,\ldots,#u#_{#b#}$ contenha exatamente 
#b# elementos. Agora o bloco de $#u#_0$ contém somente #b# elementos, então ele tem espaço para inserir #x#.
\end{enumerate}

\codeimport{ods/SEList.add(i,x)}

O tempo de execução da operação  #add(i,x)# depende de 
quais dos três casos anteriores ocorrem. 
 Casos~1 e 2 envolvem examinar e deslocar elementos ao longo 
 de até #b# blocos e levam $O(#b#)$ de tempo.
Caso~3 envolve chamar o método #spread(u)#, que move $#b#(#b#+1)$
elementos e leva $O(#b#^2)$ de tempo. Se ignorarmos o custo do Caso~3
(que iremos levar em consideração depois, com amortização) isso significa
que o tempo total de execução para localizar #i# e realizar a inserção de #x#
é $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$.

\subsection{Remoção de um Elemento}

Remover um elemento de uma 
 #SEList# é similar a adicionar um elemento. 
 Nós primeiro localizamos o nodo #u# que contém o elemento com índice #i#.
Agora, temos que estar preparados para o caso no qual não podemos remover
um elemento de #u# sem fazer que o bloco de #u# se torne menor que $#b#-1$.

Novamente, sejam
 $#u#_0,#u#_1,#u#_2,\ldots$ os ponteiros #u#, #u.next#, #u.next.next# e assim por diante.
Nós examinamos $#u#_0,#u#_1,#u#_2,\ldots$ para procurar por um nodo 
do qual podemos emprestar um elemento para fazer o tamanho do bloco
de $#u#_0$ pelo menos $#b#-1$.  Há outros casos a considerar 
(veja a \figref{selist-remove}):

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics[scale=0.90909]{figs/selist-remove-a}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-b}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-c}\\
    \end{tabular}
  \end{center}
\caption[Remoção na SEList]{Os três casos que ocorrem durante a remoção de um item #x# no interior de uma #SEList#. (Essa #SEList# tem bloco de tamanho $#b#=3$.)}
  \figlabel{selist-remove}
\end{figure}

\begin{enumerate}
\item Rapidamente (em $r+1\le #b#$ passos) achamos um nodo cujo bloco contém
  mais de $#b#-1$ elementos. Nesse caso, realizamos $r$ deslocamentos de um bloco no anterior, de forma tal que o elemento extra em 
$#u#_r$ se torna um elemento extra em $#u#_0$. Podemos então remover o elemento apropriado do bloco de $#u#_0$.

\item Podemos rapidamente (em $r+1\le #b#$ passos) pular ao fim da lista de blocos.
Nesse caso, $#u#_r$ é o último bloco e não há necessidade para o bloco de 
$#u#_r$ conter pelo menos $#b#-1$ elementos. Portanto, da mesma maneira que
    acima, emprestamos um elemento 
de $#u#_r$ para fazer um elemento extra em 
$#u#_0$.  Se isso causar o bloco de $#u#_r$ se tornar vazio, então o removemos.

\item Após #b# passos, não achamos nenhum bloco contendo mais de 
$#b#-1$ elementos.  Nesse caso, $#u#_0,\ldots,#u#_{#b#-1}$ é uma sequência de 
#b# blocos que contém $#b#-1$ elementos. Nós \emph{acumulamos}
esses $#b#(#b#-1)$ elementos em $#u#_0,\ldots,#u#_{#b#-2}$ de forma tal que cada
um desses 
$#b#-1$ blocos contém exatamente #b# elementos e removemos 
$#u#_{#b#-1}$, que agora está vazio. Agora o bloco de $#u#_0$ contém #b#
elementos e então podemos remover o elemento apropriado dele.
\end{enumerate}

\codeimport{ods/SEList.remove(i)}

Assim como a operação 
 #add(i,x)#, o tempo de execução da operação #remove(i)#
é $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$ se ignorarmos o custo do método
#gather(u)# que ocorre no Caso~3.

\subsection{Análise amortizada do Espalhamento e Acumulação}

A seguir, avaliaremos o custo dos métodos
 #gather(u)# e #spread(u)# que podem ser executados pelos métodos
#add(i,x)# e #remove(i)#.  Por questão de completude, eles são mostrados a seguir:

\codeimport{ods/SEList.spread(u)}
\codeimport{ods/SEList.gather(u)}

O tempo de execução de cada um desses métodos é dominado pelos dois laços aninhados.
Ambos laços interno e externo executam no máximo 
$#b#+1$ vezes, então o tempo total de execução de cada um desses métodos é
$O((#b#+1)^2)=O(#b#^2)$. Porém, o lema a seguir mostra que esses métodos executam em, no máximo, uma a cada #b# chamadas de #add(i,x)# ou #remove(i)#.

\begin{lem}\lemlabel{selist-amortized}
  Se uma #SEList# é criada e qualquer sequência de 
   $m\ge 1$ chamadas a 
  #add(i,x)# e #remove(i)# é realizada então o tempo total gasto durante
  todas as chamadas a 
  #spread()# e #gather()# é $O(#b#m)$.
\end{lem}

\begin{proof}
  Usaremos o método do potencial da análise amortizada.
  \index{potential method}%
  \index{método do potencial}%
  Dizemos que um nodo #u# é 
   \emph{frágil} se o bloco de #u# não contém #b#
  elementos (então #u# pode ser o último nodo ou contém $#b#-1$
  ou $#b#+1$ elementos).  Qualquer nodo cujo bloco contém #b# elementos é 
  \emph{durável}. Defina o \emph{potencial} de uma #SEList# como o número
  de nodos frágeis que possui. 
  Iremos considerar somente a operação #add(i,x)#
  e sua relação com o número de chamadas a #spread(u)#.
  As análises de #remove(i)# e #gather(u)# são idênticas.

   Note que, se o Caso~1 ocorrer durante o método #add(i,x)#, então somente um nodo 
  $#u#_r$ tem o tamanho de seu bloco alterado. Portanto, no máximo um nodo,
  $#u#_r$, deixa de ser durável e torna-se frágil. Se o Caso~2 ocorrer, 
  então um novo nodo é criado, e esse nó é frágil,
  mas nenhum outro nodo muda de tamanho, então o número de nodos frágeis aumenta em um. Então, nos Casos~1 ou 2 o potencial da SEList aumenta em até uma unidade.

  Finalmente, se Caso~3 ocorrer, é porque
   $#u#_0,\ldots,#u#_{#b#-1}$ são todos nodos frágeis.
  Então $#spread(#u_0#)#$ é chamado e esses #b# nodos frágeis são
  substituídos por
  $#b#+1$ nodos duráveis.  Finalmente, #x# é adicionado ao bloco de 
  $#u#_0$ tornando $#u#_0$ frágil. No total, o potencial reduz em 
  $#b#-1$.

  Em resumo, o potencial inicia em 0 (não há nodos na lista).
  Cada vez que um Caso~1 ou um Caso~2 ocorre o potencial aumenta em até 1 unidade.
  Cada vez que um Caso~3 ocorre, o potencial decresce em 
  $#b#-1$. O potencial (que conta o número de nodos frágeis) nunca é menor que 0.
  Concluímos que, para toda ocorrência do Caso~3, houveram pelo menos
  $#b#-1$ ocorrências de um Caso~1 ou um Caso~2.  Então, para toda chamada a 
  #spread(u)#, existem pelo menos #b# chamadas a #add(i,x)#.  Isso conclui a prova.
\end{proof}

\subsection{Resumo}

O seguinte teorema resume o desempenho da estrutura de dados #SEList#: 

\begin{thm}\thmlabel{selist}
Uma #SEList# implementa a interface #List#. Ignorando o custo de chamadas
a #spread(u)# e #gather(u)#, uma #SEList# tamanho de bloco #b#
  possui as operações 
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1+\min\{#i#,#n#-#i#\}/#b#)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$ por operação.
  \end{itemize}
  Além disso, iniciando com uma #SEList# vazia, uma sequência de 
   $m$ operações 
  #add(i,x)# e #remove(i)# resulta em um total de $O(#b#m)$ de tempo gasto
  durante todas as chamadas a #spread(u)# e #gather(u)#.

  O espaço (medido em palavras)
  \footnote{Consulte a \secref{model} para uma discussão de como a memória é medida.
  } usada por uma #SEList# que guarda #n# elementos é 
  $#n# +O(#b# + #n#/#b#)$.
\end{thm}

A #SEList# foi projetada visando um meio termo de custos entre #ArrayList# e uma #DLList#, onde a mistura de custos entre essas duas estruturas depende do tamanho do bloco #b#.
Em um extremo
 $#b#=2$, cada nodo da #SEList# guarda no máximo três valores,
 o que não é muito diferente em uma #DLList#. No outro extremo,
$#b#>#n#$, todos os elementos são guardados em um único array, assim como em uma 
#ArrayList#.  Entre esses dois extremos está um balanceamento entre o tempo que leva para adicionar ou remover um item da lista e o tempo que leva para localizar um dado item na lista.

\section{Discussão e Exercícios}

Ambas as listas simplesmente e duplamente encadeadas são técnicas bem estabelecidas e têm sido usadas em programas por mais de 40 anos. Elas são discutidas, por exemplo, por Knuth \cite[Sections~2.2.3--2.2.5]{k97v1}.  Mesmo a estrutura de dados 
#SEList# parece ser uma variante de estrutura de dados bem conhecida. 
A 
#SEList# é às vezes conhecida pelo nome de \emph{unrolled linked list}
\cite{sra94}.
\index{unrolled linked list|seealso{#SEList#}}%
\index{linked list!unrolled|seealso{#SEList#}}%

Outra forma de economizar espaço em uma lista duplamente encadeada é usar uma XOR-list.
\index{XOR-list}%
Em uma XOR-list, cada nodo, #u# contém somente um ponteiro, chamado 
#u.nextprev#, que guarda o resultado do ou-exclusivo bit-a-bit entre #u.prev#
e #u.next#.  A lista em si precisa guardar dois ponteiros,
um para o nodo #dummy# e um para #dummy.next#
(o primeiro nodo, ou #dummy# se a lista está vazia).
Essa técnica usa o fato que, se temos os ponteiros para #u# e #u.prev#,
então podemos extrair #u.next# usando a fórmula
\[
   #u.next# = #u.prev# \verb+^+ #u.nextprev# \enspace .
\]
(Aqui \verb+^+ computa o ou-exclusivo bit-a-bit de seus dois argumentos.)
Essa técnica complica o código um pouco e não é possível em algumas linguagem como Java e Python, que têm coletores de lixo, porém fornece uma implementação de uma lista duplamente encadeada que requer somente um ponteiro por nodo.
Veja o artigo de Sinha \cite{s04} para uma discussão detalhada dessa XOR-list.

\begin{exc}
  Porque não é possível usar um nodo dummy em uma #SLList# para evitar 
  todos os casos especiais que ocorrem nas operações 
#push(x)#, #pop()#, #add(x)# e #remove()#?
\end{exc}

\begin{exc}
  Projete e implemente um novo método para a 
   #SLList# chamado #secondLast()# que retorna 
   o penúltimo elemento de uma #SLList#.
   Faça isso sem usar a variável membro, #n#, que registra o tamanho da lista. 
\end{exc}

\begin{exc}
  Implemente as operações de uma #List#
   #get(i)#, #set(i,x)#,
  #add(i,x)# e #remove(i)# em uma #SLList#.  Cada uma dessas operações devem rodar 
   tempo $O(1+#i#)$.
\end{exc}

\begin{exc}
  Projete e implemente um método para a
   #SLList# chamado #reverse()# que inverte a ordem de elementos em uma 
    #SLList#.  Esse método deve rodar em tempo $O(#n#)$, não deve usar recursão, não deve usar nenhuma estrutura de dados secundária e
    não deve criar nenhum nodo novo. 
\end{exc}

\begin{exc}
  Projete e implemente o método #checkSize()# para as estruturas 
  #SLList# e #DLList#.
  Esse método percorre a lista e conta o número de nodos para verificar se
  ele é igual ao valor #n# guardado na lista. Esse método
  não retorna nada, mas lança uma exceção se o tamanho que ele computa 
  não for idêntico ao valor de #n#.
\end{exc}

\begin{exc}
  Tente recriar o código para a operação 
  #addBefore(w)# que cria um nodo 
  , #u#, o adiciona em uma  #DLList# logo antes do nodo #w#.
  Não use o código deste capítulo. Mesmo se o seu código não seja exatamente igual ao código deste livro ele ainda pode estar correto.  Teste e veja se funciona.
\end{exc}

Os próximos exercícios envolvem a realização de manipulações em 
#DLList#s.
Você deve realizá-los sem alocar novos nodos ou arrays temporários.
Todos eles podem ser feitos apenas com trocas de valores
de #prev# e #next# dos nodos existentes.

\begin{exc}
Escreva um método #isPalindrome()# para a #DLList# que retorna #true# se a lista for um 
   \emph{palíndromo},
  \index{palíndromo}%
  isto é, o elemento na posição #i# é igual ao elemento na posição 
  $#n#-i-1$ para todo $i\in\{0,\ldots,#n#-1\}$. O seu código deve rodar em 
 $O(#n#)$ de tempo.
\end{exc}

\begin{exc}
  Implemente um método #rotate(r)# que ``rotaciona'' uma #DLList# tal que o item #i# da lista se torna o item 
$(#i#+#r#)\bmod #n#$.  Esse método deve rodar em 
   $O(1+\min\{#r#,#n#-#r#\})$ de tempo não deve modificar quaisquer nodos na lista.
\end{exc}


\begin{exc}\exclabel{linkedlist-truncate}
Escreva um método, #truncate(i)#, que trunca uma #DLList# na posição #i#.
Após executar esse método, o tamanho da lista será #i# e deve conter somente os elementos nos índices $0,\ldots,#i#-1$.  
  Esse método retorna outra 
  #DLList# que contém os elementos nos índices
  $#i#,\ldots,#n#-1$.  Esse método deve rodar em tempo $O(\min\{#i#,#n#-#i#\})$.
\end{exc}

\begin{exc}
  Escreva um método para a 
  #DLList# chamado #absorb(l2)#, que recebe como argumento
  uma #DLList#, #l2#, a esvazia e coloca seu conteúdo, em ordem, na lista receptora.
  Por exemplo, se #l1# contém $a,b,c$ e #l2#
  contém
   $d,e,f$, então após chamar #l1.absorb(l2)#, #l1# terá 
  $a,b,c,d,e,f$ e #l2# estará vazia. 
\end{exc}

\begin{exc}
  Escreva um método
   #deal()# que remove todos os elementos com índices ímpares de uma
   #DLList# e retorna uma #DLList# contendo esses elementos.
   Por exemplo, se 
   #l1# contém os elementos $a,b,c,d,e,f$, então depois de chamar
  #l1.deal()#, #l1# deve conter $a,c,e$ e uma lista contendo
  $b,d,f$ deve ser retornada.
\end{exc}

\begin{exc}
  Escreva um método 
  #reverse()# que inverte a ordem de elementos em uma
  #DLList#.  
\end{exc}

\begin{exc}\exclabel{dllist-sort}
  \index{merge-sort}%
  Este exercício te guia para realizar uma implementação do algoritmo
  merge-sort para a ordenação de uma
   #DLList#, conforme discutido na \secref{merge-sort}.
  \javaonly{Na sua implementação, realize comparações entre elementos usando o método 
   #compareTo(x)# de forma que a implementação resultante possa ordenar
  qualquer #DLList# contendo elementos que implementam a interface #Comparable#.}
  \begin{enumerate}
    \item Escreva um método para a #DLList# chamado #takeFirst(l2)#.
      Esse método recebe o primeiro nodo de #l2# e o combina à lista que efetua
      o método. Isso é equivalente a 
        #add(size(),l2.remove(0))#,
       exceto que não deve criar um nodo novo.
    \item Escreva um método estático para a #DLList# chamado #merge(l1,l2)#, que recebe sua listas ordenadas 
      #l1# e #l2#, faz a junção (em inglês, merge) delas, e retorna uma nova lista ordenada contendo o resultado.
        Essa junção faz com que 
        #l1# e #l2# sejam esvaziadas no processo. 
        Por exemplo, se #l1# contém
       $a,c,d$ e #l2# contém
       $b,e,f$, então esse método retorna uma nova lista contendo $a,b,c,d,e,f$.
    \item Escreva um método para a #DLList# que ordena os elementos contidos na lista usando o algoritmo merge sort.
      Esse algoritmo recursivo funciona da seguinte maneira:
       \begin{enumerate}
          \item Se a lista contém 0 ou 1 elemento então não há nada a fazer. Caso contrário, 
          \item Usando o método #truncate(size()/2)#, divida a lista em duas listas de tamanhos aproximadamente iguais 
#l1# e #l2#;
          \item Recursivamente ordene #l1#;
          \item Recursivamente ordene #l2#; e finalmente
          \item Faça a junção (em inglês, merge) da #l1# com #l2# em uma única lista ordenada.
       \end{enumerate}
  \end{enumerate}
\end{exc}

Os próximos exercícios são mais avançados e requerem a compreensão 
aprofundada do que acontece ao valor mínimo armazenado em uma 
#Stack# ou #Queue# quando itens são adicionados ou removidos.

\begin{exc}
  \index{MinStack@#MinStack#}%
  Projete e implemente uma 
  estrutura de dados #MinStack# que pode guardar elementos comparáveis 
  e possui as operações de stack 
#push(x)#, #pop()# e #size()#, assim como a operação #min()#, que retorna 
  o valor mínimo atualmente guardado na estrutura de dados. 
  Todas as operações devem rodar em tempo constante.
\end{exc}

\begin{exc}
  \index{MinQueue@#MinQueue#}%
  Projete e implemente uma estrutura de dados 
  #MinQueue# que pode guardar elementos comparáveis
  e possui as operações de queue
  #add(x)#,
  #remove()# e #size()#, assim como a operação #min()#, que 
  retorna o valor mínimo atualmente armazenado na estrutura de dados. 
  Todas as operações devem rodar em tempo constante amortizado.
\end{exc}

\begin{exc}
  \index{MinDeque@#MinDeque#}%
  Projete e implemente uma estrutura de dados 
  #MinDeque# que pode guardar elementos comparáveis e possui todas as operações
  da deque 
  #addFirst(x)#,
  #addLast(x)#, #removeFirst()#, #removeLast()# e #size()#, e a operação 
  #min()#, que retorna o valor mínimo atualmente guardado na estrutura de dados.
  Todas as operações devem rodas em tempo amortizado constante. 
\end{exc}

Os exercícios a seguir são planejados para testar a compreensão do leitor
a respeito da implementação e análise da lista eficiente em uso do espaço #SEList#:

\begin{exc}
Prove que, se uma #SEList# é usada como uma #Stack# (
  adicionado à #SEList# as operações de inserção 
   $#push(x)#\equiv#add(size(),x)#$ e remoção $#pop()#\equiv #remove(size()-1)#$), então elas
  operações rodam em tempo constante amortizado, independentemente 
  do valor de #b#.
\end{exc}

\begin{exc}
  Projete e implemente uma versão de uma #SEList# que aceita todas as operações 
  de uma #Deque# em tempo amortizado constante por operação, independentemente
  do valor de #b#.
\end{exc}

\begin{exc}
  Explique como usar o operador ou-exclusivo bit-a-bit, 
   \verb+^+ para trocar os valores de duas variáveis 
   #int# sem usar uma terceira variável. 
\end{exc}

