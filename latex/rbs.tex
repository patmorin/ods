\chapter{Árvores Binárias de Busca Aleatórias}
\chaplabel{rbs}

Neste capítulo, apresentamos uma estrutura de árvore binária de busca que
usa randomização para obter 
$O(\log #n#)$ de tempo esperado para todas as operações. 

\section{Árvores Binárias de Busca Aleatórias}
\seclabel{rbst}

Considere as duas árvores binárias de busca mostradas na \figref{rbs-lvc}, cada uma com 
$#n#=15$ nodos. A árvore na esquerda é uma lista e a outra é uma árvore binária de busca perfeitamente balanceada. A altura da árvore na esquerda é 
$#n#-1=14$ e da direita é três.

\begin{figure}
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[scale=0.90909,scale=0.95]{figs/bst-path} &
      \includegraphics[scale=0.90909,scale=0.95]{figs/bst-balanced}
    \end{tabular}
  \end{center}
  \caption{Duas árvores binárias de busca contendo inteiros $0,\ldots,14$.}
  \figlabel{rbs-lvc}
\end{figure}

Imagine como essas duas árvores podem ter sido construídas. 
Aquela na esquerda ocorre se iniciamos com uma #BinarySearchTree# 
vazia e adicionando a sequência 
\[
    \langle 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14 \rangle \enspace .
\]
Nenhuma outra sequência de adições irá criar essa árvore (como você pode provar
por indução em #n#). Por outro lado, a árvore na direita pode ser criada pela sequência
\[
    \langle 7,3,11,1,5,9,13,0,2,4,6,8,10,12,14 \rangle  \enspace .
\]
Outras sequências também funcionariam, incluindo
\[
    \langle 7,3,1,5,0,2,4,6,11,9,13,8,10,12,14 \rangle  \enspace ,
\]
e
\[
    \langle 7,3,1,11,5,0,2,4,6,9,13,8,10,12,14 \rangle \enspace .
\]
De fato, há 
 $21,964,800$ sequências que gerariam a árvore na direita e somente uma gera a árvore na esquerda. 

 O exemplo acima passa uma ideia de que, se escolhermos uma permutação
 aleatória de $0,\ldots,14$, e a adicionarmos em uma árvore binária de 
 busca, então teremos uma chance maior de obter uma árvore muito 
 balanceada (o lado direito da \figref{rbs-lvc}) que temos de obter uma 
 árvore muito desbalanceada (o lado esquerdo da \figref{rbs-lvc}).

Podemos formalizar essa noção ao estudar árvores binárias de busca aleatórias.
Uma \emph{árvore binária de busca aleatória}
\index{árvore binária de busca aleatória}%
\index{árvore binária de busca!aleatória}%
de tamanho #n# é obtida da seguinte forma: pegue uma permutação aleatória,
 $#x#_0,\ldots,#x#_{#n#-1}$,
de inteiros $0,\ldots,#n#-1$ e adicione seus elementos, um por um em uma 
#BinarySearchTree#.  Queremos dizer com \emph{permutação aleatória}
\index{permutação!aleatória}%
\index{permutação aleatória}%
que cada uma das possíveis $#n#!$ permutações (reordenações) de $0,\ldots,#n#-1$
é igualmente provável, tal que a probabilidade de obter uma permutação em particular é 
$1/#n#!$.

Note que os valores $0,\ldots,#n#-1$ poderiam ser substituídos por qualquer conjunto ordenado de #n# elementos sem alterar a propriedade das árvores binárias de busca aleatória. 
O elemento $#x#\in\{0,\ldots,#n#-1\}$ está simplesmente 
representando o elemento de ranking (posição) #x# em um conjunto 
ordenado de tamanho #n#.

Antes de apresentarmos o nosso principal resultado sobre árvores binárias de busca aleatórias, precisamos fazer uma curta discussão sobre um tipo de número que 
aparece frequentemente ao estudar estruturas randomizadas. Para um inteiro não-negativo $k$, o $k$-ésimo \emph{número harmônico},
\index{número harmônico}%
\index{H@$H_k$ (número harmônico)}%
denotado por $H_k$, é definido como 
\[
  H_k = 1 + 1/2 + 1/3 + \cdots + 1/k \enspace .
\] 
O número harmônico 
 $H_k$ na tem forma fechada simples, mas é muito relacionado ao
 logaritmo natural de $k$. Em particular, 
\[
  \ln k < H_k \le \ln k + 1  \enspace .
\]
\newcommand{\hint}{\int_1^k\! (1/x)\, \mathrm{d}x}%
Leitores que estudaram cálculo podem perceber que isso é verdadeiro
pois a integral
$\hint = \ln k$.  Tendo em mente que uma integral pode ser interpretada como a área entre uma curva e o eixo $x$, o valor de 
$H_k$ pode ser limitado inferiormente pela integral $\hint$ e limitado superiormente por 
$1+ \hint$.  (Veja a \figref{harmonic-integral} para uma explicação gráfica.)

\begin{figure}
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width=\HalfScaleIfNeeded]{figs/harmonic-2} 
        & \includegraphics[width=\HalfScaleIfNeeded]{figs/harmonic-3}
    \end{tabular}
  \end{center}
  \caption{O $k$-ésimo número harmônico $H_k=\sum_{i=1}^k 1/i$ é limitado superiormente e inferiormente por duas integrais. O valor dessas integrais é dado pela área da região sombreada, enquanto o valor de 
  $H_k$ é dado pela área dos retângulos.} 
  \figlabel{harmonic-integral}
\end{figure}


\begin{lem}\lemlabel{rbs}
  Em uma árvore binária de busca aleatória de tamanho #n#, as seguintes afirmações valem:
  \begin{enumerate}
    \item Para qualquer $#x#\in\{0,\ldots,#n#-1\}$, o comprimento esperado do caminho de busca por 
    #x# é $H_{#x#+1} + H_{#n#-#x#} - O(1)$.\footnote{As expressões
    $#x#+1$ e $#n#-#x#$ podem ser interpretadas respectivamente 
    como o número de elementos na árvore menor que ou igual a #x#
      e o número de elementos na árvore maiores que ou iguais a #x#.}
    \item Para qualquer $#x#\in(-1,n)\setminus\{0,\ldots,#n#-1\}$, o
      comprimento esperado do caminho de busca para #x# é 
    $H_{\lceil#x#\rceil}
    + H_{#n#-\lceil#x#\rceil}$.
  \end{enumerate}
\end{lem}

Provaremos o \lemref{rbs} na seção a seguir. Por enquanto, considere 
o que as duas partes do  \lemref{rbs} nos diz. A primeira parte nos 
diz que se buscarmos por um elemento em uma árvore de tamanho #n#, 
então o comprimento esperado do caminho de busca é no máximo $2\ln n + O(1)$.  

A segunda parte no diz sobre a busca de um valor não guardado na árvore.
Ao compararmos duas partes do lema, vemos que buscar algo que está na 
árvore é apenas um pouco mais rápido do que algo que não está.

\subsection{Prova do \lemref{rbs}}

A observação chave necessária para provar o
\lemref{rbs} é a seguinte: 
o caminho de busca por um valor #x# no intervalo aberto
 $(-1,#n#)$ em uma árvore binária de busca aleatória
$T$ contém o nodo com chave $i < #x#$
se, e somente se, na permutação aleatória usada para criar
 $T$, $i$
aparece antes de qualquer $\{i+1,i+2,\ldots,\lfloor#x#\rfloor\}$.

Para ver isso, 
observe a \figref{rbst-records} e note que até que algum valor em
$\{i,i+1,\ldots,\lfloor#x#\rfloor\}$ seja adicionado, os caminhos de busca 
para cada valor no intervalo aberto 
$(i-1,\lfloor#x#\rfloor+1)$
são idênticos. (Lembre-se que para dois valores terem caminhos distintos,    
precisa haver algum elemento na árvore que compara de modo diferente entre eles.)
Seja $j$ o primeiro elemento em 
$\{i,i+1,\ldots,\lfloor#x#\rfloor\}$ para aparecer na permutação aleatória.
Note que $j$ está e sempre estará no caminho de busca por #x#.
Se $j\neq i$ então o nodo $#u#_j$ contendo $j$ é criado antes do nodo 
$#u#_i$ que contém $i$.  Depois, quando $i$ for adicionado, ele será adicionado na subárvore enraizada em  
$#u#_j#.left#$, pois $i<j$.  Por outro lado, o caminho de busca para #x# nunca
visitará essa subárvore porque irá proceder para
$#u#_j#.right#$ após visitar $#u#_j$.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rbst-records}
  \end{center}
  \caption[O caminho de busca em uma árvore binária de busca aleatória]{O valor $i<#x#$ está no caminho de árvore para #x# se e somente se 
   $i$ for o primeiro elemento entre $\{i,i+1,\ldots,\lfloor#x#\rfloor\}$ adicionado à árvore.}
  \figlabel{rbst-records}
\end{figure}

De modo similar, para 
$i>#x#$, $i$ aparece no caminho de busca para #x#
se e somente se
 $i$ aparecer antes de $\{\lceil#x#\rceil,
\lceil#x#\rceil+1,\ldots,i-1\}$ na permutação aleatória usada para criar $T$.

Note que, se iniciarmos com uma permutação aleatória de 
 $\{0,\ldots,#n#\}$,
 então as subsequências contendo somente
 $\{i,i+1,\ldots,\lfloor#x#\rfloor\}$
e $\{\lceil#x#\rceil, \lceil#x#\rceil+1,\ldots,i-1\}$ também são 
permutações aleatórias de seus respectivos elementos.
Cada elemento, então, nos subconjuntos 
$\{i,i+1,\ldots,\lfloor#x#\rfloor\}$ e $\{\lceil#x#\rceil,
\lceil#x#\rceil+1,\ldots,i-1\}$ é igualmente provável de aparecer antes
de que qualquer outro no seu subconjunto na permutação aleatória 
usada para criar $T$.
Então temos
\[
  \Pr\{\mbox{$i$ está no caminho de busca por #x#}\}
  = \left\{ \begin{array}{ll}
     1/(\lfloor#x#\rfloor-i+1) & \mbox{se $i < #x#$} \\
     1/(i-\lceil#x#\rceil+1) & \mbox{se $i > #x#$} 
     \end{array}\right . \enspace .
\]

A partir dessa observação, a prova do \lemref{rbs}
envolve alguns cálculos simples com números harmônicos:

\begin{proof}[Prova do \lemref{rbs}]
Seja
$I_i$ seja uma variável indicadora aleatória que é igual a um quando $i$
aparecer no caminho de busca por #x# e zero caso contrário. Então, o comprimento
do caminho de busca é dado por 
\[
  \sum_{i\in\{0,\ldots,#n#-1\}\setminus\{#x#\}} I_i
\]
  então, se $#x#\in\{0,\ldots,#n#-1\}$, o comprimento esperado do caminho
  de busca é dado por (veja a \figref{rbst-probs}.a)
\begin{align*}
  \E\left[\sum_{i=0}^{#x#-1} I_i + \sum_{i=#x#+1}^{#n#-1} I_i\right]
   & =  \sum_{i=0}^{#x#-1} \E\left[I_i\right]
         + \sum_{i=#x#+1}^{#n#-1} \E\left[I_i\right] \\
   & = \sum_{i=0}^{#x#-1} 1/(\lfloor#x#\rfloor-i+1)
         + \sum_{i=#x#+1}^{#n#-1} 1/(i-\lceil#x#\rceil+1) \\
   & = \sum_{i=0}^{#x#-1} 1/(#x#-i+1)
         + \sum_{i=#x#+1}^{#n#-1} 1/(i-#x#+1) \\
   & = \frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{#x#+1} \\
   & \quad {} + \frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{#n#-#x#} \\
   & = H_{#x#+1} + H_{#n#-#x#} - 2  \enspace .
\end{align*}
Os cálculos correspondentes para um valor buscado 
$#x#\in(-1,n)\setminus\{0,\ldots,#n#-1\}$ são quase idênticos (veja a 
\figref{rbst-probs}.b).
\end{proof}

\begin{figure}
  \begin{center}
    \begin{tabular}{@{}c@{}}
      \includegraphics[width=\ScaleIfNeeded]{figs/rbst-probs-a} \\ (a) \\[2ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/rbst-probs-b} \\ (b) \\[2ex]
    \end{tabular}
  \end{center}
  \caption[As probabilidades de um elemento estar em um caminho de busca]{As probabilidades de um elemento estar no caminho de busca por #x# quando 
   (a)~#x# é um inteiro e (b)~quando #x# não é um inteiro. }
  \figlabel{rbst-probs}
\end{figure}

\subsection{Resumo}

O teorema a seguir resume o desempenho de uma árvore binária de busca aleatória:

\begin{thm}\thmlabel{rbs}
  Uma árvore binária de busca aleatória pode ser construída em 
$O(#n#\log #n#)$ de tempo.
Em uma árvore binária de busca aleatória, a operação 
#find(x)# leva $O(\log #n#)$ de tempo esperado.
\end{thm}

Devemos enfatizar novamente que o valor esperado no \thmref{rbs} é em respeito à permutação aleatória usada 
para criar a árvore binária de busca aleatória.
Em especial, não depende de uma escolha aleatória de #x#; isso é verdade para
todo valor de #x#.

\section{#Treap#: Uma Árvore Binária de Busca Aleatória}
\seclabel{treap}

\index{Treap@#Treap#}%
O problema com uma árvore binária de busca aleatória é, claramente, que
ela não é dinâmica. Ela não aceita as operações
#add(x)# ou #remove(x)# necessárias para implementar a interface #SSet#.
Nesta seção descrevemos uma estrutura de dados chamada de #Treap# que usa
o \lemref{rbs} para implementar 
a interface #SSet#.\footnote{O nome #Treap# vem do fato que essa estrutura 
de dados é simultâneamente uma árvore binária de busca 
(do inglês, binary search \textbf{tr}ee) (\secref{binarysearchtree}) e uma 
 h\textbf{eap} (\chapref{heaps}).}

Um nodo em uma #Treap# é como um nodo em uma #BinarySearchTree# em que ele tem um valor de dado #x# mas também contém um valor único chamado \emph{priority} #p#
#p#, que é atribuído aleatoriamente: 
\javaimport{ods/Treap.Node<T>}
\cppimport{ods/Treap.TreapNode}
Além de ser uma árvore binária de busca, os nodos em uma #Treap# também obedecem a \emph{propriedade das heaps}:
\begin{itemize}
\item (Propriedade das Heaps)  Em todo nodo #u#, exceto na raiz, 
      $#u.parent.p# < #u.p#$.
      \index{propriedade raiz}%
\end{itemize}
Em outras palavras, cada nodo tem uma prioridade menor que aquelas de seus filhos.
Um exemplo é mostrado na \figref{treap}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/treap}
  \end{center}
  \caption[Uma Treap]{Um exemplo de uma #Treap# contendo os inteiros $0,\ldots,9$. Cada nodo, #u#, é ilustrado como uma caixa contendo $#u.x#,#u.p#$.}
  \figlabel{treap}
\end{figure}

As condições impostas pela heap e por uma árvore binária de busca juntas asseguram que, uma vez que a chave (#x#) e a prioridade (#p#) para cada nodo estejam definidos, a forma da #Treap# está completamente determinada.

A propriedade das heaps nos diz que o nodo com prioridade mínima tem que ser a raiz #r# da #Treap#. A propriedade das árvores binárias de busca nos diz que todos os nodos com
chaves menores que #r.x# são guardados na subárvores enraizada na #r.left# e todos
os nodos com chaves maiores que #r.x# são guardados na subárvore enraizada em #r.right#.

Um fato importante sobre os valores de prioridades em uma #Treap# é que
ele são únicos e atribuídos aleatoriamente.
Por causa disso, existem duas formas equivalentes que podem pensar sobre uma #Treap#. Conforme definido acima, uma #Treap# obedece as propriedades das árvores binárias de busca e das heaps.
Alternativamente,
podemos usar uma #Treap# como uma
#BinarySearchTree# cujos nodos são adicionados em ordem crescente de prioridade.
Por exemplo, a #Treap#
na \figref{treap} pode ser obtida ao adicionar a sequência $(#x#,#p#)$
\[
  \langle
   (3,1), (1,6), (0,9), (5,11), (4,14), (9,17), (7,22), (6,42), (8,49), (2,99)
  \rangle
\]
em uma #BinarySearchTree#.

Como as prioridades são escolhidas aleatoriamente, isso é equivalente a obter uma permutação aleatória das chaves --- nesse caso a permutação é 
\[
  \langle 3, 1, 0, 5, 9, 4, 7, 6, 8, 2 \rangle
\]
---e inseri-las à uma #BinarySearchTree#.  

Mas isso significa que a forma de uma treap é idêntica àquela de uma árvore binária de busca binária.
Em particular, se substituímos cada chave #x# por seu rank, \footnote{O
rank de um elemento #x# em um conjunto $S$ de elementos é o número de elementos
em $S$ que são menores que #x#.} então o \lemref{rbs} aplica-se.

Reafirmando o
\lemref{rbs} em termos de #Treap#s, temos:
\begin{lem}\lemlabel{rbs-treap}
  Em uma #Treap# que guarda um conjunto $S$ de #n# chaves, as seguintes afirmações valem: 
  \begin{enumerate}
    \item Para qualquer $#x#\in S$, o comprimento esperado do caminho
      de busca por #x# é $H_{r(#x#)+1} + H_{#n#-r(#x#)} - O(1)$.
    \item Para qualquer $#x#\not\in S$,  comprimento esperado do caminho de busca por #x# é $H_{r(#x#)} + H_{#n#-r(#x#)}$.
  \end{enumerate}
  Aqui, $r(#x#)$ denota o rank #x# no conjunto $S\cup\{#x#\}$.
\end{lem}
Novamente, enfatizamos que o valor esperado no \lemref{rbs-treap} é obtido sobre escolhas aleatórias das prioridades para cada nodo. Isso não requer quaisquer premissas sobre a aleatoriedade nas chaves.

O \lemref{rbs-treap} nos diz que #Treap#s podem implementar a operação #find(x)#
eficientemente. Contudo, o benefício real de uma #Treap# é que ela
pode implementar as operações
 #add(x)# e #delete(x)#.  Para fazer isso, ela precisa realizar rotações 
 para manter a propriedade das heaps.
Veja a \figref{rotations}.
Uma \emph{rotação}
\index{rotação}%
em uma árvore binária de busca é uma modificação local que pega um pai #u#
de um nodo #w# e torna #w# o pai de #u#, preservando a propriedade das árvores binárias de busca. Rotações vêm em dois sabores: à \emph{esquerda} e à \emph{direita}
dependendo em se #w# é um filho à direita ou esquerda de #u#, respectivamente.
\index{rotação à esquerda}%
\index{rotação à direita}%

\begin{figure}
  \begin{center}
     \includegraphics[width=\ScaleIfNeeded]{figs/rotation}
  \end{center}
  \caption{Rotações à esquerda e direita em uma árvore binária de busca.}
  \figlabel{rotations}
\end{figure}

O código que implementa isso tem que lidar com essas duas possibilidades
e ser cuidadoso com um caso especial (quando #u# for a raiz), então o
código real é um pouco mais longo que 
 a \figref{rotations} daria a entender:
\codeimport{ods/BinarySearchTree.rotateLeft(u).rotateRight(u)}
\label{page:rotations}
Em termos da estrutura de dados #Treap#, a propriedade mais importante de uma rotação é que a profundidade de #w# é reduzida em 1 enquanto a profundidade de
#w# aumenta em 1.

Usando rotações, podemos implementar a operação
#add(x)# da forma a seguir: 
criamos um nodo #u#, atribuímos #u.x = x# e pegamos um valor aleatório
para #u.p#. Depois adicionamos #u# usando o algoritmo #add(x)# usual para
uma #BinarySearchTree#, então #u# é agora uma folha da #Treap#.
Nesse ponto, nossa #Treap# satisfaz a propriedade das árvores binárias de busca,
mas não necessariamente a propriedade das heaps.

Em particular, pode ser o caso que #u.parent.p > u.p#.
Se esse é o caso, então realizamos uma rotação no nodo 
#w#=#u.parent# tal que #u# se torna o pai de #w#.

Se #u# continua a violar a propriedade das heaps, teremos que repetir isso,
decrescendo a profundidade de #u# em um a cada vez, até que
#u# se torne raiz ou 
$#u.parent.p# < #u.p#$.
\codeimport{ods/Treap.add(x).bubbleUp(u)}
Um exemplo de uma operação
#add(x)# é mostrado na \figref{treap-add}.

\begin{figure}
  \begin{center}
  \includegraphics[width=\ScaleIfNeeded]{figs/treap-insert-a} \\
  \includegraphics[width=\ScaleIfNeeded]{figs/treap-insert-b} \\
  \includegraphics[width=\ScaleIfNeeded]{figs/treap-insert-c} \\
  \end{center}
  \caption[Adicionando a uma Treap]{Adicionando o valor 1.5 na #Treap# na \figref{treap}.}
  \figlabel{treap-add}
\end{figure}

O tempo de execução da operação 
#add(x)# é dado pelo tempo que leva para seguir o caminho de busca para #x# mais o número de rotações realizadas para mover o novo nodo #u# subindo a árvore até sua posição correta na
#Treap#.  
De acordo com o \lemref{rbs-treap}, o comprimento esperado do caminho de busca é no máximo $2\ln #n#+O(1)$.  Além disso, cada rotação reduz a profundidade de #u#.
Isso para se #u# se torna raiz, então o número esperado de rotações não pode exceder o comprimento esperado do caminho de busca. Portanto o tempo esperado de execução da operação #add(x)# em uma #Treap# é $O(\log #n#)$. (o \excref{treap-rotates} pede que se mostre que o número esperado de rotações realizadas durante uma adição é na verdade somente $O(1)$.)

A operação
#remove(x)# em uma #Treap# é o oposto da operação #add(x)#. 
Buscamos pelo nodo #u#, contendo #x#, então realizamos rotações para mover
#u# abaixo na árvore até que se torne uma folha e então removemos
#u# da #Treap#. Note que, para mover #u# árvore abaixo, podemos fazer
uma rotação à esquerda ou direita em #u#, que substitui #u# com #u.right#
ou #u.left#, respectivamente.
A escolha é feita de acordo com a primeira das seguintes regras que for adequada:
\begin{enumerate}
\item Se #u.left# e #u.right# forem ambos #null#, então #u# é uma folha e nenhuma rotação é realizada.
\item Se #u.left# (ou #u.right#) for #null#, então realizamos uma rotação à direita (ou à esquerda, respectivamente) em #u#.
\item Se $#u.left.p# < #u.right.p#$ (ou $#u.left.p# > #u.right.p#)$, então realizamos uma rotação à direita (ou rotação à esquerda, respectivamente) em #u#.
\end{enumerate}
Essas três regras asseguram que #Treap# não se torne desconectada e que a propriedade das heaps é restabelecida uma vez que #u# seja removida. 
\codeimport{ods/Treap.remove(x).trickleDown(u)}
Um exemplo da operação #remove(x)# é mostrado na \figref{treap-remove}.
\begin{figure}
  \begin{center}
  \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/treap-delete-a} \\
  \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/treap-delete-b} \\
  \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/treap-delete-c} \\
  \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/treap-delete-d} 
  \end{center}
  \caption[Removendo de uma treap]{Removendo o valor 9 da #Treap# na \figref{treap}.}
  \figlabel{treap-remove}
\end{figure}

O truque para analisar o tempo de execução da operação 
#remove(x)# é notar que essa operação inverte a operação #add(x)#. 
Em particular, se fossemos reinserir #x# usando a mesmo prioridade #u.p#, 
então a operação #add(x)# faria exatamente o mesmo número de rotações
e reestabeleceria a #Treap# para exatamente o mesmo estado que estava antes
que a operação #remove(x)# ocorreu.

(Lendo de cima para baixo, a \figref{treap-remove} ilustra a adição do valor $9$ em uma #Treap#.)
Isso significa que o tempo de execução esperado de #remove(x)# em uma #Treap# de tamanho #n# é proporcional ao tempo esperado de execução da operação #add(x)# em uma #Treap# de tamanho $#n#-1$.  Concluímos que o tempo esperado de execução de #remove(x)# é $O(\log #n#)$.

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados #Treap#:

\begin{thm}
Uma #Treap# implementa a interface #SSet#. Uma #Treap# executa 
as operações #add(x)#, #remove(x)# e #find(x)# em tempo esperado $O(\log #n#)$
por operação.
\end{thm}

Vale a pena comparar a estrutura de dados 
#Treap# à estrutura de dados #SkiplistSSet#.  Ambas implementam 
operações #SSet# em $O(\log #n#)$ de tempo esperado por operação. 
Na duas estruturas de dados, #add(x)# e #remove(x)# envolvem uma busca 
e então um número constante de mudanças de ponteiros
(veja a \excref{treap-rotates} a seguir). Então, para essas estruturas,
o comprimento esperado do caminho de busca é um valor crítico na avaliação de seus desempenhos.
Em uma #SkiplistSSet#, o comprimento esperado de um caminho de busca é 
\[
     2\log #n# + O(1) \enspace ,
\]
Em uma #Treap#, o comprimento esperado de um caminho de busca é
\[
    2\ln #n# +O(1) \approx 1.386\log #n#  + O(1) \enspace .
\]
Então, os caminhos de busca em um 
#Treap# são consideravelmente mais curtos e isso traduz em operações 
mais rápidas em #Treap#s que #Skiplist#s. 
O \excref{skiplist-opt} no \chapref{skiplists} mostra como o comprimento
esperado do caminho de busca em uma 
 #Skiplist# pode ser reduzido a 
\[
     e\ln #n# + O(1) \approx 1.884\log #n# + O(1) 
\]
pelo uso de lançamentos de moedas tendenciosas.
Mesmo com essa otimização, o comprimento esperado de caminhos de busca 
em uma 
#SkiplistSSet# é notavelmente maior que em uma 
#Treap#.

\section{Discussão e Exercícios}

Árvores binárias de busca aleatórias têm sido estudadas extensivamente.
Devroye
\cite{d88} provou o \lemref{rbs} e outros resultados relacionados. 
Há resultados bem mais precisos na literatura também, o mais impressionante é
de Reed
\cite{r03}, que mostra que a altura esperada de uma árvore binária de busca aleatória é 
\[
  \alpha\ln n - \beta\ln\ln n + O(1)
\]
onde $\alpha\approx4.31107$ é a única solução no intervalo 
$[2,\infty)$ da equação $\alpha\ln((2e/\alpha))=1$ e 
$\beta=\frac{3}{2\ln(\alpha/2)}$ .  Além disso, a variância da altura é constante.

O nome #Treap# foi cunhado por 
 Seidel e Aragon \cite{as96} que discutiram as propriedades das 
#Treap#s e algumas de suas variantes. Entretanto, 
a estrutura básica da #Treap# foi estudada muito antes por 
Vuillemin \cite{v80} que chamou elas de árvores cartesianas.

Uma possibilidade de otimização relacionada ao uso de memória da 
estrutura de dados #Treap# é a eliminação do armazenamento explícito
da prioridade #p# em cada nodo. Em vez disso, a prioridade de um nodo #u#
é computado usando o hashing do endereço de #u# em memória
\javaonly{ (no Java 32-bits, isso é equivalente a fazer o hashing de 
#u.hashCode()#)}.  Embora muitas funções hash provavelmente funcionam bem para isso na prática, para que as partes importantes da prova 
do \lemref{rbs} continuarem a serem válidas, a função hash deve ser randomizada e ter 
a \emph{propriedade de independência em relação ao mínimo}:
\index{independência em relação ao mínimo}%
Para quaisquer valores distintos
$x_1,\ldots,x_k$, cada um dos valores hash $h(x_1),\ldots,h(x_k)$
devem ser distintos com alta probabilidade e para cada $i\in\{1,\ldots,k\}$,
\[
   \Pr\{h(x_i) = \min\{h(x_1),\ldots,h(x_k)\}\} \le c/k
\]
para alguma constante $c$.
Uma classe de funções hash desse tipo que é fácil de implementar e
razoavelmente rápida é o \emph{hashing por tabulação} 
(\secref{tabulation}).
\index{hashing por tabulação}%
\index{hashing!tabulação}%

Outra variante 
#Treap# que não guarda a prioridade em cada nodo é a árvore binária de busca randomizada. 
\index{árvore binária de busca randomizada}%
\index{árvore binária de busca!randomizada}%
de Mart\'\i nez e Roura \cite{mr98}.
Nessa variante, todo nodo #u# guarda o tamanho #u.size# da subárvore enraizada em #u#. 
Ambos algoritmos 
 #add(x)# e #remove(x)# são randomizados. 
 O algoritmo para adicionar #x# à subárvore enraizada em #u#
faz o seguinte:
\begin{enumerate}
   \item Com probabilidade $1/(#size(u)#+1)$, 
     o valor #x# é adicionado normalmente, como uma folha, e rotações 
     são então feitas para trazer #x# à raiz dessa subárvore. 
   \item Caso contrário, (com probabilidade $1-1/(#size(u)#+1)$), o valor #x#
   é recursivamente adicionado em uma das duas subárvores enraizadas em #u.left#
   ou #u.right#, conforme apropriado.
\end{enumerate}
O primeiro caso corresponde a uma operação #add(x)# em uma #Treap# onde o nodo 
de #x# recebe uma prioridade aleatória que é menor que qualquer uma das prioridades 
na subárvore de #u# e esse caso ocorre com exatamente a mesma probabilidade.

A remoção de um valor #x# de uma árvore binária de busca randomizada é similar
ao processo de remoção de uma #Treap#. Achamos o nodo #u# que contém #x# e então
realizamos rotações que repetidamente aumentar a profundidade de #u# até tornar-se uma folha, onde podemos remover da árvore. A decisão de fazer rotações à esquerda
ou direita é randomizada. 
\begin{enumerate}
  \item Com probabilidade #u.left.size/(u.size-1)#, fazemos uma rotação à direita em #u#, fazendo #u.left# a raiz da subárvore que anteriormente estava enraizada em #u#. 
  \item  Com probabilidade #u.right.size/(u.size-1)#, realizamos uma rotação 
    à esquerda em #u#, fazendo #u.right# a raiz da subárvore que estava 
    anteriormente enraizada em #u#.
\end{enumerate}
Novamente, podemos facilmente verificar que essas são exatamente as mesmas
probabilidades que o algoritmo de remoção em uma #Treap# terá para 
fazer rotação à esquerda ou direita de #u#.

Árvores binárias de busca randomizada têm a desvantagem, em relação às treaps,
de que ao adicionar ou remover elementos elas farão muitas escolhas aleatórias
e precisam manter os tamanhos das subárvores.
Uma vantagem de árvores binárias de busca randomizada em relação às treaps é que
os tamanhos de subárvores pode servir para outro uso: prover acesso por rank
em tempo esperado $O(\log #n#)$ (veja o \excref{treap-get}).  
Em comparação, as prioridades aleatórias guardadas em nodos de uma 
treap somente são úteis para manter a treap balanceada. 

\begin{exc}
  Simule a adição de 4.5 (com prioridade 7) e então 7.5 (com prioridade 20) na #Treap# da \figref{treap}.
\end{exc}

\begin{exc}
  Simule a remoção do elemento 5 e então do elemento 7 na #Treap# da
  \figref{treap}.
\end{exc}

\begin{exc}
  Prove que existem $21,964,800$ sequências que geram a árvore
  do lado direito da 
   \figref{rbs-lvc}.  (Dica: obtenha uma fórmula recursiva para o número
   de sequências que geram uma árvore binária de altura $h$ e use essa
   fórmula para $h=3$.) 
\end{exc}

\begin{exc}
  Projete e implemente o método
  #permute(a)# que contém #n# valores distintos e aleatoriamente permuta #a#. 
  O método deve rodar em 
  $O(#n#)$ de tempo e você deve provar que cada uma das 
  $#n#!$ possíveis permutações de #a# é igualmente provável.
\end{exc}

\begin{exc}\exclabel{treap-rotates}
  Use ambas partes do \lemref{rbs-treap} para provar que o número esperado de rotações
  realizadas por uma operação #add(x)# (e portanto uma operação 
  #remove(x)#) é $O(1)$.
\end{exc}

\begin{exc}
  Modifique a implementação
  #Treap# dada aqui para que não guarde explicitamente os valores de prioridades. 
  Em vez disso, você deve simular esses valores com o hashing de cada nodo usando #hashCode()#.
\end{exc}

\begin{exc}
  Suponha que uma árvore binária de busca guarda em cada nodo #u# a altura 
  #u.height# da subárvore enraizada em #u# e o tamanho #u.size# da subárvore
  enraizada em #u#. 
  \begin{enumerate}
    \item Mostre que, se fizermos uma rotação à esquerda ou direita em #u#, então
      essas duas quantidades podem ser atualizadas em tempo constante 
      para todos os nodos afetados pela rotação. 
    \item Explique porque o mesmo resultado não é possível se tentarmos 
      também guardar a profundidade #u.depth# de cada nodo #u#. 
  \end{enumerate}
\end{exc}

\begin{exc}
  Projete e implemente um algoritmo que constrói uma #Treap# a partir de um 
  array ordenado #a# de #n# elementos. Esse método deve rodar em 
  $O(#n#)$ de tempo no pior caso e deve construir uma #Treap# que é
  indistinguível de uma em que os elementos de #a# foram adicionados um
  por vez usando o método #add(x)#.
\end{exc}

\begin{exc}
  \index{finger}%
  \index{busca finger!em uma treap}%
  Este exercício trabalha os detalhes de como é possível eficientemente
  fazer buscas em uma #Treap# dado um ponteiro que está próximo ao nodo
  que estamos procurando. 
  \begin{enumerate}
    \item Projete e implemente uma #Treap# em que cada nodo registra os 
      valores mínimos e máximos em sua subárvore. 
    \item Usando essa informação extra, adicione um método #fingerFind(x,u)#
      que executa a operação
      #find(x)# com a ajuda de um ponteiro para o nodo #u# (que 
      espera-se que não esteja distante do nodo que contém #x#).
      Essa operação deve iniciar em #u# subir na árvore até que alcance um
      nodo #w# tal que 
       $#w.min#\le #x#\le #w.max#$.
      A partir desse ponto, deve-se realizar uma busca padrão por #x#
      partindo de #w#. 
      (É possível mostrar que #fingerFind(x,u)# leva
      $O(1+\log r)$ de tempo, onde $r$ é o número de elementos em uma treap 
      cujo valor está entre #x# e #u.x#.) 
    \item Estenda sua implementação em uma versão de treap que 
      inicia todas as operações #find(x)# a partir do nodo mais 
      recentemente encontrado por #find(x)#. 
  \end{enumerate}
\end{exc}

\begin{exc}\exclabel{treap-get}
  Projete e implemente uma versão de #Treap# que inclui uma operação
  #get(i)#
  que retorna a chave com rank #i# na #Treap#.  (Dica: faça que cada 
  nodo #u# registre o tamanho da subárvore enraizada em #u#.) 
\end{exc}

\begin{exc}
  \index{TreapList@#TreapList#}%
  Codifique uma #TreapList#, uma implementação de uma interface #List# na forma de uma treap.  
  Cada nodo na treap deve guardar um item da lista e uma 
  travessia em-ordem na treap encontra os itens na mesma 
  ordem que ocorrem na lista.
  Todas as operações da #List#,
  #get(i)#, #set(i,x)#,
  #add(i,x)# e #remove(i)# devem rodar em tempo esperado $O(\log #n#)$.
\end{exc}

\begin{exc}\exclabel{treap-split}
  Projete e implemente uma versão de uma #Treap# que aceita a operação #split(x)#.
  Essa operação remove todos os valores da #Treap# que são maiores que 
  #x# e retorna uma segunda #Treap# que contém todos os valores removidos.

  \noindent Exemplo: o código #t2 = t.split(x)# remove de #t# todos os
  valores maiores que #x# e retorna uma nova #Treap# #t2# contendo todos esses valores. 
 A operação #split(x)# deve rodar em tempo esperado $O(\log #n#)$.

  \noindent Aviso: Para essa modificação funcionar adequadamente e ainda 
  possibilitar o método #size()# rodar em tempo constante, é necessário 
  implementar as modificações no \excref{treap-get}.
\end{exc}

\begin{exc}\exclabel{treap-join}
  Projete e implemente uma versão de uma #Treap# que aceita a operação
  #absorb(t2)#, que pode ser pensada como o inverso da operação #split(x)#. 
  Essa operação remove todos os valores da #Treap# #t2# e os adiciona ao receptor.
  Essa operação pressupõe que o menor valor em #t2# é maior que o 
  maior valor no receptor. A operação #absorb(t2)# deve rodar em tempo esperado 
  $O(\log #n#)$.
\end{exc}

\begin{exc}
  Implemente a árvore binária de busca randomizada de Martinez conforme discutido nesta seção.
  Compare o desempenho da sua implementação com a implementação da #Treap#. 
\end{exc}
