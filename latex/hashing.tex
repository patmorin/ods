\renewcommand{\cite}[1]{}

\chapter{Hash Tables}
\chaplabel{hashtables}
\chaplabel{hashing}

Hash tables are an efficient method of storing a small number,
#n#, of integers from a large range $U=\{0,\ldots,2^{#w#}-1\}$.
The term \emph{hash table} includes a broad range of data structures.
This chapter focuses on one of the most common implementations of hash
tables, namely hashing with chaining.

Very often hash tables store data that are not integers.  In this case,
an integer \emph{hash code} is associated with each data item and this
hash code is used in the hash table.  The second part of this chapter
discusses how such hash codes are generated.

Some of the methods used in this chapter require random choices of
integers in some specific range.  In the code samples, some of these
``random'' integers are hard-coded constants.  These constants were
obtained using random bits generated from atmospheric noise.


\section{HashTable: Hashing with Chaining}
\seclabel{hashtable}

A #HashTable# data structure uses \emph{hashing with chaining} to store
data as an array, #t#, of lists.  An integer, #n#, keeps track of the
total number of items in all lists:
\javaimport{ods/HashTable.t.n}
The \emph{hash value} of a data item #x#, denoted #hash(x)# is a value in the range $\{0,\ldots,#t.length#-1\}$.  All items with hash value #i# are stored in the list at #t[i]#.
To ensure that lists don't get too long, we maintain the invariant
\[
    #n# \le #t.length#
\]
so that the average number of elements stored in one of these lists is 
$#n#/#t.length# \le 1$.

To add an element #x# to the hash table, we first check if the length
of #t# needs to be increased and, if so, we grow #t#.  With
this out of the way we hash #x# to get an integer #i# in the range
$\{0,\ldots,#t.length#-1\}$ and we append #x# to the list #t[i]#:
\javaimport{ods/HashTable.add(x)}
Growing the table, if necessary, involves doubling the length of #t#
and reinserting all elements into the new table.  This is exactly the
same strategy used in the implementation of #ArrayStack# and the same
result applies: The cost of growing is only constant when amortized
over a sequence of insertions (see~\lemref{arraystack-amortized} on
page~\pageref{lem:arraystack-amortized}).

Besides growing, the only other work done when adding #x# to a
#HashTable# involves appending #x# to the list #t[hash(x)]#.  For any of
the list implementations described in Chapters~\ref{chap:arrays} or
\ref{chap:linkedlists}, this takes only constant time.

To remove an element #x# from the hash table we iterate over the list
#t[hash(x)]# until we find #x# so that we can remove it:
\javaimport{ods/HashTable.remove(x)}
\javaimport{ods/HashTable.removeOne(x)}
This takes $O(#n#_{#hash(x)#})$time, where $#n#_{#i#}$ denotes the length
of the list stored at #t[i]#.

Searching for the element #x# in a hash table is similar.  We perform a linear search on the list
#t[hash(x)]#:
\javaimport{ods/HashTable.find(x)}
Again, this takes time proportional to the length of the list #t[hash(x)]#.

The performance of a hash table depends critically on the choice of the
hash function.  A good hash function will spread the elements evenly
among the #t.length# lists, so that the expected size of the list
#t[hash(x)]# is $O(#n#/#t.length)# = O(1)$.  On the other
hand, a bad hash function will hash all values (including #x#) to the
same table location, in which case the size of the list #t[hash(x)]#
will be #n#.  In the next section we describe a good hash function.

\subsection{Multiplicative Hashing}
\seclabel{multihash}

Multiplicative hashing is an efficient method of generating hash
values based on modular arithmetic (discussed in \secref{arrayqueue})
and integer division.  It uses the $\ddiv$ operator, which calculates
the integral part of a quotient, while discarding the remainder.
Formally, for any integers $a\ge 0$ and $b\ge 1$, $a\ddiv b = \lfloor
a/b\rfloor$.

In multiplicative hashing, we use a hash table of size $2^{#d#}$ for some
integer #d# (called the \emph{dimension}).  The formula for hashing an
integer $#x#\in\{0,\ldots,2^{#w#}-1\}$ is
\[
    #hash#(#x#) = ((#z#\cdot#x#) \bmod 2^{#w#}) \ddiv 2^{#w#-#d#} \enspace .
\]
Here, #z# is a randomly chosen \emph{odd} integer in
$\{1,\ldots,2^{#w#}-1\}$.
This hash function can be realized very
efficiently by observing that, by default, operations on integers
are already done modulo $2^{#w#}$ where $#w#$ is the number of bits in
an integer.  (See \figref{multihashing}.) Furthermore, integer division
by $2^{#w#-#d#}$ is equivalent to dropping the rightmost $#w#-#d#$ bits in
a binary representation (which is implemented by shifting the bits
right by $#w#-#d#$).  In this way, the code that
implements the above formula is simpler than the formula itself:
\javaimport{ods/HashTable.hash(x)}

\begin{figure}
  \begin{center}
    \begin{tabular}{|lr@{}r|}\hline
    $2^#w#$ (4294967296)&            #1#&#00000000000000000000000000000000# \\
    #z# (4102541685)&                   &#11110100100001111101000101110101# \\
    #x# (42) &                          &#00000000000000000000000000101010# \\
    $#z#\cdot#x#$ &             #101000#&#00011110010010000101110100110010# \\
    $(#z#\cdot#x#)\bmod 2^{#w#}$ &      &#00011110010010000101110100110010# \\
    $((#z#\cdot#x#)\bmod 2^{#w#})\ddiv 2^{#w#-#d#}$ &&
                      \multicolumn{1}{@{}l|}{#00011110#} \\\hline
    \end{tabular}
  \end{center}
  \caption{The operation of the multiplicative hash function with $#w#=32$
    and $#d#=8$.}
  \figlabel{multihashing}
\end{figure}

The following lemma, whose proof is deferred until later in this section,
shows that multiplicative hashing does a good job of avoiding collisions:

\begin{lem}\lemlabel{universal-hashing}
  Let #x# and #y# be any two values in $\{0,\ldots,2^{#w#}-1\}$ with
  $#x#\neq #y#$. Then $\Pr\{#hash(x)#=#hash(y)#\} \le 2/2^{#d#}$.
\end{lem}

With \lemref{universal-hashing}, the performance of #remove(x)#, and
#find(x)# are easy to analyze:

\begin{lem}
  For any data value #x#, the expected length of the list #t[hash(x)]#
  is at most $#n#_{#x#} + 2$, where $#n#_{#x#}$ is the number of
  occurrences of #x# in the hash table.
\end{lem}

\begin{proof}
  Let $S$ be the (multi-)set of elements stored in the hash table that
  are not equal to #x#.  For an element $#y#\in S$, define the indicator
  variable
    \[ I_{#y#} = \left\{\begin{array}{ll}
       1 & \mbox{if $#hash(x)#=#hash(y)#$} \\
       0 & \mbox{otherwise}
       \end{array}\right.
    \]
  and notice that, by \lemref{universal-hashing}, $\E[I_{#y#}] \le
  2/2^{#d#}=2/#t.length#$.  The expected length of the list #t[hash(x)]#
  is given by
  \begin{eqnarray*}
   \E\left[#t[hash(x)].size()#\right] &=& \E\left[#n#_{#x#} + \sum_{#y#\in S} I_{#y#}\right] \\
    &=& #n#_{#x#} + \sum_{#y#\in S} \E [I_{#y#} ] \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#t.length# \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#n# \\
    &\le& #n#_{#x#} + (#n#-#n#_{#x#})2/#n# \\
    &\le& #n#_{#x#} + 2 \enspace ,
  \end{eqnarray*}
  as required.
\end{proof}

Now, we want to prove \lemref{universal-hashing}, but first we need a
result from number theory.  In the following proof, we use the
notation $(b_r,\ldots,b_0)_2$ to denote $\sum_{i=0}^r b_i2^i$, where
each $b_i$ is a bit either 0 or 1, i.e., the integer whose binary
representation is given by $b_r,\ldots,b_0$.  We use $\star$ to denote
a bit of unknown value.

\begin{lem}\lemlabel{hashing-mapping}
  Let $S$ be the set of odd integers in $\{1,\ldots,2^{#w#}-1\}$, Let $q$
  and $i$ be any two elements in $S$.  Then there is exactly one value
  $#z#\in S$ such that $#z#q\bmod 2^{#w#} = i$.
\end{lem}

\begin{proof}
  Since the number of choices for $#z#$ and $i$ is the same, it is
  sufficient to prove that there is \emph{at most} one value $#z#\in S$ that
  satisifies $#z#q\bmod 2^{#w#} = i$.

  Suppose, for the sake of contradiction, that there are two such values
  #z# and #z'#, with $#z#>#z#'$.  Then
  \[
     #z#q\bmod 2^{#w#} = #z#'q \bmod 2^{#w#} = i
  \]
  So
  \[ 
     (#z#-#z#')q\bmod 2^{#w#} = 0 
  \]
  But this means that 
  \begin{equation}
    (#z#-#z#')q = k 2^{#w#} \eqlabel{factors} 
  \end{equation}
  for some integer $k$.  Thinking in terms of binary numbers, we have 
  \[
    (#z#-#z#')q = k\cdot(1,\underbrace{0,\ldots,0}_{#w#})_2 \enspace ,
  \]
  so that the #w# trailing bits in the binary representation of
  $(#z#-#z#')q$ are all 0's.

  Furthermore $k\neq 0$ since $q\neq 0$ and $#z#-#z#'\neq 0$.  Since $q$
  is odd, it has no trailing 0's in its binary representation:
  \[
    q = (\star,\ldots,\star,1)_2 \enspace .
  \]
  Since $|#z#-#z#'| < 2^{#w#}$, $#z#-#z#'$ has fewer than #w# trailing
  0's in its binary representation:
  \[
    #z#-#z#' = (\star,\ldots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2
      \enspace .
  \]
  Therefore, the product $(#z#-#z#')q$ has fewer than #w# trailing 0's in
  its binary representation:
  \[
   (#z#-#z#')q = (\star,\cdots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2 
    \enspace .
  \]
  Therefore $(#z#-#z#')q$ cannot satisfy \eqref{factors}, yielding a
  contradiction and completing the proof.
\end{proof}

The utility of \lemref{hashing-mapping} comes from the following
observation:  If #z# is chosen uniformly at random from $S$, then #zt#
is uniformly distributed over $S$.  In the following proof, it helps
to think of the binary representation of #z#, which consists of $#w#-1$
random bits followed by a 1.

\begin{proof}[Proof of \lemref{universal-hashing}]
  First we note that the condition $#hash(x)#=#hash(y)#$ is equivalent to
  the statement ``the highest-order #d# bits of $#z##x#\bmod2^{#w#}$
  and the highest-order #d# bits of $#z##y#\bmod 2^{#w#}$ are the same.''
  A necessary condition of that statement is that the highest-order
  #d# bits in the binary representation of $#z#(#x#-#y#)\bmod 2^{#w#}$
  are either all 0's or all 1's.  That is,
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} = 
      (\underbrace{0,\ldots,0}_{#d#},\underbrace{\star,\ldots,\star}_{#w#-#d#})_2 
      \eqlabel{all-zeros}
  \end{equation}
  when $#zx#\bmod 2^{#w#} > #zy#\bmod 2^{#w#}$ or
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} = 
      (\underbrace{1,\ldots,1}_{#d#},\underbrace{*,\ldots,*}_{#w#-#d#})_2 
       \enspace .
      \eqlabel{all-ones}
  \end{equation}
  when $#zx#\bmod 2^{#w#} < #zy#\bmod 2^{#w#}$.
  Therefore, we only have to bound the probability that 
  $#z#(#x#-#y#)\bmod 2^{#w#}$ looks like \eqref{all-zeros} or \eqref{all-ones}.
  
  Let $q$ be the unique odd integer such that $(#x#-#y#)\bmod
  2^{#w#}=q2^r$ for some integer $r\ge 0$. By
  \lemref{hashing-mapping}, the binary representation of $#z#q\bmod
  2^{#w#}$ has $#w#-1$ random bits, followed by a 1:
  \[
   #z#q\bmod 2^{#w#}  = (\underbrace{b_{#w#-1},\ldots,b_{1}}_{#w#-1},1)_2
  \]
  Therefore, the binary representation of $#z#(#x#-#y#)\bmod 2^{#w#}=#z#q2^r\bmod 2^{#w#}$ has
  $#w#-r-1$ random bits, followed by a 1, followed by $r$ 0's:
  \[
  #z#(#x#-#y#)\bmod 2^{#w#}  =
  #z#q2^{r}\bmod 2^{#w#} =
      (\underbrace{b_{#w#-r-1},\ldots,b_{1}}_{#w#-r-1},1,\underbrace{0,0,\ldots,0}_{r})_2
  \]
  We can now finish the proof:  If $r > #w#-#d#$, then the #d#
  higher order bits of $#z#(#x#-#y#)\bmod 2^{#w#}$  contain both 0's
  and 1's, so the probability that $#z#(#x#-#y#)\bmod 2^{#w#}$ looks
  like \eqref{all-zeros} or \eqref{all-ones} is 0.  If $#r#=#w#-#d#$,
  then the probability of looking like \eqref{all-zeros} is 0, but the
  probability of looking like \eqref{all-ones} is $1/2^{#d#-1}=2/2^{#d#}$
  (since we must have $b_1,\ldots,b_{d-1}=1,\ldots,1$).  If $r < #w#-#d#$
  then we must have $b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=0,\ldots,0$ or
  $b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=1,\ldots,1$.  The probability of each
  of these cases is $1/2^{#d#}$ and they are mutually exclusive, so the
  probability of either of these cases is $2/2^{#d#}$.  This completes
  the proof.
\end{proof}

\subsection{Summary}

The following theorem summarizes the performance of the #HashTable#
data structure:

\begin{thm}\thmlabel{hashtable}
  A #HashTable# implements the #USet# interface.  Ignoring the cost of
  calls to #grow()#, a #HashTable# supports the operations #add(x)#,
  #remove(x)#, and #find(x)# in $O(1)$ expected time per operation.
  Furthermore, beginning with an empty #HashTable#, any sequence of
  $m$ #add(x)# and #remove(x)# operations results in a total of $O(m)$
  time spent during all calls to #grow()#.
\end{thm}

\section{Hash Codes}

The hash tables discussed in the previous section are used to associate
data with integer keys consisting of #w# bits.  In many cases, we
have keys that are not integers.  They may be strings, objects, arrays,
or other compound structures.  To use hash tables for these types of data,
we must map these data types to #w#-bit hash codes.  Hash code mappings should
have the following properties:

\begin{enumerate}
  \item If #x# and #y# are equal, then #x.hashCode()# and #y.hashCode()#
  are equal.

  \item If #x# and #y# are not equal, then the probability that
  $#x.hashCode()#=#y.hashCode()#$ should be small (close to
  $1/2^{#w#}$).
\end{enumerate}

The first property ensures that if we store #x# in a hash table and later
look up a value #y# equal to #x#, then we will find #x#---as we should.
The second property minimizes the loss from converting our objects
to integers.  It ensures that unequal objects usually have different
hash codes and so are likely to be stored at different locations in
our hash table.

\subsection{Hash Codes for Primitive Data Types}

Small primitive data types like #char#, #byte#, #int#, and #float# are
usually easy to find hash codes for.  These data types always have a
binary representation and this binary representation usually consists
of #w# or fewer bits. (For example, in Java, #byte# is an 8-bit type
and #float# is a 32-bit type.) In these cases, we just treat these bits
as the representation of an integer in the range $\{0,\ldots,2^#w#-1\}$.
If two values are different, they get different hash codes.  If they
are the same, they get the same hash code.

A few primitive data types are made up of more than #w# bits, usually
$c#w#$ bits for some constant integer $c$. (Java's #long# and #double#
types are examples of this with $c=2$.)  These data types can be treated
as compound objects made of $c$ parts, as described in the next section.

\subsection{Hash Codes for Compound Objects}
\seclabel{stringhash}

For a compound object, we want to create a hash code by combining the
individual hash codes of the object's constituent parts.  This is not
as easy as it sounds.  Although one can find many hacks for this (for
example, combining the hash codes with bitwise exclusive-or operations),
many of these hacks turn out to be easy to foil.

However, if one is willing to do arithmetic with $2#w#$ bits of
precision, then there are simple and robust methods available.
Suppose we have an object made up of several parts
$P_0,\ldots,P_{r-1}$ whose hash codes are $#x#_0,\ldots,#x#_{r-1}$.
Then we can choose mutually independent random #w#-bit integers
$#z#_0,\ldots,#z#_{r-1}$ and a random $2#w#$-bit odd integer #z# and
compute a hash code for our object with
\[
   h(#x#_0,\ldots,#x#_{r-1}) =  
   \left(\left(#z#\sum_{i=0}^{r-1} #z#_i #x#_i\right)\bmod 2^{2#w#}\right)
   \ddiv 2^{#w#} \enspace .
\]
Note that this hash code has a final step (multiplying by #z# and
dividing by $2^{#w#}$) that uses the multiplicative hash function
from \secref{multihash} to take the $2#w#$-bit intermediate result and
reduce it to a #w#-bit final result.  Here is an example of this method applied to a simple compound object with 3 parts #x0#, #x1#, and #x2#:
\javaimport{junk/Point3D.x0.hashCode()}
The following theorem shows that, in addition to being straightforward to implement, this method is provably good:

\begin{thm}\thmlabel{multihash}
Let $#x#_0,\ldots,#x#_{r-1}$ and $#y#_0,\ldots,#y#_{r-1}$ each be sequences of #w# bit integers in $\{0,\ldots,2^{#w#}-1\}$ and assume $#x#_i \neq #y#_i$ for at least one index $i\in\{0,\ldots,r-1\}$. Then 
\[
   \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \} 
        \le 3/2^{#w#} \} \enspace .  
\] 
\end{thm}

\begin{proof}
  We will first ignore the final multiplicative hashing step and see how
  that step contributes later.  Define:
  \[
    h'(#x#_0,\ldots,#x#_{r-1}) =  
       \left(\sum_{j=0}^{r-1} #z#_j #x#_j\right)\bmod 2^{2#w#} \enspace .
  \]
  Suppose that $h'(#x#_0,\ldots,#x#_{r-1}) =  h'(#y#_0,\ldots,#y#_{r-1})$.
  We can rewrite this as:
  \begin{equation}  \eqlabel{bighash-x}
      #z#_i(#x#_i-#y#_i) \bmod 2^{2#w#} = t
  \end{equation}
  where 
  \[
     t = \left(\sum_{j=0}^{i-1} #z#_j(#y#_j-#x#_j) + \sum_{j=i+1}^{r-1} #z#_j(#y#_j-#x#_j)\right) \bmod 2^{2#w#}
  \]
  If we assume, without loss of generality that $#x#_i> #y#_i$, then
  \eqref{bighash-x} becomes
  \begin{equation}
      #z#_i(#x#_i-#y#_i) = t \eqlabel{bighash-xx} \enspace ,
  \end{equation}
  since each of $#z#_i$ and $(#x#_i-#y#_i)$ is at most $2^{#w#}-1$, so
  their product is at most $2^{2#w#}-2^{#w#+1}+1 < 2^{2#w#}-1$.
  By assumption, $#x#_i-#y#_i\neq 0$, so \eqref{bighash-xx} has at most one
  solution in $#z#_i$.  Therefore, since $#z#_i$ and $t$ are
  independent ($#z#_0,\ldots,#z#_{r-1}$ mutually independent), the
  probability that we select $#z#_i$
  so that $h'(#x#_0,\ldots,#x#_{r-1})=h'(#y#_0,\ldots,#y#_{r-1})$ is at most
  $1/2^{#w#}$.

  The final step of the hash function is to apply multiplicative hashing
  to reduce our $2#w#$-bit intermediate result $h'(#x#_0,\ldots,#x#_{r-1})$ to
  a #w#-bit final result $h(#x#_0,\ldots,#x#_{r-1})$.  By \thmref{multihash},
  if $h'(#x#_0,\ldots,#x#_{r-1})\neq h'(#y#_0,\ldots,#y#_{r-1})$, then
  $\Pr\{h(#x#_0,\ldots,#x#_{r-1}) = h(#y#_0,\ldots,#y#_{r-1})\} \le 2/2^{#w#}$.

  To summarize, 
  \begin{eqnarray*}
    \Pr\left\{\begin{array}{l}
         h(#x#_0,\ldots,#x#_{r-1}) \\
          \quad = h(#y#_0,\ldots,#y#_{r-1})\end{array}\right\}
      &=& \Pr\left\{\begin{array}{ll}
            \mbox{$h'(#x#_0,\ldots,#x#_{r-1}) = h'(#y#_0,\ldots,#y#_{r-1})$ or} \\
            \mbox{$h'(#x#_0,\ldots,#x#_{r-1}) \neq h'(#y#_0,\ldots,#y#_{r-1})$} \\
                  \mbox{\quad and
$#z#h'(#x#_0,\ldots,#x#_{r-1})\ddiv2^{#w#} = #z# h'(#y#_0,\ldots,#y#_{r-1})\ddiv 2^{#w#}$}
          \end{array}\right\} \\
      &\le& 1/2^{#w#} + 2/2^{#w#} = 3/2^{#w#}
  \end{eqnarray*}
  and this completes the proof.
\end{proof}


\subsection{Hash Codes for Arrays and Strings}
\seclabel{polyhash}

The method from the previous section works well for objects that have a
fixed, constant, number of components.  However, it breaks down when we
want to use it with objects that have a variable number of components
since it requires a random #w#-bit integer $#z#_i$ for each component.
We could use a pseudorandom sequence to generate as many $#z#_i$'s as we
need, but then the $#z#_i$'s are not mutually independent, and it becomes
difficult to prove that the pseudorandom numbers don't interact badly
with the hash function we are using.  In particular, the values of $t$
and $#z#_i$ in the proof of \thmref{multihash} are no longer independent.

A more rigorous approach is to base our hash codes on polynomials over
prime fields.  This method is based on the following theorem, which says that
polynomials over prime fields behave pretty-much like usual polynomials:

\begin{thm}\thmlabel{prime-polynomial}
 Let $#p#$ be a prime number, and let $f(#z#) = #x#_0#z#^0 + #x#_1#z#^1 +
 \cdots + #x#_{r-1}#z#^{r-1}$ be a non-trivial polynomial with coefficients
 $#x#_i\in\{0,\ldots,#p#-1\}$. Then the equation $f(#z#)\bmod #p# = 0$
 has at most $r-1$ solutions for $#z#\in\{0,\ldots,p-1\}$.
\end{thm}

To use \thmref{prime-polynomial}, we hash a sequence of integers
$#x#_0,\ldots,#x#_{r-1}$ with each $#x#_i\in \{0,\ldots,#p#-2\}$ using
a random integer $#z#\in\{0,\ldots,#p#-1\}$ via the formula
\[
   h(#x#_0,\ldots,#x#_{r-1}) 
    = \left(#x#_0#z#^0+\cdots+#x#_{r-1}#z#^{r-1}+(#p#-1)#z#^r \right)\bmod #p# \enspace .
\]

Note the extra $(#p#-1)#z#^r$ term at the end of the formula.  It helps
to think of $(#p#-1)$ as the last element, $#x#_r$, in the sequence
$#x#_0,\ldots,#x#_{r}$.  Note that this element differs from every other
element in the sequence (each of which is in the set $\{0,\ldots,#p#-2\}$).
We can think of $#p#-1$ as an end-of-sequence marker.

The following theorem, which considers the case of two sequences of
the same length, shows that this hash function gives a good return for
the small amount of randomization needed to choose #z#:

\begin{thm}\thmlabel{stringhash-eqlen}
Let $#p#>2^{#w#}+1$ be a prime,
let $#x#_0,\ldots,#x#_{r-1}$ and $#y#_0,\ldots,#y#_{r-1}$ each be sequences of #w#-bit integers in $\{0,\ldots,2^{#w#}-1\}$, and assume $#x#_i \neq #y#_i$ for at least one index $i\in\{0,\ldots,r-1\}$. Then 
\[
   \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \} 
        \le (r-1)/#p# \} \enspace .  
\] 
\end{thm}

\begin{proof}
The equation $h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1})$
can be rewritten as
\begin{equation}  \eqlabel{strhash-eqlen}
  \left(
     (#x#_0-#y#_0)#z#^0+\cdots+(#x#_{r-1}-#y#_{r-1})#z#^{r-1} 
  \right)\bmod #p# = 0.
\end{equation}
Since $#x_i#\neq #y_i#$, this polynomial is non-trivial.  Therefore,
by \thmref{prime-polynomial}, it has at most $r-1$ solutions in #z#.
The probability that we pick #z# to be one of these solutions is therefore
at most $(r-1)/#p#$.
\end{proof}

Note that this hash function also deals with the case in which two
sequences have different lengths, even when one of the sequences is a
prefix of the other. This is because this function effectively hashes
the infinite sequence
\[
#x#_0,\ldots,#x#_{r-1}, #p#-1,0,0,\ldots \enspace .
\]
This guarantees that if we have two sequences of length $r$ and $r'$
with $r > r'$, then these two sequences differ at index $i=r$.  In
this case, \eqref{strhash-eqlen} becomes
\[
  \left(
     \sum_{i=0}^{i=r'-1}(#x#_i-#y#_i)#z#^i + (#x#_{r'} - #p# + 1)#z#^{r'}
     \sum_{i=r'+1}^{i=r-1}#x#_i#z#^i + (#p#-1)#z#^{r}
  \right)\bmod #p# = 0 \enspace ,
\]
which, by \thmref{prime-polynomial}, has at most $r$ solutions in $#z#$.
This combined with \thmref{stringhash-eqlen} suffice to prove the following
more general theorem:

\begin{thm}\thmlabel{stringhash}
Let $#p#>2^{#w#}+1$ be a prime,
let $#x#_0,\ldots,#x#_{r-1}$ and $#y#_0,\ldots,#y#_{r'-1}$ be distinct sequences of #w#-bit integers in $\{0,\ldots,2^{#w#}-1\}$. Then 
\[
   \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \} 
        \le r/#p# \} \enspace .  
\] 
\end{thm}

The following example code shows how this hash function is applied to
an object that contains an array, #x#, of values:
\javaimport{junk/GeomVector.hashCode()}

The above code sacrifices some collision probability for implementation
convenience.  In particular, the multiplier #z# was chosen randomly from
$\{0,\ldots,2^{30}-1\}$ to avoid overflow with unsigned 63-bit arithmetic.
This means that the probability of two different sequences, the longer
of which has length $r$, having the same hash code is at most $r/2^{30}$, rather
than the $r/(2^{32}+15)$ specified in \thmref{stringhash}.

\section{Summary and Further Topics}

Hash tables and hash codes are a enormous and active area of research
that is just touched upon in this chapter.
A variety of different hash table implementations exist. The one described
in \secref{hashtable} is known as \emph{hashing with chaining} (each array
entry contains a chain (#List#) of elements).

Another alternative is
that used by \emph{open addressing} schemes, where all data is stored
directly in an array.  Open addressing schemes must deal with the
problem of \emph{collision resolution}: the case where two values hash
to the same array location.  Different strategies exist for collision
resolution and these provide different performance guarantees and often
require more sophisticated hash functions than the multiplicative hash
function described here.

Yet another category of hash table implementations are the so-called
\emph{perfect hashing} methods.   These are methods in which #find(x)#
operations take $O(1)$ time in the worst-case.  For static data sets,
this can be accomplished by finding \emph{perfect hash functions} for
the data; these are functions that map each piece of data to a unique
array location.  For data that changes over time, perfect hashing methods
include \emph{FKS two-level hash tables} \cite{fksXX} and \emph{cuckoo hashing} \cite{prXX}.

The hash functions presented in this chapter are probably among the most
practical currently known methods that can be proven to work well for any
set of data.  Other provably good methods date back to the pioneering work
of Carter and Wegman who introduced the notion of \emph{universal hashing}
and described several hash functions for different scenarios \cite{cwXX}.

The \emph{multiplicative hashing} method described in \secref{multihash}
is one of the simplest, but it's collision probability of $2/2^{#d#}$
is a factor of 2 larger than what one could expect with a random function
from $2^{#w#}\to 2^{#d#}$.   The \emph{multiply-add hashing} method
uses the function
\[
   h(#x#) = ((#z##x# + b2^{\lceil(#w#-#d#)/2\rceil}) \bmod 2^{#w#}) \ddiv 2^{#w#-#d#}
\]
where #z# is a randomly-chosen odd integer in $\{1,\ldots,2^{#w#}-1\}$
and $b$ is a randomly chosen integer in $\{0,\ldots,2^{\lfloor(#w#-#d#)/2\rfloor}\}$.
Multiply-add hashing has a collision probability of only $1/2^{#d#}$.
(The proof of this is considerably harder than the proof for multiplicative
hashing.)

There are a number of methods of obtaining hash codes from fixed-length
sequences of #w#-bit integers.  One particularly fast method \cite{umac}
is the function
\[
  h(#x#_0,\ldots,#x#_{r-1}) = \left(\sum_{i=0}^{r/2-1} ((#x#_{2i}+#a#_{2i})\bmod 2^{#w#})((#x#_{2i+1}+#a#_{2i+1})\bmod 2^{#w#})\right) \bmod 2^{2#w#}
\]
where $r$ is even and $#a#_0,\ldots,#a#_{r-1}$ are randomly chosen from
$\{0,\ldots,2^{#w#}\}$. This yields a $2#w#$-bit hash code that has
collision probability $1/2^{#w#}$.  This can be reduced to a #w#-bit
hash code using multiplicative (or multiply-add) hashing. This method
is fast because it requires only $r/2$ $2#w#$-bit multiplications whereas
the method described in \secref{stringhash} requires $r$ multiplications.
(The $\bmod$ operations occur implicitly by using #w# and $2#w#$-bit arithmetic for the additions and multiplications, respectively.)

The method from \secref{polyhash} of using polynomials over prime
fields to hash variable-length arrays and strings is, unfortunately, not
very fast.  This is due to its use of the $\bmod$ operator which relies
on a costly machine instruction.  Some variants of this method choose
the prime #p# to be one of the form $2^{#w#}-1$, in which case the
$\bmod$ operator can be replaced with addition (#+#) and bitwise-and
(#&#) operations \cite{knuth}.  Another option is to apply one of the
fast methods for fixed-length strings to blocks of length $c$ for some
constant $c>1$ and then apply the prime field method to the resulting
sequence of $\lceil r/c\rceil$ hash codes.

\section{Exercises}

\begin{enumerate}
\item Prove that the bound $2/2^{#d#}$ in \lemref{universal-hashing} is
the best possible by showing that, if $x=2^{#w#-#d#-2}$ and $#y#=3#x#$,
then $\Pr\{#hash(x)#=#hash(y)#\}=2/2^{#d#}$.  (Hint look at the binary
representations of $#zx#$ and $#z#3#x#$ and use the fact that $#z#3#x# =
#z##x#+2#z##x#$.)

\item Suppose you have an object made up of two #w#-bit integers #x# and #y#.  Show why $#x#\oplus#y#$ does not make a good hash code for your object.  Give an example of a large set of objects that would all have hash code 0.

\item Suppose you have an object made up of two #w#-bit integers #x# and #y#.  Show why $#x#+#y#$ does not make a good hash code for your object.
Give an example of a large set of objects that would all have the same hash code.

\item Suppose you have an object made up of two #w#-bit integers #x#
and #y#.  Suppose that the hash code for your object is defined by some
deterministic function $h(x,y)$.  Prove that there exists a large set
of objects that have the same hash code.

\item Let $p=2^{#w#}-1$ for some positive integer #w#.  Explain why, for a positive integer $x$
\[
    (x\bmod 2^{#w#}) + (x\ddiv 2^{#w#}) \equiv x \bmod (2^{#w#}-1) \enspace .
\]
(This gives an algorithm for computing $x \bmod (2^{#w#}-1)$ by repeatedly setting
\[
   #x= x&((1<<w)-1) + x>>>w#
\]
until $#x# \le 2^{#w#}-1$.)
\end{enumerate}
