\chapter{Introdução}
\pagenumbering{arabic}
Todo currículo de Ciência da Computação no mundo inclui ao menos uma disciplina sobre estruturas de dados e algoritmos.
Estruturas de dados são \emph{realmente} importantes;
elas melhoram nossa qualidade de vida e até salvam vidas.
Muitas companhias que valem múltiplos milhões e bilhões de dólares foram construídas em torno de estruturas de dados.

Como isso é possível? Se pararmos para pensar nisso, percebemos que interagimos com estruturas de dados constantemente.

\begin{itemize}
  \item  Abrir um arquivo: Sistema de arquivos 
    \index{sistema de arquivos}%
    estruturas de dados são usadas para encontrar
    as partes daquele arquivo em disco para serem recuperadas.
    Isso não é fácil; discos contém centenas de milhões de blocos.
    O conteúdo do seu arquivo pode estar armazenado em qualquer um deles.
  \item Procurar contato no telefone: Uma estrutura de dados é usada para procurar por um número de telefone na sua lista de contatos 
    \index{lista de contatos}%
    baseada em informação parcial mesmo antes de você terminar de discar/digitar
    Isso não é fácil;
o seu telefone pode ter informações sobre muitas pessoas---todos que você já entrou em contato via telefone ou email---e seu telefone não tem um processador muito rápido ou muita memória.
  \item Entrar na sua rede social preferida:
    \index{rede social}%
    Os servidores da rede
    usam a sua informação de login para procurar as informações da sua conta.
    Isso não é fácil; as redes sociais mais populares tem centenas de milhões de usuários ativos.
  \item Fazer uma busca na web: 
    \index{busca na web}%
    Um motor de busca usa estruturas de dados para buscar
    por páginas na web que contêm os seus termos de busca.
    Isso não é fácil; existem mais de 8.5 bilhões de páginas web na Internet
    e cada página tem muitos potenciais termos de busca.
  \item Serviços de telefone de emergência (9-1-1):
    \index{serviços de emergência}\index{9-1-1}%
    Os serviços de emergência procuram pelo seu número de telefone em uma estrutura de dados que mapeia números de telefone em endereços para que carros de polícia, ambulâncias ou os bombeiros possam ser enviados imediatamente.
    Isso é importante; a pessoa fazendo a chamada pode não conseguir fornecer o endereço exato de onde está e um atraso pode significar a diferença entre a vida e a morte.
\end{itemize}

\section{A Necessidade de Eficiência}
Na próxima seção, estudaremos as operações aceitas pelas estruturas de dados mais comuns.
Qualquer pessoa com um pouco de experiência em programação verá que essas operações não são difíceis de implementar corretamente.
Podemos implementar ao percorrer/iterar todos os elementos de um array ou lista e possivelmente adicionar ou remover um elemento.
Esse tipo de implementação é fácil, mas não muito eficiente.
Isso realmente importa? Computadores estão ficando cada vez mais rápidos.
Talvez a implementação mais óbvia seja boa o suficiente.
Vamos fazer alguns cálculos aproximados para verificar isso.

\paragraph{Número de operações:}
Imagine uma aplicação com um conjunto de dados de tamanho moderado, digamos de um milhão ($10^6$) de itens. 
É razoável, na maior parte das aplicações, presumir que a aplicação vai precisar verificar cada item pelo menos uma vez. Isso significa que podemos esperar fazermos pelo menos um milhão ($10^6$) de buscas nesses dados. Se cada uma dessas $10^6$ buscas inspecionar cada um dos $10^6$ itens, isso resulta em um total de $10^6\times 10^6=10^{12}$ (mil bilhões) inspeções.

\paragraph{Velocidades do Processador:} 

No momento de escrita deste livro, mesmo um computador de mesa bem rápido não pode fazer mais de um bilhão ($10^9$) de operações por segundo.\footnote{Velocidades de computadores vão até poucos gigahertz (bilhões de ciclos por segundo) e cada operação tipicamente exige alguns ciclos.}
Isso significa que essa aplicação vai levar pelo menos $10^{12}/10^9 = 1000$
segundos, ou aproximadamente 16 minutos e 40 segundos. Dezesseis minutos é uma eternidade em tempos computacionais, mas uma pessoa pode estar disposta a aceitá-lo (por exemplo, se ela estiver indo para um café).

\paragraph{Conjuntos de dados maiores:} 

Agora considere que uma empresa como a Google, 
\index{Google}%
que indexa mais de 8.5 bilhões de páginas web. 
De acordo com os nossos cálculos, fazer qualquer tipo de consulta nesses dados levaria pelo menos 8.5 segundos.
Nós já sabemos que esse não é o caso; buscas web são respondidas em bem menos de 8.5 segundos e elas são bem mais complicadas do que somente verificar se uma dada página está na lista de páginas indexadas.
No momento de escrita deste livro, a Google recebe aproximadamente $4500$ consultas por segundo, o que significa que eles precisariam de pelo menos $4500 \times 8.5 =38,250$ excelentes servidores somente para manter esse tempo de resposta.

\paragraph{A solução:} 
Esses exemplos nos dizem que as implementações mais óbvias de estruturas de dados não funcionam bem quando o número de itens, #n#, na estrutura de dados e o número de operações $m$, feitas na estrutura de dados são altos.
Nesses casos, o tempo (medido em termos de, digamos, duração da execução de instruções de máquina) é proporcional a $#n#\times m$.

A solução, é claro, cuidadosamente organizar os dados na estrutura de dados de forma que nem toda operação exija que todos os itens de dados sejam inspecionados.
Embora pareça inicialmente impossível, descreveremos estruturas de dados em que a busca requer olhar em apenas dois itens em média, independentemente do número de itens guardados na estrutura de dados. No nosso computador de um bilhão de instruções por segundo, levaria somente $0.000000002$
segundos para buscar em uma estrutura de dados contendo um bilhão de itens (ou um trilhão ou um quatrilhão ou mesmo um quintilhão de itens).

Nós veremos também implementações de estruturas de dados que mantêm os itens em ordem, onde o número de itens inspecionados durante uma operação cresce muito lentamente em função do número de itens na estrutura de dados.
Por exemplo, podemos manter um conjunto ordenado com um bilhão de itens e inspecionar no máximo 60 itens para qualquer operação.
No nosso computador de um bilhão de instruções por segundo, essas operações levam $0.00000006$ segundos cada.

O resto deste capítulo revisa brevemente alguns dos principais conceitos usados ao longo do resto do livro. A \secref{interfaces} descreve as interfaces implementadas por todas as estruturas de dados descritas neste livro e devem ser consideradas como leitura obrigatória.

O restante das seções discute:

\begin{itemize}
  \item revisão de conceitos matemáticos incluindo exponenciais, logaritmos, fatoriais, notação assintótica (big-Oh), probabilidades e aleatorização;
  \item modelos de computação;
  \item corretude, tempo de execução e uso de espaço;
  \item uma visão geral do resto dos capítulos; e 
  \item uma amostra de código juntamente com convenções de escrita de código usadas neste livro.
\end{itemize}

Um leitor com (ou mesmo sem) conhecimento desses assuntos pode simplesmente pulá-las agora e voltar se necessário.

\section{Interfaces}
\seclabel{interfaces}
Ao discutir sobre estruturas de dados, é importante entender a diferença entre a interface de uma estrutura de dados e sua implementação. 
Uma interface descreve o que uma estrutura de dados faz, enquanto uma implementação descreve como a estrutura o faz.

Uma \emph{interface},
\index{interface}%
\index{tipo abstrato de dados|see{interface}}%
às vezes também chamada de \emph{tipo abstrato de dados}(\emph{abstract data type}, em inglês),
define o conjunto de operações aceitas por uma estrutura de dados e a semântica, ou significado, dessas operações.

Uma interface nos diz nada sobre como a estrutura de dados implementa essas operações; ela somente provê uma lista de operações aceitas juntamente com as especificações sobre quais tipos de argumentos cada operação aceita e o valor retornado por cada operação.

A \emph{implementação} de uma estrutura de dados, por outro lado, inclui a representação interna da estrutura de dados assim como as definições dos algoritmos que implementam as operações aceitas pela estrutura de dados. 
Então, pode haver muitas implementações de uma dada interface.
Por exemplo, no \chapref{arrays}, veremos implementações da interface #List# usando arrays e no \chapref{linkedlists} veremos implementações da interface #List# usando estruturas de dados baseadas em ponteiros. Ambas implementam a mesma interface, #List#, mas de modos distintos.

\subsection{As Interfaces #Queue#, #Stack# e #Deque#}

A interface #Queue# (Fila, em português) representa uma coleção de elementos à qual podemos
\index{fila}%
\index{queue}%
adicionar elementos e remover o próximo elemento. Mais precisamente, as operações
realizadas pela interface #Queue# são
\begin{itemize}
  \item #add(x)#: adiciona o valor #x# à #Queue#
  \item #remove()#: remove o próximo (previamente adicionado) valor, #y#, da #Queue# e retorna #y#
\end{itemize}
Note que a operação #remove()# não recebe argumentos.
A \emph{política de enfileiramento} da #Queue# decide qual é o próximo elemento a ser removido.

Existem muitas políticas de enfileiramento possíveis, sendo que entre as mais comuns estão FIFO, por prioridades e LIFO.

Uma \emph{#Queue# FIFO (first-in-first-out -- primeiro a entrar, primeiro a sair)},
\index{FIFO queue}%
\index{queue!FIFO}%
que é ilustrada em 
\figref{queue}, remove itens na mesma ordem em que são adicionados, do meio jeito que uma fila de compras em um supermercado funciona.
Esse é o tipo mais comum de #Queue# de forma que o qualificador FIFO é frequentemente omitido.
Em outros textos, as operações #add(x)# e #remove()# em uma #Queue# FIFO são frequentemente chamadas de  #enqueue(x)# (enfileirar) e #dequeue()# (desenfileirar), respectivamente.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption[Uma queue (fila) FIFO]{Uma #Queue# FIFO --- primeiro a entrar, primeiro a sair.}
  \figlabel{queue}
\end{figure}

Uma \emph{#Queue# com prioridades},
\index{priority queue}%
\index{fila com prioridades}%
\index{priority queue|seealso{heap}}%
\index{queue!priority}%
ilustrada em \figref{prioqueue}, sempre 
remove o menor elemento da #Queue#, decidindo empates arbitrariamente.
Isso é similar ao modo no qual pacientes passam por triagem em uma sala de emergência de um hospital. Conforme os pacientes chegam, eles são avaliados e, então, vão à sala de espera. Quando um médico torna-se disponível, ele primeiro trata o paciente na situação mais grave. A operação #remove()# em um #Queue# com prioridades é geralmente chamada de #deleteMin()# em outros textos.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption[Uma fila com prioridades]{Uma #Queue# com prioridades --- itens são removidos de acordo com suas prioridades (em inglês, priority queue).}
  \figlabel{prioqueue}
\end{figure}

%This is similar to the way
%many airlines manage upgrades to the business class on their flights.
%When a business-class seat becomes available it is given to the most
%important customer waiting on an upgrade.

Uma política de enfileiramento muito comum é a política LIFO (last-in-first-out, último-a-entrar-primeiro-a-sair, em português)
\index{LIFO queue}%
\index{LIFO queue|seealso{stack}}%
\index{fila LIFO}%
\index{queue!LIFO}%
\index{stack}%
\index{pilha}%
, ilustrada na \figref{stack}. Em uma \emph{Queue LIFO},
o elemento mais recentemente adicionado é o próximo a ser removido. 
Isso é melhor visualizado em termos de uma pilha de pratos; pratos são 
posicionados no topo da pilha a também removidos do topo. Essa estrutura
é tão comum que recebe seu próprio nome: #Stack# (pilha, em português). Frequentemente, ao referenciar uma #Stack#, as operações #add(x)# e #remove()# 
recebem os nomes de #push(x)# e #pop()#; isso é para evitar confusões entre as políticas LIFO e FIFO.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption[Uma stack]{Uma stack (pilha, em português) --- LIFO (último-a-entrar-primeiro-a-sair).}
  \figlabel{stack}
\end{figure}


Uma #Deque#
\index{deque}%
é uma generalização de ambos #Queue# FIFO e #Queue# LIFO (#Stack#).

Uma #Deque# representa uma sequência de elementos, com uma frente e um verso (um início e um fim). 
Elementos podem ser adicionados no início da sequência ou no final da sequência.
Os nomes das operações da #Deque# são auto-explicativos: 
#addFirst(x)#, #removeFirst()#, #addLast(x)# e #removeLast()#.  
Vale notar que uma #Stack# pode ser implementada usando somente #addFirst(x)#
e #removeFirst()# enquanto uma #Queue# FIFO pode ser implementada usando 
#addLast(x)# e #removeFirst()#.

\subsection{A Interface #List#: Sequências Lineares}

Este livro vai falar muito pouco sobre 
as interfaces #Queue# FIFO, #Stack#, ou #Deque#. Isso porque essas interfaces são englobadas pela interface 
#List#.  Uma #List#,
\index{List@#List#}%
ilustrada na \figref{list}, representa uma
sequência, $#x#_0,\ldots,#x#_{#n#-1}$, de valores. A interface #List# inclui as operações a seguir:

\begin{enumerate}
  \item #size()#: retorna #n#, o comprimento da lista 
  \item #get(i)#: retorna o valor $#x#_{#i#}$
  \item #set(i,x)#: atribui o valor $#x#_{#i#}$ igual a #x#
  \item #add(i,x)#: adicionar #x# à posição #i#, deslocando 
    $#x#_{#i#},\ldots,#x#_{#n#-1}$; \\ 
    Atribua $#x#_{j+1}=#x#_j$, para todo 
    $j\in\{#n#-1,\ldots,#i#\}$, incremente #n#, e faça $#x#_i=#x#$
  \item #remove(i)# remove o valor $#x#_{#i#}$, deslocando 
    $#x#_{#i+1#},\ldots,#x#_{#n#-1}$; \\ 
    Atribua $#x#_{j}=#x#_{j+1}$, para todo 
    $j\in\{#i#,\ldots,#n#-2\}$ e decremente #n#
\end{enumerate}
Note que essas operações são facilmente suficientes para implementar
a interface #Deque#:
\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CONTINUAR 
\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption[Uma List]{Uma #List# representa uma sequência indexada por 
   $0,1,2,\ldots,#n#-1$.  Nessa #List# uma chamada para #get(2)# retornaria o valor $c$.}
  \figlabel{list}
\end{figure}

Embora normalmente não discutiremos as interfaces #Stack#, #Deque# e #Queue# FIFO nos seguintes capítulos, os termos 
#Stack# e #Deque#
são às vezes usados nos nomes das estruturas de dados que implementam a interface #List#. Quando isso acontece, queremos destacar que essas estruturas de dados podem ser usadas para implementar a interface #Stack# ou #Deque# eficientemente. 
Por exemplo, a classe #ArrayDeque# é uma implementação da interface #List# que implementa todas as operações #Deque# em tempo constante por operação.

\subsection{A Interface #USet#: Unordered Sets (Conjuntos Desordenados)}

A interface #USet#
\index{USet@#USet#}%
representa um conjunto desordenado de elementos únicos (sem repetições), que simulam um \emph{conjunto} (em inglês, \emph{set}) matemático. 
Um #USet# contém #n# elementos \emph{distintos}; nenhum elemento aparece mais de uma vez; os elementos não estão em nenhuma ordem específica.
Um #USet# possui as seguintes operações:

\begin{enumerate}
  \item #size()#: retorna o número, #n#, de elementos no conjunto 
  \item #add(x)#: adiciona o elemento #x# ao conjunto, se não já presente; \\
    Adiciona #x# ao conjunto considerando que não há elemento #y# no conjunto tal que #x# é considerado igual a #y#. Retorna #true# se #x# foi adicionado ao conjunto e #false# caso contrário.
  \item #remove(x)#: remove #x# do conjunto; \\
    Achar um elemento #y# no conjunto tal que #x# iguala 
    a #y# e remove #y#. Retorna #y#, ou #null# se tal elemento não existe.
  \item #find(x)#: achar #x# no conjunto se existir; \\
    Achar um elemento #y# no conjunto tal que #y# seja igual a #x#. Retornar #y# ou #null# se tal elemento não exista no conjunto. 
\end{enumerate}

Essas definições são um pouco cuidadosas ao distinguir #x#, o elemento que estamos a remover ou buscar, de #y#, o elemento que poderemos remover ou achar.
Isso é porque #x# e #y# podem ser na realidade objetos distintos que são, para todos os efeitos nessa situação, tratados como iguais.

\javaonly{\footnote{Em Java, isso é realizado ao sobrescrever, #@Override#, os métodos da classe dos elementos #equals(y)# e #hashCode()#.}}
Tal distinção é útil porque permite a criação de 
\emph{dicionários} ou \emph{mapas} que mapeiam chaves em valores. 
\index{dictionary}%
\index{dicionário}%
\index{map}%
\index{mapa}%

Para criar um dicionário/mapa, forma-se objetos chamados #Pair# (par, em português)
\index{pair}%
cada qual contém uma \emph{chave} (\emph{key}) e um \emph{valor} (\emph{valor}).
Dois pares #Pair# são tratados como iguais se suas chaves forem iguais.
Se armazenamos algum par 
$(#k#,#v#)$
em um #USet# e então depois chamamos o método #find(x)# usando 
o par
$#x#=(#k#,#null#)$ o resultado será $#y#=(#k#,#v#)$.  
Em outras palavras, é possível recuperar o valor, #v#, usando somente a chave #k#.

\subsection{A Interface #SSet#: Sorted Sets---Conjuntos Ordenados}
\seclabel{sset}

\index{SSet@#SSet#}%
A interface #SSet# representa um conjunto ordenado de elementos. 
Um #SSet# guarda elementos provenientes de alguma ordem total, tal que quaisquer dois elementos #x# e #y# podem ser comparados. Em código, isso será feito com um método chamado #compare(x,y)# no qual
\[
    #compare(x,y)# 
      \begin{cases}
        {}<0 & \text{se $#x#<#y#$} \\
        {}>0 & \text{se $#x#>#y#$} \\
        {}=0 & \text{se $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%
Um #SSet# possui os métodos #size()#, #add(x)# e #remove(x)# com 
exatamente a mesma semântica que a interface #USet#. A diferença
entre um #USet# e um #SSet# está no método #find(x)#:
\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: localizar #x# no conjunto ordenado; \\
   Achar o menor elemento #y# no conjunto tal que $#y# \ge #x#$.
   Retornar #y# ou #null# se o elemento não existir.
\end{enumerate}

Essa versão da operação #find(x)# é às vezes chamada de \emph{busca de sucessor} (em inglês, \emph{successor search}).
\index{successor search}%
\index{busca de sucessor}%
Ela difere de um modo fundamental de #USet.find(x)# pois retorna um resultado mesmo quando não há elemento igual a #x# no conjunto.

A distinção entre a operação #find(x)# de #USet# e de #SSet# é muito importante e frequentemente esquecida ou não percebida. A funcionalidade extra provida por um #SSet# frequentemente vem com um preço que inclui tanto tempo de execução mais alto quanto uma complexidade de implementação maior.
Por exemplo, a maior parte das implementações #SSet# discutidas neste livro tem operações #find(x)# com tempo de execução que são logarítmicas no tamanho do conjunto.
Por outro lado, a implementação de um #USet# com uma #ChainedHashTable# no
\chapref{hashing} tem uma operação #find(x)# que roda em tempo esperado constante. 
Ao escolher quais dessas estruturas usar, deve-se sempre usar um #USet# a menos que a funcionalidade extra oferecida por um #SSet# seja verdadeiramente necessária.

% \subsection{The DynamicString Interface}

\section{Conceitos Matemáticos}
Nesta seção, revisaremos algumas noções matemáticas e ferramentas usadas 
ao longo deste livro, incluindo logaritmos, notação big-Oh e
teoria das probabilidades. Esta revisão será breve e não tem a intenção
de ser uma introdução. Leitores que acham que precisam saber mais desses conceitos
são encorajados a ler, e fazer os exercícios relacionados, as seções apropriadas
do excelente (e livre) texto didático sobre Matemática para Ciência da Computação
\cite{llm11}.

%% CONTINUAR AQUI TODO
\subsection{Exponenciais e Logaritmos}

\index{exponencial}%
A expressão $b^x$ denota o número $b$ elevado à potência $x$.
Se $x$ é um inteiro positivo, então o resultado é somente o valor de $b$ multiplicado por ele mesmo $x-1$ vezes:
\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x} \enspace .
\]
Quando $x$ é um inteiro negativo, $b^{-x}=1/b^{x}$. 
Quando $x=0$, $b^x=1$.
Quando $b$ não é um inteiro, podemos ainda definir a exponenciação em termos da função exponencial 
$e^x$ (ver a seguir), que é definida em termos de uma série exponencial, mas isso é melhor deixar para um texto sobre cálculo. 

\index{logaritmo}%
Neste livro, a expressão 
$\log_b k$ denota o \emph{logaritmo base $b$}
de $k$.  Isto é, o único valor $x$ que satisfaz 
\[
    b^{x} = k  \enspace .
\]
A maior parte dos logaritmos neste livro são base 2 (\emph{logaritmos binários}).
\index{logaritmo binário}%
\index{logaritmo!binário}%
Nesse caso, omitiremos a base, de forma que 
$\log k$ é uma abreviação para 
$\log_2 k$.

Um jeito informal, mas útil, de pensar sobre logaritmos é pensar
de $\log_b k$ como o número de vezes que temos que dividir $k$ por $b$
antes do resultado ser menor ou igual a 1. Por exemplo, quando alguém usa uma
busca binária, cada comparação reduz o número de possíveis respostas por 
um fator de 2. Isso é repetido até que exista no máximo uma única resposta 
possível. Portanto, o número de comparações feitas pela busca binária 
quando existem inicialmente até $n+1$ possíveis respostas é, no máximo, 
$\lceil\log_2(n+1)\rceil$.

\index{logaritmo natural}%
\index{logaritmo!natural}%
Outro logaritmo que aparece várias vezes neste livro é o 
\emph{logaritmo natural}. Aqui usamos a notação $\ln k$ para denotar 
$\log_e k$, onde $e$ --- a \emph{constante de Euler} --- é dada por 
\index{constante de Euler}%
\index{e@$e$ (constante de Euler)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828 \enspace .
\]
O logaritmo natural aparece frequentemente porque é o valor
de uma integral particularmente comum:
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k \enspace .
\]
Duas das manipulações mais comuns que fazemos com logaritmos são
removê-los de um expoente:
\[
    b^{\log_b k} = k
\]
e trocar a base de um logaritmo:
\[
    \log_b k = \frac{\log_a k}{\log_a b} \enspace .
\]
Por exemplo, podemos usar essas duas manipulações para comparar os logaritmos natural e binário:
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} = 
    (\ln 2)(\log k) \approx 0.693147\log k \enspace .
\]

\subsection{Fatoriais}
\seclabel{factorials}

\index{fatorial}%
Em uma ou duas partes deste livro,
a função \emph{fatorial} é usada.
Para um inteiro não-negativo $n$, a notação $n!$ (pronunciada ``$n$ fatorial'') é definida para representar 
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n \enspace .
\]
Fatoriais aparecem porque $n!$ conta o número de permutações distintas, i.e., ordenações, de $n$ elementos distintos. 
\index{permutação}%
Para o caso especial $n=0$, $0!$  é definido como 1. 

\index{Aproximação de Stirling}%
A quantidade $n!$ pode ser aproximada usando a \emph{aproximação de Stirling}:
\[
	n! 
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)} \enspace ,
\]
onde
\[  
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}  \enspace .
\]
a aproximação de Stirling também aproxima $\ln(n!)$:
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
(De fato, a aproximação de Stirling é mais facilmente provada aproximando
$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$ com a integral
$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$.)

\index{coeficientes binomiais}%
Relacionados à função fatorial são os 
\emph{coeficientes binomiais}.
Para um inteiro não-negativo $n$ e um inteiro $k\in\{0,\ldots,n\}$,
a notação $\binom{n}{k}$ denota:
\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!} \enspace .
\]
O coeficiente binomial
 $\binom{n}{k}$ (pronunciado ``$n$, escolhidos $k$ a $k$'')
 conta o número de subconjuntos com $k$ elementos de um conjunto de tamanho $n$ 
i.e., o número de formas de escolher $k$ inteiros distintos do conjunto $\{1,\ldots,n\}$.

\subsection{Notação assintótica}

\index{notação assintótica}%
\index{notação big-Oh}%
\index{O@$O$ notation}%
Ao analisar estruturas de dados neste livro, queremos discutir
os tempos de execução de várias operações. O tempo exato de execução irá,
é claro, variar de computador para computador e até entre diferentes execuções 
no mesmo computador.
Ao discutirmos sobre o tempo de execução de uma operação estamos nos 
referindo ao número de instruções de computador executadas durante a operação.
Mesmo para códigos simples, essa quantidade pode ser difícil de computar com exatidão.
Portanto, em vez de analisar tempos de execução exatos, iremos usar a famosa \emph{notação big-Oh}: Para uma função $f(n)$, $O(f(n))$ denota um conjunto de funções,

\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{existe $c>0$, e $n_0$ tal que} \\
             \quad\mbox{$g(n) \le c\cdot f(n)$ para todo $n\ge n_0$}   
     \end{array} \right\} \enspace .
\]
Pensando graficamente, esse conjunto consiste das funções $g(n)$ 
onde 
$c\cdot f(n)$ começa a dominar $g(n)$ quando $n$ é suficientemente grande. 

Geralmente usamos notação assintótica para simplificar funções. Por exemplo,
em vez de usarmos $5n\log n + 8n - 200$ podemos escrever $O(n\log n)$.
Isso é provado da seguinte forma:
\begin{align*} 
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ for $n\ge 2$ (tal que $\log n \ge 1$)}
            \\
        & \le 13n\log n  \enspace .
\end{align*}
Isso demonstra que a função $f(n)=5n\log n + 8n - 200$ está no conjunto 
$O(n\log n)$ usando as constantes $c=13$ e $n_0 = 2$.

Vários atalhos podem ser aplicados ao usar notação assintótica. 
Primeiro:
\[ O(n^{c_1}) \subset O(n^{c_2}) \enspace ,\]
para qualquer $c_1 < c_2$.  Segundo: para quaisquer constantes $a,b,c > 0$,
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \enspace . \]
Essas relações de inclusão podem ser multiplicadas por qualquer valor positivo,
e eles ainda valerão.
Por exemplo, multiplicar por $n$ chegamos a:
\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \enspace . \]

Continuando uma longa e notável tradição, iremos abusar dessa notação 
ao escrever 
coisas do tipo $f_1(n) = O(f(n))$ quando o que realmente queremos
expressar é $f_1(n) \in O(f(n))$.  
Também iremos fazer afirmações do tipo ``o tempo de execução dessa operação 
é $O(f(n))$'' quando essa afirmação deveria ser na verdade 
``o tempo de execução dessa operação é \emph{um membro de} $O(f(n))$.''
Esses atalhos servem principalmente para evitar frases estranhas e tornar 
mais fácil o uso de notação assintótica em manipulações sequenciais de equações.

A exemplo particularmente estranho disso ocorre quando escrevemos afirmações tipo
\[
  T(n) = 2\log n + O(1)  \enspace .
\]
De novo, isso seria mais corretamente escrito da forma
\[
  T(n) \le 2\log n + [\mbox{algum membro de $O(1)$]}  \enspace .
\]

A expressão $O(1)$ também traz à tona outra questão. 
Como não tem nenhuma variável nessa expressão, pode não ser claro qual variável está aumentando. Sem contexto, não tem como dizer.

No exemplo anterior, como a única variável no resto da equação é $n$, podemos assumir que isso deve ser lido como $T(n)=2\log n+O(f(n))$, onde $f(n) = 1$.

A notação
Big-Oh não é nova nem exclusiva à Ciência da Computação. Ela foi 
usada pelo matemático especialista em Teoria dos Números
Paul Bachmann desde pelo menos 1894 e é imensamente útil 
para descrever o tempo de execução de algoritmos de computadores.
Considere o seguinte trecho de código:
\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}
Uma execução desse método envolve: 
\begin{itemize}
      \item $1$ atribuição (#int\, i\, =\, 0#),
      \item $#n#+1$ comparações (#i < n#),
      \item #n# incrementos (#i++#),
      \item #n# cálculos de deslocamentos em array (#a[i]#), e 
      \item #n# atribuições indiretas (#a[i] = i#).
\end{itemize}
Então podemos escrever esse tempo de execução como 
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n# \enspace , 
\]
onde $a$, $b$, $c$, $d$ e $e$ são constantes que dependem
da máquina rodando o código e que representam o tempo para realizar atribuições,
comparações, incrementos, cálculos de deslocamento em array e atribuições indiretas, respectivamente.
Entretanto, se essa expressão representa o tempo de execução de duas linhas de
código, então claramente esse tipo de análise não será viável para códigos ou algoritmos complicados.
Ao usar a notação big-Oh, o tempo de execução pode ser simplificado a 
\[
    T(#n#)= O(#n#) \enspace .
\]
Isso não somente é mais compacto, mas também provê praticamente a mesma informação.
O fato de que o tempo de execução depende das constantes 
$a$, $b$, $c$, $d$ e $e$
no exemplo anterior significa que, em geral, não será possível comparar
dois tempos de execução para saber qual é mais rápido sem saber os valores dessas constantes.
Mesmo se fizermos esforços para determinar essas constantes (digamos, usando testes de medição de tempo), então nossa conclusão será válida somente para a máquina
na qual rodamos nossos testes.

Notação Big-Oh nos permite avaliar a situação a um nível mais alto, 
possibilitando a análise de funções mais complicadas.
Se dois algoritmos têm o mesmo tempo Big-Oh, então não saberemos qual é mais rápido
e que não pode não haver um ganhador em todas as situações. Um algoritmo pode ser mais rápido em uma máquina enquanto o outro em uma máquina diferente. Porém, se os dois algoritmos têm tempos de execução big-Oh distintos, então
teremos certeza que aquele com menor função Big-Oh será mais rápido \emph{para valores #n# grandes o suficiente}.

Um exemplo de como a notação big-Oh nos permite comparar duas diferentes funções é mostrado na \figref{intro-asymptotics}, que compara a taxa de crescimento 
de $f_1(#n#)=15#n#$ versus $f_2(n)=2#n#\log#n#$.  
Hipoteticamente, $f_1(n)$ seria o tempo de execução de um complicado algoritmo de tempo linear enquanto $f_2(n)$ é o tempo de execução de um algoritmo bem mais simples baseado no paradigma de divisão e conquista.
Isso exemplifica essa situação,
embora  $f_1(#n#)$ seja maior que $f_2(n)$ para valores baixos de #n#,
 o oposto é verdade para valores mais altos de #n#.
Eventualmente $f_1(#n#)$ ganha e por uma margem crescente.
Análise usando notação big-Oh nos indica que isso aconteceria, pois
 $O(#n#)\subset O(#n#\log #n#)$.

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{Comparação de $15#n#$ versus $2#n#\log#n#$.}
  \figlabel{intro-asymptotics}
\end{figure}

Em alguns casos, usaremos notação assintótica em funções com mais de uma variável. 
Pode não ser comum mas, para nossos objetivos, a seguinte definição é suficiente:
\[
   O(f(n_1,\ldots,n_k)) = 
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{existe $c>0$, e $z$ tal que} \\
             \quad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$} \\
             \qquad \mbox{para todo $n_1,\ldots,n_k$ tal que $g(n_1,\ldots,n_k)\ge z$}   
   \end{array}\right\} \enspace .
\]
Essa definição captura a situação que mais nos importa:
quando os argumentos 
$n_1,\ldots,n_k$ faz $g$ assumir valores altos. 
Essa definição também está de acordo com a definição univariada de
$O(f(n))$ quando $f(n)$ é uma função crescente de  $n$.
O leitor deve ficar atento ao fato de que, embora isso funcione para o objetivo deste livro, outros textos podem tratar funções multivariadas e notação assintótica diferentemente. 

\subsection{Randomização e Probabilidades}
\seclabel{randomization}

\index{aleatorização}%
\index{probabilidade}%
\index{randomized data structure}%
\index{estrutura de dados aleatorizada}%
\index{randomized algorithm}%
Algumas das estruturas de dados apresentadas neste livro são \emph{randomizadas}\footnote{Randomização é um neologismo para indicar o uso de números aleatórios que influenciam na execução de um algoritmo};
eles fazem escolhas aleatórias que são independentes dos dados nelas armazenadas
ou das operações realizadas neles. Por essa razão,
realizar o mesmo conjunto de operações mais de uma vez usando essas
estruturas pode resultar em tempos de execução variáveis. Ao analisar essas
estruturas de dados estamos interessados no seu tempo de execução médio ou \emph{esperado}.
\index{tempo de execução esperado}%
\index{tempo de execução!esperado}%

Formalmente, o tempo de execução de uma operação em uma estrutura de dados randomizada é aleatório e queremos estudar o seu \emph{valor esperado};
\index{valor esperado}%
Para uma variável aleatória discreta
 $X$ assumindo valores em um universo contável 
$U$, o valor esperado de $X$, denotado $\E[X]$, é dado pela fórmula 
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\} \enspace .
\]

Aqui $\Pr\{\mathcal{E}\}$ denota a probabilidade que o evento 
$\mathcal{E}$ ocorre.  Em todos os exemplos deste livro, essas probabilidades
são somente em relação às escolhas aleatórias feitas pela estrutura de dados randomizada; não supomos que os dados armazenados na estrutura, nem a sequência de operações realizadas na estrutura de dados, seja aleatória.

Uma das propriedades mais importantes dos valores esperados é 
\emph{linearidade da esperança}.
\index{linearidade da esperança}%
Para duas variáveis aleatórias $X$ e $Y$,
\[
   \E[X+Y] = \E[X] + \E[Y] \enspace .
\]
De forma geral, para quaisquer variáveis aleatórias $X_1,\ldots,X_k$,
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i] \enspace .
\]
Linearidade da esperança nos permite quebrar variáveis aleatórias complicadas (como os lados esquerdos das equações acima) em somas de variáveis aleatória mais simples (lados direitos).

Um truque útil, que usaremos repetidamente, é definir \emph{variáveis aleatórias indicadoras}.
\index{variável aleatória indicadora}%
Essas variáveis binárias são úteis quando queremos contar algo e são melhor ilustradas por um exemplo. Suponha que lancemos uma moeda honesta $k$ vezes e que queremos saber o número esperado de vezes que o lado cara aparece.
\index{lançamento de moeda}%
Intuitivamente, sabemos que a resposta é $k/2$,
mas se tentarmos provar usando a definição de valor esperado, temos
\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2 \enspace .
\end{align*}
Isso requer que saibamos o suficiente para calcular que $\Pr\{X=i\}
= \binom{k}{i}/2^k$, e que saibamos as identidades binomiais 
$i\binom{k}{i}=k\binom{k-1}{i-1}$ e $\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$.

Usando variáveis indicadoras e linearidade da esperança torna esse trabalho bem mais fácil. Para cada
$i\in\{1,\ldots,k\}$, defina a variável aleatória indicadora 
\[
    I_i = \begin{cases}
           1 & \text{se o $i$-ésimo lançamento de moeda é cara} \\
           0 & \text{caso contrário.}
          \end{cases}
\]
Então
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \enspace . \]
Agora, $X=\sum_{i=1}^k I_i$, então
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2 \enspace .
\end{align*}
Esse caminho é mais longo, mas não exige que saibamos identidades mágicas ou que obtenhamos expressões não triviais de probabilidades. Melhor ainda, 
ele vai de encontro à intuição que temos sobre metade das moedas saírem cara precisamente porque cada moeda individual sai cara com probabilidade $1/2$.

\section{O Modelo de Computação}
\seclabel{model}

Neste livro, iremos analisar o tempo teórico de execução das operações das estruturas de dados que estudamos. Para fazer precisamente isso, precisamos um modelo matemático de computação. Para isso, usamos
o modelo \emph{#w#-bit word-RAM}
\index{word-RAM}%
\index{RAM}%
.  RAM é uma sigla para Random Access Machine --- Máquina de Acesso Aleatório. Nesse modelo, temos acesso a uma memória de acesso aleatório consistindo de \emph{células}, cada qual armazena uma #w#-bit \emph{word}, ou seja, uma palavra com #w# bits de memória.
\index{word}%
Isso implica que uma célula de memória pode representar, por exemplo, qualquer inteiro no conjunto $\{0,\ldots,2^{#w#}-1\}$.

No modelo word-RAM, operações básicas em words levam tempo constante. 
Isso inclui operações aritméticas (#+#, #-#, #*#, #/#, #%#), comparações 
($<$, $>$, $=$, $\le$, $\ge$) e operações booleanas bit-a-bit (AND, OR e OR exclusivo bit-a-bit).

Qualquer célula pode ler lida ou escrita em tempo constante. 
A memória do computador é gerenciada por um sistema gerenciador de memória a partir do qual podemos alocar ou desalocar um bloco de memória de qualquer tamanho que quisermos. Alocar um bloco de memória de tamanho $k$ leva tempo $O(k)$ e retorna uma referência (um ponteiro) para o bloco recém alocado. Essa referência é pequena o suficiente para ser representada por uma única word.

O tamanho da word #w# é um parâmetro muito importante desse modelo.
A única premissa que faremos sobre #w# é um limitante inferior $#w# \ge \log #n#$,
onde #n# é o número de elementos guardados em qualquer estrutura de dados.
Essa premissa é bem razoável, pois caso contrário uma word não seria 
grande o suficiente para contar o número de elementos guardados na estrutura
de dados.

Espaço é medido em words, de forma que quando falarmos sobre a quantidade de espaço usado por uma estrutura de dados, estamos nos referindo ao número de words de memória usada pela estrutura.
Todas as nossas estruturas de dados guardam valores de um tipo genérico #T#,
e nós presumimos que um elemento de tipo #T# ocupa uma word de memória.
\javaonly{(Na realidade, estamos guardando referências para objetos do tipo #T#, e essas referências ocupam somente uma word de memória.)}

\javaonly{O modelo word-RAM com #w# bits é bem próximo à Java Virtual Machine (JVM) de 32 bits quando $#w#=32$.}
\cpponly{O modelo word-RAM com #w# bits é bem representativo a computadores desktop modernos quando $#w#=32$ or $#w#=64$.} 
As estruturas de dados apresentadas neste livro não usam truques especiais que não são implementáveis \javaonly{na JVM ou na maior parte das arquiteturas.}\cpponly{em C++ ou na maior parte das arquiteturas.} 

\section{Corretude, Complexidade de Tempo e Complexidade de Espaço}

Ao estudar o desempenho de um estrutura de dados, existem três coisas mais importantes:

\begin{description}
  \item[Corretude:] A estrutura de dados deve implementar corretamente sua interface. 
    \index{corretude}%
  \item[Complexidade de tempo:] Os tempos de execução de operações na estrutura de dados deve ser o menor possível. 
    \index{complexidade de tempo}%
    \index{complexidade!tempo}%
  \item[Complexidade de espaço:] A estrutura de dados deve usar o mínimo de memória possível.
    \index{complexidade de espaço}%
    \index{complexidade!espaço}%
\end{description}

%Sometimes these three requirements are in conflict with each other. For
%example, it may be possible to have a faster data structure by using
%more space.  It may be possible to have a data structure that is faster,
%or uses less space, if the data structure is allowed to make (hopefully
%occasional) mistakes.

Neste texto introdutório, consideraremos a corretude como presumida; não consideramos estruturas de dados que dão respostas incorretas a consultas ou que não realizam atualizações corretamente. Iremos, entretanto, ver estruturas de dados que fazem um esforço extra para manter uso de espaço a um mínimo.
Isso não irá em geral afetar o tempo (assintótico) de execução de operações, mas pode fazer as estruturas de dados um pouco mais lentas na prática.

Ao estudar tempos de execução no contexto de estruturas de dados tendemos a encontrar três tipos diferentes de garantias de tempo de execução:

\begin{description}
\item[Tempos de execução no pior caso:] 
  \index{tempo de execução}%
  \index{tempo de execução!pior caso}%
  \index{tempo de execução no pior caso}%
  Esse é o tipo mais forte de garantia de tempo de execução.
    Se uma operação de uma estrutura de dados tem tempo 
    de execução no pior caso de 
   $f(#n#)$, então uma dessas operações  \emph{nunca}
   leva mais tempo que 
$f(#n#)$.
\item[Tempo de execução amortizado:]
  \index{tempo de execução!amortizado}%
  \index{tempo de execução amortizado}%
    Se dizemos que o tempo de execução amortizado de uma operação em uma estrutura de dados é $f(#n#)$, então isso significa que o custo de uma operação típica é no máximo $f(#n#)$.  Mais precisamente, se uma estrutura de dados tem tempo de execução amortizado de
 $f(#n#)$,
 então uma sequência de $m$ operações leva de tempo, no máximo,
 $mf(#n#)$.
 Algumas operações individuais podem levar mais tempo que 
    $f(#n#)$ mas o tempo médio, sobre a sequência inteira de operações, é até $f(#n#)$.
\item[Tempo de Execução Esperado:] 
  \index{tempo de execução!esperado}%
  \index{tempo de execução esperado}%
  Se dizemos que o tempo de esperado de execução de uma operação em uma estrutura de dados é 
   $f(#n#)$, isso significa que o tempo real de execução é uma variável aleatória
 (veja a \secref{randomization})
    e o valor esperado dessa variável aleatória é no máximo 
 $f(#n#)$.
A randomização aqui é em respeito às escolhas aleatórias feitas pela estrutura de dados.
\end{description}

Para entender a diferença entre o pior caso, o tempo amortizado e o tempo esperado, ajuda se considerarmos um exemplo financeiro. Considere o custo de comprar uma casa:

\paragraph{Pior caso versus custo amortizado:}
\index{custo amortizado}%
Suponha que uma casa custe \$120\,000.  A fim de comprar essa casa, podemos pegar um empréstimo de 120 meses (10 anos) com pagamentos mensais de 
\$1\,200. Nesse caso, o custo mensal no pior caso de pagar o empréstimo é 
\$1\,200.

Se temos suficiente dinheiro em mãos, podemos escolher comprar a casa sem empréstimo, com um pagamento de \$120\,000. 
Nesse caso, para o período de 10 anos, o custo amortizado mensal de comprar essa casa é 
\[
   \$120\,000 / 120\text{ meses} = \$1\,000\text{ por mês} \enspace .
\]
Esse valor é bem menor que os \$1\,200 mensais que teríamos que pagar se pegássemos o empréstimo. 

\paragraph{Pior caso versus custo esperado:}
\index{custo esperado}%
A seguir, considere o caso de um seguro de incêndio na nossa casa de \$120\,000.
Por analisar centenas de milhares de casos, seguradoras têm determinado que a quantidade esperada de danos por incêndios causado a uma casa como a nossa é de
\$10 por mês.  
Esse é uma valor bem baixo, pois a maior parte das casa nunca pegam fogo, algumas poucas tem incêndios pequenos que causam algum dano e um número minúsculo de casas queimam até as cinzas. Baseada nessa informação, a seguradora cobra
\$15 mensais para a contratação de um seguro.

Agora é o momento da decisão. Devemos pagar os 
 \$15 referente ao custo de pior caso mensalmente para o seguro ou devemos
apostar fazer uma poupança anti-incêndio ao custo esperado de
\$10 mensal?  Claramente, \$10 por mês custa menos \emph{em expectativa},
mas temos que aceitar que a possibilidade de \emph{custo real} possa ser bem maior.
No evento improvável que a casa queime inteira, o custo real será de \$120\,000.

Esses exemplos financeiros também oferecem a oportunidade de vermos porque às vezes aceitamos um tempo de execução amortizado ou esperado em vez de considerarmos o tempo de execução no pior caso. Frequentemente é possível obter um tempo de execução esperado ou amortizado menor que o obtido no pior caso. 
No mínimo, frequentemente é possível obter uma estrutura de dados bem mais simples se estivermos dispostos a aceitar tempo de execução amortizado ou esperado.

\section{Trechos de Código}

\pcodeonly{

  Os trechos de código neste livro são escritos em pseudocódigo.

\index{pseudocode}%
Eles devem ser de fácil leitura para qualquer um que teve alguma experiência de programação em qualquer linguagem de programação comum dos últimos 40 anos.
Para ter uma ideia do que o pseudocódigo neste livro parece, segue uma função que computa a média de um array, #a#:
\pcodeimport{ods/Algorithms.average(a)}
Como esse código exemplifica, a atribuição a uma variável é feita usando a notação
 $\gets$.
% WARNING: graphic typesetting of assignment operator
Nós usamos a convenção de que o comprimento de um array, #a#, é denotado por
#len(a)# e os índices do array começam com zero,
tal que #range(len(a))# são índices válidos para #a#.  
Para encurtar o código e algumas vezes facilitar a leitura, nosso pseudocódigo permite atribuições de subarrays.
As duas funções a seguir são equivalentes:
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
O código a seguir atribui todos os valores em um array para zero:
\pcodeimport{ods/Algorithms.zero(a)}
Ao analisar o tempo de execução de um código desse tipo, é necessário cuidado;
comandos como 
    #a[0:len(a)] = 1#
ou
   #a[1:len(a)] = a[0:len(a)-1]#
não rodam em tempo constante. Eles rodam em tempo $O(#len(a)#)$.

Usamos atalhos similares com atribuições a variáveis, tal que o código
#x,y=0,1# atribui #x# a zero e #y# a 1 e o código #x,y = y,x# troca
os valores das variáveis #x# e #y#.
\index{swap}

Nosso pseudocódigo usa alguns operadores que podem ser desconhecidos.
Como é padrão em matemática, divisão (normal) é denotada pelo operador $/$
Em muitos casos, queremos fazer divisão inteira e, nesse caso, usamos
o operador
$#//#$, tal que $#a//b# = \lfloor a/b\rfloor$ é a parte inteira de 
$a/b$. Então, por exemplo, $3/2=1.5$ mas $#3//2# = 1$.
\index{divisão inteira}%
\index{operador div}%
Ocasionalmente, também usamos o operador $\bmod$ para obter o resto 
da divisão inteira, mas isso será definido quando for o caso.
\index{operador mod}%
\index{operador div}%
Mais adiante no livro, podemos usar alguns operadores bit-a-bit incluindo o deslocamento à esquerda (#<<#), à direita (#>>#), AND bit-a-bit (#&#) e XOR bit-a-bit (#^#).
\index{deslocamento à esquerda}%
\index{left shift}%
\index{#<<#|see {left shift}}%
\index{#<<#|see {deslocamento à esquerda}}%
\index{deslocamento à direita}%
\index{right shift}%
\index{#>>#|see {right shift}}%
\index{#>>#|see {deslocamente à direita}}%
\index{bitwise and}%
\index{E bit-a-bit}%
\index{#&#|see {bitwise and}}%
\index{#&#|see {E bit-a-bit}}%
\index{#^#|see {bitwise exclusive-or}}%
\index{#^#|see {XOR bit-a-bit}}%

Os trechos de pseudocódigo neste livro são traduções automáticas do código Python que pode ser baixado do website do livro.
\footnote{ \url{http://opendatastructures.org}}  
Se você encontrar qualquer ambiguidade no pseudocódigo que você não consegue resolver por si só, então você pode resolver essa questões com o correspondente código em Python.
Se você não lê Python, o código também está disponível em Java e C++. Se você não consegue decifrar o pseudocódigo, ou ler Python, C++, ou Java, então talvez você não esteja pronto para ler este livro.
}

\notpcode{
Os trechos de código neste livro são escritos na linguagem 
 \lang
. Entretanto, para fazer o livro acessível para leitores não familiares com todas as construções e \emph{keywords} definidas pela linguagem \lang, os trechos de código foram simplificados. Por exemplo, um leitor não irá encontrar keywords como
#public#, #protected#, #private# ou #static#.  
O leitor também não encontrará discussão aprofundada sobre hierarquia de classes. 
Quais interfaces uma determinada classes implementa ou qual classes ela estende, se relevante para a discussão, deve estar claro do texto do contexto em questão.

Essas convenções deve fazer os trechos de código mais fáceis de entender por qualquer um com background linguagens que variantes da linha ALGOL, incluindo B, C, C++, C\#, Objective-C, D, Java, JavaScript e assim por diante.
Leitores em busca de detalhes das implementações são encorajados a olhar no código fonte da linguagem \lang\  que acompanha este livro.

Este livro mistura análise matemática de tempos de execução com o código-fonte da linguagem \lang\ para os algoritmos em análise. Isso significa que algumas equações contêm variáveis também encontradas no código-fonte.

Essas variáveis são formatadas consistentemente, tanto dentro do código-fonte quanto nas equações. A variável mais comum desse tipo é a variável #n# 
\index{n@#n#}%
que, sem exceção, sempre refere-se ao número de itens atualmente armazenados na estrutura de dados.
}

\section{Lista de Estruturas de Dados}

As tabelas~\ref{tab:summary-i} e \ref{tab:summary-ii} resumem o desempenho 
das estruturas de dados que neste livro implementam cada uma das interfaces, #List#, #Uset# e #SSet#, descritas em \secref{interfaces}.
A \Figref{dependencies} mostra as dependências entre vários capítulos neste livro.
\index{dependências}%
Uma linha tracejada indica somente uma dependência fraca, na qual somente uma pequena parte do capítulo depende em um capítulo anterior ou somente nos resultados principais do capítulo anterior.

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{Implementações de #List# } \\ \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{Implementações de #USet#} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{Denota um tempo de execução \emph{amortizado}.}
\item[E]{Denota um tempo de execução \emph{esperado}.}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption[Resumo das implementações de List e USet.]{Resumo das implementações de #List# e #USet#.}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{Implementações de #SSet#} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
  \multicolumn{4}{|c|}{Implementações da #Queue# (de prioridades)} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{Essa estrutura somente pode guardar dados inteiros de #w#-bit.}
\javaonly{\item[X]{Isso denota o tempo de execução no modelo de memória externa; veja o \chapref{btree}.}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption[Resumo das implementações SSet e priority Queue.]{Resumo de implementações de #SSet# e #Queue# de prioridades.}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{Dependências entre capítulos deste livro.}
  \figlabel{dependencies}
\end{figure}

\section{Discussão e Exercícios}

As interfaces 
#List#, #USet# e #SSet# descritas na \secref{interfaces} são influenciadas pela Java Collections Framework
\cite{oracle_collections}.
\index{Java Collections Framework}%
Elas são essencialmente versões simplificadas das interfaces 
#List#, #Set#, #Map#, #SortedSet# e #SortedMap# encontradas no 
Java Collections Framework.  \javaonly{O código-fonte que acompanha este livro inclui 
classes auxiliares para fazer das implementações #USet# e #SSet# também
implementações de #Set#, #Map#, #SortedSet# e #SortedMap#.}

Para um soberbo (e livre) tratamento da matemática discutida neste capítulo, incluindo notação assintótica, logaritmos, fatoriais, aproximação de Sterling, probabilidade básica e muito mais, veja o livro-texto por
Leyman, Leighton e Meyer \cite{llm11}.  
Para um texto de cálculo suave que inclui definições formais de exponenciais e logaritmos, veja o (disponível livremente) texto clássico de Thompson \cite{t14}.

Para maiores informações em probabilidade básica, especialmente como ela se relaciona com Ciência da Computação, veja o livro didático de Ross \cite{r01}.  
Outra boa referência, que cobre notação assintótica e probabilidades, é o livro-texto de Graham, Knuth e Patashnik \cite{gkp94}.

\javaonly{Leitores que desejam afinar suas habilidades de programação Java
podem achar muitos tutoriais online em \cite{oracle_tutorials}.}

\begin{exc}
  Este exercício é planejado para auxiliar o leitor a se familiarizar na escolha da estrutura de dados certa para o problema correto. Se implementado, partes deste exercício devem ser realizadas usando uma implementação da interface relevante
  (#Stack#, #Queue#, #Deque#, #USet# ou #SSet#)
  providas pela \javaonly{Java Collections Framework}\cpponly{C++
  Standard Template Library}.

  Resolva os seguintes problemas fazendo a leitura de um arquivo de texto uma linha por vez e executando operações em cada linha nas estrutura(s) de dados adequada(s).
Suas implementações devem ser rápidas o suficiente tal que até arquivos com um milhão de linhas podem ser processadas em alguns segundos.
  \begin{enumerate}
    \item Leia a entrada uma linha por vez e então escreva as linha em ordem invertida, tal que a última linha lida é a primeira imprimida, e então a penúltima lida é a segunda a ser imprimida e assim por diante. 
    \item Leia as primeiras 50 linhas da entrada e então as escreva na saída em ordem reversa. Leia as seguintes 50 linhas e então escreva na saída em ordem reversa. Faça isso até que não haja mais linhas a serem linhas. Nesse ponto as linhas restantes lidas também devem ser imprimidas invertidas.
Em outras palavras, sua saída iniciará com a 50ª linha, então 49ª linha, então a 48ª e assim até a primeira linha. Isso deverá ser seguido pela centésima linha, seguida pela 99ª até a 51ª linha. Assim por diante.
O seu código nunca deverá manter mais de 50 linhas a qualquer momento.

\item Leia a entrada uma linha por vez.
  A qualquer momento após ler as primeiras 42 linhas, se alguma linha está em branco (i.e., uma string de comprimento 0), então produza a linha que ocorreu 42 linhas antes dela. Por exemplo, se Linha 242 está em branco, então o seu programa deve imprimir a linha 200. Esse programa deve ser implementado tal que ele nunca guarda mais que 43 linhas da entrada a qualquer momento.

\item Leia a entrada uma linha por vez e imprima cada linha se não for uma duplicata de alguma linha anterior. Tenha cuidado especial para que um arquivo com muitas linhas duplicadas não use mais memória do que é necessário para o número de linhas únicas.

\item Leia a entrada uma linha por vez e imprima cada linha somente se você já encontrou uma linha igual antes. (O resultado final é que você remove a primeira ocorrência de cada linha.). Tenha cuidado para que um arquivo com muitas linhas duplicadas não use mais memória do que seja necessário para o número de linhas únicas.

\item Leia a entrada uma linha por vez. Então, imprima todas as linhas ordenadas por comprimento, com as linhas mais curtas primeiro. No caso em que duas linhas 
  tem o mesmo tamanho, decida sua ordem usando a ordem usual de texto. Linhas duplicadas devem ser impressas somente uma vez.

\item Faça o mesmo que a questão anterior exceto que linhas duplicadas devem ser impressas o mesmo número de vezes que elas aparecem na entrada.

\item Leia a entrada uma linha por vez e então imprima as linhas pares (começando com a primeira linha, linha 0) seguidas das linhas ímpares.

\item Leia a entrada inteira uma linha por vez e aleatoriamente troque as linhas antes de imprimir. Para ser claro: você não deve mudar o conteúdo de qualquer linha. Em vez disso, a mesma coleção de linhas deve ser impressa, mas em ordem aleatória.

  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Palavra Dyck}%
  Uma \emph{palavra Dyck} é uma sequência de +1s e -1s com propriedade de que soma de qualquer prefixo da sequência nunca é negativa. Por exemplo,
  $+1,-1,+1,-1$ é uma palavra Dyck, mas $+1,-1,-1,+1$ não é uma palavra Dyck 
  pois o prefixo $+1-1-1<0$.  Descreva qualquer relação entre 
  palavras Dyck e as operações da interface #Stack#, #push(x)# e #pop()#.
\end{exc}

\begin{exc}
  \index{string pareada}%
  \index{string!pareada}%
  Uma \emph{string pareada} é uma sequência de caracteres \{, \}, (, ), [, e ]
  que são apropriadamente pareados. Por exemplo, ``\{\{()[]\}\}''
 é uma string pareada, mas esta ``\{\{()]\}'' não é, uma vez que a segunda \{
  é pareada ].  Mostre como usar uma stack para que, dada uma string de comprimento 
  #n#, você possa determinar se é uma string pareada em tempo $O(#n#)$.
\end{exc}

\begin{exc}
  Suponha que você tem uma 
 #Stack#, #s#, que aceita somente as operações #push(x)#
  e #pop()#. Mostre como, usando somente uma #Queue# FIFO, #q#,
  você pode inverter a ordem de todos os elementos em #s#.
\end{exc}

\begin{exc}
  \index{Bag@#Bag#}%
Usando #USet#, implemente uma #Bag#.  Uma #Bag# é como um #USet#---ele aceita os métodos 
#add(x)#, #remove(x)# e #find(x)# --- mas ele aceita armazenar elementos duplicados. A operação #find(x)# em uma #Bag# retorna
  algum elemento (se tiver) que é igual a #x#. Em adição, uma 
  #Bag# aceita a operação #findAll(x)# que retorna uma lista de todos os elementos
  na #Bag# que são iguais a #x#.
\end{exc}

\begin{exc}
  A partir do zero, escreva e teste implementações das interfaces #List#, #USet# 
  e #SSet#. Essas implementações não precisam ser eficientes. Elas podem ser usadas depois para testar a corretude e desempenho de implementações mais eficientes. (O jeito mais fácil de fazer isso é guardar os elementos em um array.)
\end{exc}

\begin{exc}
Trabalhe para melhorar o desempenho das suas implementações da questão anterior usando quaisquer truques que puder imaginar. Experimente e pense como você pode melhorar o desempenho de #add(i,x)# e #remove(i)# na sua implementação de #List#. Pense como você poderia melhorar o desempenho da operação #find(x)# na suas implementações dos #USet# e #SSet#. Este exercício é tem o intuito de te dar uma ideia da dificuldade de conseguir implementações eficientes dessas interfaces.
\end{exc}
